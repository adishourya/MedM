{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8672086720867209,
  "eval_steps": 1000,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 14.129926681518555
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 10.987876892089844
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.557652473449707
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.1863431930542
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 14.943990707397461
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.802595138549805
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.903421401977539
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.384498596191406
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 11.09670352935791
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.613395690917969
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.124246597290039
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 11.94039249420166
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.062873840332031
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.933205604553223
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.185049057006836
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.294814109802246
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.3217,
      "step": 4
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.577984809875488
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.268057823181152
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.713841438293457
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.191978454589844
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 12.615577697753906
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 11.070530891418457
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 11.047524452209473
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 10.725958824157715
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 13.691665649414062
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 11.939536094665527
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 10.67147445678711
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 12.520310401916504
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.12801742553711
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.262654304504395
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.351304054260254
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 11.123879432678223
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 40.368019104003906,
      "learning_rate": 1e-05,
      "loss": 11.8688,
      "step": 8
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.890803337097168
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.987373352050781
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 10.584735870361328
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.384757041931152
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.245636940002441
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 13.458088874816895
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.077686309814453
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 20.728862762451172
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 36.670955657958984
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 11.576823234558105
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 9.953940391540527
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 12.280098915100098
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.311910629272461
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 11.046745300292969
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 11.21998405456543
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.478476524353027
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 57.13459777832031,
      "learning_rate": 1e-05,
      "loss": 13.9936,
      "step": 12
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.741504669189453
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.608926773071289
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 12.865472793579102
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.580991744995117
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.551597595214844
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.194153785705566
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.380539894104004
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.721258163452148
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 12.327311515808105
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 14.72461223602295
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.198002815246582
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.78889274597168
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 11.589438438415527
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 12.854010581970215
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 12.525895118713379
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 11.336115837097168
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 124.1433334350586,
      "learning_rate": 1e-05,
      "loss": 12.2493,
      "step": 16
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.673766136169434
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 12.111603736877441
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.936027526855469
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.786352157592773
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 13.175630569458008
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 10.511706352233887
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 12.884683609008789
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 12.899235725402832
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 11.828648567199707
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.130088806152344
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.403667449951172
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 13.090483665466309
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 10.568077087402344
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 12.776058197021484
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 11.860496520996094
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 13.441543579101562
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 38.48869323730469,
      "learning_rate": 1e-05,
      "loss": 12.1924,
      "step": 20
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 11.59726619720459
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 12.38797664642334
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 11.997509956359863
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 12.171975135803223
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 13.252991676330566
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 11.797636985778809
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 12.351250648498535
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 12.240026473999023
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.585379600524902
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 19.705284118652344
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.349409103393555
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 11.908636093139648
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 24.068218231201172
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 12.227001190185547
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 12.088069915771484
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 13.330714225769043
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 411.0171813964844,
      "learning_rate": 1e-05,
      "loss": 13.5037,
      "step": 24
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 11.038077354431152
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 10.62121295928955
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.416570663452148
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.03183364868164
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 13.300182342529297
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 12.596121788024902
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 11.770462036132812
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 10.030396461486816
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.13257884979248
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.982141494750977
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 12.298714637756348
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 12.26333999633789
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.968616485595703
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.122617721557617
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 12.440252304077148
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 12.12020492553711
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 11.8208,
      "step": 28
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 12.061192512512207
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 10.90265941619873
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 12.013381004333496
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 11.932342529296875
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 13.995896339416504
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 13.08926010131836
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 10.756449699401855
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 12.076013565063477
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.211387634277344
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 13.422768592834473
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.660667419433594
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 10.969892501831055
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 12.335991859436035
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.659415245056152
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.375340461730957
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.980965614318848
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 27.553009033203125,
      "learning_rate": 1e-05,
      "loss": 12.0902,
      "step": 32
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 14.025689125061035
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 12.932597160339355
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 10.210565567016602
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 13.066004753112793
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 13.513357162475586
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 13.421650886535645
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 14.564518928527832
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 14.454317092895508
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 12.03699016571045
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 13.2438383102417
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 12.322173118591309
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 10.199790954589844
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 12.012755393981934
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 10.50222396850586
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 12.61445140838623
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 11.925227165222168
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 32.004520416259766,
      "learning_rate": 1e-05,
      "loss": 12.5654,
      "step": 36
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 12.147042274475098
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 11.876177787780762
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 13.665154457092285
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 13.972586631774902
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 12.473161697387695
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 10.58101749420166
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 12.095193862915039
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 9.499665260314941
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 12.277303695678711
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 10.623516082763672
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 12.467352867126465
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.0867280960083
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 12.746847152709961
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 13.503342628479004
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 11.63931655883789
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 12.60031795501709
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 14.570959091186523,
      "learning_rate": 1e-05,
      "loss": 12.0784,
      "step": 40
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 14.338016510009766
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 18.55862808227539
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 12.906817436218262
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 12.881967544555664
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 12.30904483795166
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 11.220399856567383
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 10.912490844726562
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 11.895825386047363
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 12.333757400512695
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 11.236323356628418
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 11.940875053405762
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 12.968191146850586
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.587560653686523
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 12.269242286682129
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.196037292480469
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.465539932250977
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 36.722023010253906,
      "learning_rate": 1e-05,
      "loss": 12.5013,
      "step": 44
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 11.402819633483887
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 12.936206817626953
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 12.485295295715332
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 10.630491256713867
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.937567710876465
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.756980895996094
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 12.865609169006348
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 12.892926216125488
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 12.017492294311523
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.977046012878418
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 11.284907341003418
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.70380973815918
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.038915634155273
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.353190422058105
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.42149543762207
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.40738582611084
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 78.1668930053711,
      "learning_rate": 1e-05,
      "loss": 11.6945,
      "step": 48
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 11.541104316711426
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 13.541463851928711
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 10.414023399353027
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 11.290352821350098
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.51103401184082
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.443431854248047
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.95838737487793
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 9.770739555358887
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 10.50637435913086
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 11.624364852905273
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 12.952534675598145
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 10.989760398864746
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 12.793395042419434
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.67139720916748
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.2592191696167
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 9.760356903076172
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 84.78911590576172,
      "learning_rate": 1e-05,
      "loss": 11.4392,
      "step": 52
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 12.081472396850586
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.038619995117188
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 12.72945499420166
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.966651916503906
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 12.189189910888672
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 12.904059410095215
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.13443374633789
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.541630744934082
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.216711044311523
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.9473295211792
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.398697853088379
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 11.98063850402832
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.213766098022461
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.971565246582031
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.111949920654297
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 13.111401557922363
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 41.4530029296875,
      "learning_rate": 1e-05,
      "loss": 12.2211,
      "step": 56
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.622699737548828
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 12.156288146972656
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.434711456298828
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 13.113154411315918
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 12.211792945861816
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 11.503585815429688
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 10.892461776733398
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 13.641779899597168
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 12.064449310302734
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 14.21044635772705
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 12.425569534301758
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 11.969542503356934
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 12.789684295654297
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 12.793673515319824
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 11.771599769592285
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 36.234397888183594
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 1420.3411865234375,
      "learning_rate": 1e-05,
      "loss": 13.8022,
      "step": 60
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.49308967590332
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.771706581115723
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 10.686327934265137
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.416329383850098
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 12.271880149841309
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 11.650886535644531
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 12.369511604309082
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 9.913251876831055
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 12.663847923278809
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 12.112873077392578
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 10.092921257019043
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 11.312943458557129
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 12.857704162597656
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.574960708618164
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 12.046663284301758
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.921147346496582
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 75.99407196044922,
      "learning_rate": 1e-05,
      "loss": 11.5098,
      "step": 64
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.450579643249512
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.541406631469727
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 12.942134857177734
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 12.004615783691406
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.192880630493164
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.413687705993652
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.061151504516602
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.230608940124512
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.248644828796387
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.5568265914917
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.653212547302246
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 10.51690673828125
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.898366928100586
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.121514320373535
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 10.499296188354492
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.558634757995605
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 139.97105407714844,
      "learning_rate": 1e-05,
      "loss": 11.6807,
      "step": 68
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 9.84030818939209
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 12.201244354248047
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 11.820385932922363
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 11.725385665893555
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 12.904593467712402
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 9.284343719482422
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.819629669189453
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.63996696472168
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 11.77999496459961
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 10.990220069885254
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 9.649593353271484
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 10.993184089660645
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 12.101469039916992
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 12.055069923400879
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 13.806293487548828
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 11.601532936096191
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 53.66135025024414,
      "learning_rate": 1e-05,
      "loss": 11.3883,
      "step": 72
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 12.781106948852539
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 12.000506401062012
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 11.054526329040527
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 11.048627853393555
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.667784690856934
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 12.172250747680664
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 12.238003730773926
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.078591346740723
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 11.856114387512207
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 10.336078643798828
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 12.70907974243164
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 11.49555778503418
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.698912620544434
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.984797477722168
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.84056568145752
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.607672691345215
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 14.534847259521484,
      "learning_rate": 1e-05,
      "loss": 11.7231,
      "step": 76
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 13.105883598327637
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 11.540433883666992
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 10.462786674499512
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 11.852398872375488
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 13.063292503356934
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 10.431804656982422
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 11.545136451721191
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 12.49875545501709
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 10.920513153076172
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.873916625976562
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.328883171081543
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.472853660583496
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 10.716094017028809
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 9.91590404510498
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 11.970865249633789
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 10.922883033752441
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 17.095088958740234,
      "learning_rate": 1e-05,
      "loss": 11.4764,
      "step": 80
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 12.132744789123535
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 10.863932609558105
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 11.483118057250977
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 12.577951431274414
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 11.609021186828613
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 10.501195907592773
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 12.02330493927002
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 11.25774097442627
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 12.602705955505371
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 12.730243682861328
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.242645263671875
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.385655403137207
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 11.50455379486084
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 11.97238826751709
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 12.154504776000977
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 10.986393928527832
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 33.50981521606445,
      "learning_rate": 1e-05,
      "loss": 11.5643,
      "step": 84
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 12.280348777770996
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.74482536315918
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.623929023742676
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.238550186157227
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 11.81783676147461
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.899602890014648
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.141265869140625
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 12.631092071533203
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.414007186889648
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 11.661026000976562
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.221563339233398
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.34593677520752
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.143449783325195
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.69706916809082
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 9.322813987731934
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 12.576661109924316
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 130.18362426757812,
      "learning_rate": 1e-05,
      "loss": 11.235,
      "step": 88
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 11.708223342895508
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 12.316985130310059
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 10.009742736816406
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 12.043660163879395
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 12.527496337890625
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 11.610227584838867
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 12.932027816772461
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 10.844712257385254
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 11.860650062561035
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 10.88185977935791
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 11.904728889465332
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 12.208636283874512
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 11.958331108093262
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 9.263877868652344
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 11.752960205078125
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 8.998052597045898
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 22.492517471313477,
      "learning_rate": 1e-05,
      "loss": 11.4264,
      "step": 92
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 11.136180877685547
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 10.843345642089844
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 11.757469177246094
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 12.19736099243164
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.040278434753418
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.467357635498047
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 11.77796745300293
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.463929176330566
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 10.93805980682373
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.678519248962402
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.139307975769043
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 10.885810852050781
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.857054710388184
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 8.778817176818848
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.219146728515625
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.679966926574707
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 63.60947799682617,
      "learning_rate": 1e-05,
      "loss": 11.0538,
      "step": 96
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.916865348815918
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 11.880400657653809
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 9.864355087280273
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.800646781921387
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 10.504168510437012
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.277281761169434
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.678836822509766
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.577343940734863
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 11.622729301452637
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 8.916601181030273
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 11.45644760131836
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 10.906023979187012
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 12.007176399230957
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 11.46556568145752
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 10.766195297241211
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 11.40876579284668
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 28.538280487060547,
      "learning_rate": 1e-05,
      "loss": 11.0656,
      "step": 100
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.849106788635254
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 11.25267219543457
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.875493049621582
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.81655216217041
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 12.866523742675781
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.47890853881836
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.623369216918945
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.938318252563477
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 8.597893714904785
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 10.821208000183105
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 11.524106979370117
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 10.418317794799805
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 11.360268592834473
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 11.503006935119629
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.96911334991455
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.988500595092773
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 41.238765716552734,
      "learning_rate": 1e-05,
      "loss": 10.7427,
      "step": 104
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.327693939208984
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.58430004119873
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.050786018371582
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 9.802963256835938
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 11.016083717346191
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 11.338485717773438
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 10.731383323669434
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 10.093250274658203
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 12.697294235229492
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 10.235753059387207
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 11.49268627166748
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 8.621170997619629
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.976311683654785
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 11.477538108825684
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.320906639099121
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 9.944619178771973
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 18.820316314697266,
      "learning_rate": 1e-05,
      "loss": 10.607,
      "step": 108
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.80138874053955
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.661816596984863
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 11.557169914245605
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 11.075803756713867
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 11.290813446044922
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 10.858915328979492
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 10.129401206970215
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 9.323092460632324
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.842066764831543
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 12.861568450927734
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.413304328918457
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 11.340656280517578
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.190388679504395
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.424149513244629
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.568138122558594
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.215180397033691
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 43.66325759887695,
      "learning_rate": 1e-05,
      "loss": 11.0346,
      "step": 112
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 11.245038032531738
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 11.173795700073242
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 9.451376914978027
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 12.407135009765625
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 11.287910461425781
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.341022491455078
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 9.15455150604248
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.13912296295166
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.365714073181152
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.81946849822998
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.130449295043945
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.584336280822754
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.04595947265625
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 12.092448234558105
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.615715980529785
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.749505996704102
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 20.692522048950195,
      "learning_rate": 1e-05,
      "loss": 10.6627,
      "step": 116
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.940417289733887
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.547669410705566
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 11.975298881530762
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 11.289423942565918
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.086949348449707
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 9.290812492370605
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.357450485229492
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.817875862121582
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 8.44282341003418
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 10.180169105529785
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 11.879542350769043
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 11.476774215698242
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 10.876595497131348
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 11.399162292480469
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 11.497428894042969
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 10.160965919494629
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 55.83879852294922,
      "learning_rate": 1e-05,
      "loss": 10.7012,
      "step": 120
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 10.489282608032227
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 11.236295700073242
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 9.995172500610352
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 9.580608367919922
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 12.118667602539062
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 10.24130630493164
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 10.9619779586792
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 12.208261489868164
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 10.963373184204102
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.035115242004395
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.566999435424805
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.98878288269043
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 10.727729797363281
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 10.703031539916992
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 11.225591659545898
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 9.971311569213867
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 36.33486557006836,
      "learning_rate": 1e-05,
      "loss": 10.9383,
      "step": 124
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 10.620291709899902
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.465699195861816
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.768173217773438
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 10.609957695007324
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 9.368616104125977
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 10.51439380645752
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 11.479109764099121
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 11.775002479553223
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 10.5986909866333
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 9.40007209777832
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 11.024740219116211
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 10.911218643188477
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 10.561436653137207
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 8.797490119934082
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 10.928735733032227
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 11.29634952545166
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 66.634033203125,
      "learning_rate": 1e-05,
      "loss": 10.445,
      "step": 128
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.015891075134277
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 8.866271018981934
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.25137710571289
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 11.149786949157715
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 10.193763732910156
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 9.491982460021973
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 10.5200777053833
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 11.410456657409668
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.918971061706543
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.610514640808105
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.881634712219238
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 11.019242286682129
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.056554794311523
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.070612907409668
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 12.080760955810547
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.679936408996582
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 19.20637321472168,
      "learning_rate": 1e-05,
      "loss": 10.5136,
      "step": 132
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 9.217036247253418
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.997283935546875
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.526914596557617
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.38538932800293
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 11.136190414428711
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 10.831467628479004
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 11.011555671691895
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 9.72938060760498
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 9.910465240478516
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 10.075799942016602
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 11.616181373596191
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 11.327160835266113
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 11.247900009155273
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 10.320304870605469
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 10.27843189239502
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 11.556682586669922
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 21.617870330810547,
      "learning_rate": 1e-05,
      "loss": 10.6355,
      "step": 136
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.068586349487305
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 10.574161529541016
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 11.008432388305664
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.473938941955566
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 9.296638488769531
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 11.094266891479492
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 11.563307762145996
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 10.037761688232422
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.979998588562012
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.117148399353027
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.430035591125488
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 9.893535614013672
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.175020217895508
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.050354957580566
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 11.28423023223877
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 8.82651138305664
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 105.61105346679688,
      "learning_rate": 1e-05,
      "loss": 10.1171,
      "step": 140
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 10.816479682922363
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.861920356750488
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.993489265441895
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.455035209655762
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 9.409661293029785
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 10.60765266418457
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 9.025524139404297
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 11.302472114562988
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 10.13127613067627
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 11.79173469543457
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 9.625326156616211
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 9.825491905212402
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 11.007796287536621
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 8.808599472045898
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 10.434618949890137
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 9.07772445678711
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 18.18276023864746,
      "learning_rate": 1e-05,
      "loss": 10.0734,
      "step": 144
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 11.359310150146484
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 9.533360481262207
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 10.016037940979004
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 8.480786323547363
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 11.618303298950195
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 9.270350456237793
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 10.656017303466797
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 10.925705909729004
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 10.453651428222656
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.224896430969238
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.673699378967285
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 11.773744583129883
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.927433013916016
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.232353210449219
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 11.061736106872559
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 10.58759593963623
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 42.537139892578125,
      "learning_rate": 1e-05,
      "loss": 10.2372,
      "step": 148
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 10.731697082519531
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.828227996826172
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.809300422668457
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 8.474058151245117
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.802331924438477
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 8.84597396850586
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.180809020996094
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 10.063895225524902
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.967555046081543
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.998435974121094
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.245038032531738
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 10.064384460449219
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 9.381134986877441
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.764579772949219
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 9.321680068969727
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 10.341085433959961
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 13.916402816772461,
      "learning_rate": 1e-05,
      "loss": 9.6138,
      "step": 152
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 9.398460388183594
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 11.251470565795898
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 8.56664752960205
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 11.114553451538086
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.300224304199219
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.244832992553711
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 8.961382865905762
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.608078956604004
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 8.850805282592773
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 9.36047077178955
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 11.810346603393555
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 9.960837364196777
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 10.651169776916504
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 9.161591529846191
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.582335472106934
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.877738952636719
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 37.48689651489258,
      "learning_rate": 1e-05,
      "loss": 10.0438,
      "step": 156
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 8.870003700256348
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 9.932001113891602
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 10.61023235321045
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 10.246857643127441
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.649466514587402
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 10.469298362731934
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 11.165931701660156
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 8.927701950073242
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.783921241760254
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 10.484967231750488
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 10.43018913269043
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.46003246307373
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 11.492266654968262
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 8.669312477111816
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 12.573001861572266
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 8.726821899414062
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 16.240442276000977,
      "learning_rate": 1e-05,
      "loss": 10.0933,
      "step": 160
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 10.525421142578125
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 10.460885047912598
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 8.369852066040039
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 9.092550277709961
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.987252235412598
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.822426795959473
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 8.282438278198242
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.070656776428223
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 9.763993263244629
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 10.434200286865234
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 8.992134094238281
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 7.936783790588379
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.92808723449707
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 10.293033599853516
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 9.982207298278809
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.977181434631348
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 28.31463050842285,
      "learning_rate": 1e-05,
      "loss": 9.4324,
      "step": 164
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 10.087586402893066
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 9.590227127075195
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 8.858109474182129
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 9.333183288574219
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.479607582092285
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 9.125449180603027
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.458152770996094
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.512654304504395
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 10.945270538330078
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 9.392878532409668
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 10.031336784362793
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 8.512808799743652
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.64546012878418
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 9.919219017028809
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.021720886230469
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.226263046264648
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 32.81133270263672,
      "learning_rate": 1e-05,
      "loss": 9.5087,
      "step": 168
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 10.195094108581543
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 10.200845718383789
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 9.468170166015625
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 9.669925689697266
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 11.13676643371582
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 9.729119300842285
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 10.373879432678223
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 8.572173118591309
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 11.096277236938477
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 8.103405952453613
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 8.225003242492676
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 9.464632987976074
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 12.195710182189941
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 11.018428802490234
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 9.171996116638184
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 8.251835823059082
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 110.4805679321289,
      "learning_rate": 1e-05,
      "loss": 9.8046,
      "step": 172
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.797334671020508
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 9.258408546447754
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.657556533813477
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 11.32192611694336
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.920894622802734
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 9.37264347076416
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 10.00223159790039
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.93328857421875
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.250020980834961
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.073384284973145
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 9.475135803222656
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.286503791809082
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.962199211120605
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.872245788574219
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 9.304991722106934
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 10.175762176513672
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 21.306596755981445,
      "learning_rate": 1e-05,
      "loss": 9.729,
      "step": 176
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.190937995910645
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 10.442930221557617
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 13.4029541015625
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.148096084594727
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 10.842682838439941
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 9.170015335083008
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 10.535585403442383
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 7.880204200744629
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 6.856189250946045
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.266321182250977
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.605414390563965
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 9.208160400390625
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.858125686645508
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.418610572814941
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.684006690979004
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 9.0328369140625
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 29.667078018188477,
      "learning_rate": 1e-05,
      "loss": 9.1589,
      "step": 180
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.18176555633545
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 10.007966041564941
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.42605972290039
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 10.04361343383789
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 7.873898029327393
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 10.032234191894531
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 9.170682907104492
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 9.39611530303955
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 8.157442092895508
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 8.899999618530273
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.822864532470703
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.374216079711914
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.258973121643066
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.333534240722656
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.597258567810059
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 8.082286834716797
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 27.6683349609375,
      "learning_rate": 1e-05,
      "loss": 9.2287,
      "step": 184
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 10.179617881774902
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 8.531780242919922
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 8.322684288024902
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 9.610331535339355
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 10.3333101272583
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.047547340393066
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 9.811383247375488
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 9.923904418945312
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 10.12647819519043
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 9.886507034301758
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 8.847984313964844
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 9.029501914978027
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 8.510956764221191
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 10.015551567077637
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 9.48879337310791
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 9.027856826782227
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 29.682771682739258,
      "learning_rate": 1e-05,
      "loss": 9.3559,
      "step": 188
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.501133918762207
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.207417488098145
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 8.88254165649414
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.142385482788086
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.77894115447998
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 9.200428009033203
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 9.45388126373291
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.236109733581543
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.099496841430664
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.502636909484863
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.73820972442627
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 10.241384506225586
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 8.114360809326172
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 8.252923965454102
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 9.643531799316406
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 7.751065254211426
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 10.694164276123047,
      "learning_rate": 1e-05,
      "loss": 8.8592,
      "step": 192
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 8.676260948181152
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 9.274267196655273
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 8.67809772491455
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 9.480671882629395
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 9.182045936584473
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 10.04173469543457
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 7.960485458374023
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 7.857387065887451
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 10.860186576843262
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 9.000139236450195
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.353124618530273
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.539379119873047
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.980469703674316
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.813726425170898
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.700019836425781
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 10.60743236541748
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 11.520845413208008,
      "learning_rate": 1e-05,
      "loss": 9.2503,
      "step": 196
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 9.195685386657715
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 10.527647018432617
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 9.044052124023438
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 8.94534969329834
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.66502571105957
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 10.28990650177002
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.749714851379395
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.974568367004395
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.593314170837402
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.412419319152832
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.98813533782959
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 10.883451461791992
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.441195487976074
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.741485595703125
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 8.38135814666748
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.792855262756348
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 16.712202072143555,
      "learning_rate": 1e-05,
      "loss": 9.2891,
      "step": 200
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 10.11628532409668
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.353677749633789
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.348518371582031
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 8.571377754211426
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 8.632287979125977
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.704370498657227
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.187885284423828
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.775745391845703
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 8.586414337158203
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 9.574522018432617
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 9.013510704040527
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 8.558547973632812
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.430643081665039
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.895831108093262
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 9.262554168701172
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.210737228393555
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 10.223176002502441,
      "learning_rate": 1e-05,
      "loss": 9.0764,
      "step": 204
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 9.912593841552734
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.235473155975342
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 9.521230697631836
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.95389986038208
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 9.561641693115234
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.780721664428711
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.284393310546875
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.517133712768555
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.965731143951416
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 10.25855541229248
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 9.10841178894043
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.545067310333252
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 9.758606910705566
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 7.658039093017578
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 9.027703285217285
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 8.982710838317871
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 26.47637939453125,
      "learning_rate": 1e-05,
      "loss": 8.7545,
      "step": 208
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.354823112487793
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.848957061767578
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.053898811340332
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.293992042541504
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.449030876159668
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 9.48731803894043
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 9.99634075164795
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.824938774108887
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.6552534103393555
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 8.244559288024902
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 10.653692245483398
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.64069128036499
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 9.28791332244873
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 7.922374725341797
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 8.96840763092041
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 7.673058032989502
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 9.75320053100586,
      "learning_rate": 1e-05,
      "loss": 8.6472,
      "step": 212
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 9.946268081665039
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 8.856410026550293
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 9.484140396118164
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 7.797473907470703
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 10.392521858215332
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 8.208340644836426
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 9.295947074890137
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 10.306452751159668
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 8.782776832580566
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 7.714079856872559
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 7.591038227081299
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 10.375707626342773
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 9.854559898376465
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 9.23068904876709
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 7.687963962554932
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 8.474515914916992
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 28.891124725341797,
      "learning_rate": 1e-05,
      "loss": 8.9999,
      "step": 216
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 8.45561695098877
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 10.410689353942871
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 10.676591873168945
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 9.136011123657227
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 8.221179008483887
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.840110778808594
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.715977668762207
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 8.515694618225098
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.378270149230957
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.848543167114258
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.912487983703613
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 7.844642639160156
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.270233154296875
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.082622528076172
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.622319221496582
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 9.690159797668457
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 13.985469818115234,
      "learning_rate": 1e-05,
      "loss": 8.7263,
      "step": 220
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 8.384882926940918
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 8.646834373474121
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 10.081990242004395
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 7.943273544311523
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.3005952835083
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 9.915096282958984
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.07335090637207
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.158790588378906
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 7.622870445251465
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 9.200005531311035
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 8.182999610900879
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 10.061273574829102
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 10.184999465942383
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 9.318503379821777
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 8.631250381469727
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 9.146003723144531
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 30.91859245300293,
      "learning_rate": 1e-05,
      "loss": 8.8658,
      "step": 224
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 9.274283409118652
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 8.896712303161621
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 7.998490810394287
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 9.344998359680176
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.142383575439453
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.975244522094727
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.391912460327148
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.618861198425293
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.598305702209473
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 9.665105819702148
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.434403419494629
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 9.732810974121094
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 8.558923721313477
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 7.901880741119385
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 9.830090522766113
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 9.125487327575684
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 8.107732772827148,
      "learning_rate": 1e-05,
      "loss": 8.8431,
      "step": 228
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 7.939123630523682
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.543916702270508
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 9.789125442504883
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 9.612396240234375
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.476707458496094
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 7.921596050262451
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 9.539976119995117
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.591914176940918
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 7.50105619430542
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.96015739440918
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 9.374116897583008
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.813368797302246
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 9.482199668884277
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 9.239583969116211
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 8.053783416748047
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 10.249286651611328
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 11.678272247314453,
      "learning_rate": 1e-05,
      "loss": 8.8805,
      "step": 232
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 7.882024765014648
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.221413612365723
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 7.750812530517578
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.95352840423584
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.54931926727295
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.656803607940674
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.269474029541016
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.393102645874023
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.427395343780518
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.818050384521484
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 8.138447761535645
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 8.382891654968262
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 7.860915184020996
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.054132461547852
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.726097106933594
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.315268516540527
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 11.049824714660645,
      "learning_rate": 1e-05,
      "loss": 8.15,
      "step": 236
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 10.31949234008789
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 9.61026382446289
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 7.917852878570557
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 8.416048049926758
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 9.7183198928833
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 10.184354782104492
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.743063449859619
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 9.641520500183105
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.718070983886719
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 7.9094977378845215
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.947247505187988
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.86545467376709
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 7.044713973999023
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 9.160497665405273
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 8.449448585510254
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 8.300745010375977
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 43.70433044433594,
      "learning_rate": 1e-05,
      "loss": 8.8092,
      "step": 240
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 8.358410835266113
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 10.699419021606445
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 9.10665225982666
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 8.443190574645996
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 7.338542938232422
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 9.145149230957031
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 8.403533935546875
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 8.446634292602539
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 9.66707706451416
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 8.744996070861816
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 7.067841053009033
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 8.54128646850586
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 9.081012725830078
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 9.884233474731445
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 10.015748977661133
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 10.199177742004395
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 33.50074768066406,
      "learning_rate": 1e-05,
      "loss": 8.9464,
      "step": 244
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 10.996451377868652
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 9.463934898376465
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 8.961787223815918
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 10.656190872192383
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 8.267074584960938
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 7.363171100616455
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 8.088752746582031
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 9.666877746582031
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.65302562713623
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.880585670471191
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 9.727646827697754
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.384279251098633
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 9.773380279541016
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 8.494054794311523
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 8.441964149475098
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 6.763637542724609
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 11.865252494812012,
      "learning_rate": 1e-05,
      "loss": 8.9114,
      "step": 248
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 10.10632038116455
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 9.045766830444336
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.85205078125
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.961782455444336
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.286725044250488
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 7.879993915557861
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.477680206298828
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.633960723876953
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 8.268553733825684
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 9.9968900680542
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 8.4429349899292
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 7.772231101989746
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.894099235534668
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 9.401931762695312
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.762146949768066
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.861817359924316
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 9.076627731323242,
      "learning_rate": 1e-05,
      "loss": 8.6653,
      "step": 252
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.517682075500488
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.686667442321777
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.751819610595703
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.28868579864502
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.862929821014404
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 8.378758430480957
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 8.25780200958252
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.967566967010498
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 8.222722053527832
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 7.03478479385376
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 7.757225513458252
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 9.479537010192871
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 8.215774536132812
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 8.08840560913086
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 10.228283882141113
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 7.293333053588867
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 8.850709915161133,
      "learning_rate": 1e-05,
      "loss": 8.3145,
      "step": 256
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.041707992553711
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.700944423675537
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 8.701765060424805
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 6.91071081161499
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 10.052162170410156
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 9.622241973876953
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 7.940246105194092
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 7.633832931518555
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 8.178584098815918
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.704583168029785
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 10.439787864685059
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 9.34423828125
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 10.4271821975708
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 8.957082748413086
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 7.539772987365723
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 9.143678665161133
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 38.594356536865234,
      "learning_rate": 1e-05,
      "loss": 8.5837,
      "step": 260
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 8.149690628051758
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 7.366560459136963
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 7.213303089141846
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 8.973657608032227
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 8.569131851196289
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.117707252502441
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.85753059387207
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.938337802886963
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 7.262900352478027
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.245474815368652
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.04386043548584
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.338813781738281
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 9.358623504638672
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 8.891729354858398
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 7.804759502410889
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 7.24848747253418
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 11.721390724182129,
      "learning_rate": 1e-05,
      "loss": 8.0238,
      "step": 264
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 6.571340084075928
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 9.408628463745117
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 7.950092792510986
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 8.225046157836914
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 8.839768409729004
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 9.584136962890625
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 8.074234008789062
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 9.556145668029785
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 9.041784286499023
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 6.968522071838379
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 10.353960990905762
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 6.97034215927124
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 9.393511772155762
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 8.863360404968262
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 9.166291236877441
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 7.412930011749268
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 11.307465553283691,
      "learning_rate": 1e-05,
      "loss": 8.5238,
      "step": 268
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 7.6980109214782715
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 9.004973411560059
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 8.233198165893555
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 10.546204566955566
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 9.321991920471191
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.262923240661621
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.829191207885742
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.149827003479004
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 8.96813678741455
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 10.8367280960083
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 8.708678245544434
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 7.612382411956787
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.97789192199707
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 8.204960823059082
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 9.16907787322998
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.610901832580566
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 7.192559719085693,
      "learning_rate": 1e-05,
      "loss": 8.6959,
      "step": 272
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.211956024169922
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 7.143931865692139
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.156781196594238
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.081438064575195
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.708268642425537
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.833703517913818
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 6.895991325378418
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 8.319313049316406
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 7.098052978515625
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 8.255372047424316
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 9.610295295715332
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 10.054465293884277
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.725369453430176
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 9.0997896194458
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 9.698041915893555
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.835655689239502
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 19.41514778137207,
      "learning_rate": 1e-05,
      "loss": 8.233,
      "step": 276
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 7.349086761474609
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 6.936474800109863
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 8.334657669067383
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 9.017963409423828
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 7.852164268493652
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 9.126914978027344
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 8.692806243896484
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 8.533787727355957
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.115591049194336
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.073673248291016
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.061239242553711
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.20637035369873
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.9252729415893555
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.285467147827148
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 9.436643600463867
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 8.289361953735352
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 9.630855560302734,
      "learning_rate": 1e-05,
      "loss": 8.2023,
      "step": 280
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 10.012991905212402
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 9.455740928649902
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 10.257798194885254
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 7.811270713806152
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 7.397870063781738
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 6.418354034423828
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 7.796180248260498
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 8.585938453674316
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.678459644317627
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.226465225219727
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 8.83869457244873
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.971148490905762
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.321772575378418
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 7.98628568649292
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.684231758117676
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 7.382626056671143
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 6.444254398345947,
      "learning_rate": 1e-05,
      "loss": 8.2391,
      "step": 284
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 8.660303115844727
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 8.256440162658691
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 7.905787467956543
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 7.719996452331543
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.996413707733154
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.702603340148926
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 8.574524879455566
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.450150966644287
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.744946479797363
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.993914604187012
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.456974983215332
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.539258003234863
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 9.539827346801758
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 8.30477237701416
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 8.330719947814941
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 9.51285171508789
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 10.49563980102539,
      "learning_rate": 1e-05,
      "loss": 8.4181,
      "step": 288
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 6.713016033172607
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 7.813157081604004
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 8.828655242919922
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 8.889424324035645
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.16108226776123
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.891965866088867
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.412369728088379
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 9.242706298828125
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.443171977996826
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 5.8830485343933105
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.293688774108887
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 8.267038345336914
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 9.025017738342285
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 7.4852495193481445
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 6.731475830078125
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 9.166031837463379
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 26.53466796875,
      "learning_rate": 1e-05,
      "loss": 8.0154,
      "step": 292
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 8.389669418334961
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 7.2123308181762695
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 9.341156005859375
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 6.903926849365234
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.49953842163086
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.782790184020996
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 7.926547527313232
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.60677433013916
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.980974197387695
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 9.126267433166504
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.926476001739502
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.389068603515625
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.144017219543457
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.55170726776123
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.619813919067383
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.3454813957214355
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 6.517373085021973,
      "learning_rate": 1e-05,
      "loss": 8.1092,
      "step": 296
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.87939453125
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 7.502101421356201
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.395060539245605
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.441081047058105
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.889405727386475
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.735417366027832
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.449061870574951
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 8.367488861083984
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 7.981578350067139
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 6.923366069793701
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 8.064813613891602
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 7.18273401260376
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 8.516828536987305
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.837547302246094
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.324316024780273
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.213824272155762
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 14.532815933227539,
      "learning_rate": 1e-05,
      "loss": 7.8565,
      "step": 300
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 8.252074241638184
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 7.6087260246276855
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 6.487219333648682
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 6.677879810333252
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 7.815026760101318
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 9.154719352722168
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 7.915988445281982
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 8.312129020690918
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 10.032352447509766
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 7.317715167999268
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 8.003211975097656
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 8.646702766418457
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.280012607574463
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.074512481689453
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 8.770028114318848
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 8.169517517089844
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 6.991789817810059,
      "learning_rate": 1e-05,
      "loss": 7.9699,
      "step": 304
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 7.910707950592041
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 8.851441383361816
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 8.706559181213379
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 6.3202643394470215
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.877843379974365
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 9.840760231018066
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 8.974129676818848
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.76145076751709
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 7.7866973876953125
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 8.774632453918457
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 6.816519260406494
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 8.052779197692871
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.340723991394043
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 10.074390411376953
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.010220527648926
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 7.8430352210998535
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 14.068233489990234,
      "learning_rate": 1e-05,
      "loss": 8.2464,
      "step": 308
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 6.388254642486572
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.852964401245117
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.737032413482666
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.142114639282227
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.610208511352539
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.9658942222595215
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 6.273921489715576
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 8.789116859436035
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 8.110419273376465
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.327016830444336
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.846200942993164
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.474301338195801
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 8.066723823547363
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.077022075653076
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.682167053222656
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.381542205810547
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 9.920146942138672,
      "learning_rate": 1e-05,
      "loss": 7.5453,
      "step": 312
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 8.33939266204834
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 6.895317554473877
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 8.396190643310547
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 7.400044918060303
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.516083717346191
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.807485580444336
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.003668785095215
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.768014430999756
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.698553085327148
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 8.067449569702148
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.569189071655273
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 6.929469108581543
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 7.595730304718018
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.766096115112305
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.365006446838379
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.457813262939453
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 13.564172744750977,
      "learning_rate": 1e-05,
      "loss": 7.9735,
      "step": 316
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 7.869708061218262
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 8.249859809875488
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 7.330406188964844
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 8.050220489501953
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 9.197529792785645
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 7.8260393142700195
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 6.945930004119873
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 6.705056190490723
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 6.9135870933532715
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 8.502147674560547
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 7.929758071899414
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 6.8894548416137695
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 7.837393283843994
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 6.301910400390625
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 8.650349617004395
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 8.492217063903809
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 6.831161975860596,
      "learning_rate": 1e-05,
      "loss": 7.7307,
      "step": 320
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.002568244934082
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.825201988220215
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.719254493713379
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 8.011031150817871
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 9.093493461608887
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 6.611315727233887
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 7.274296760559082
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 7.970090866088867
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.148209571838379
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.954981327056885
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 8.5009126663208
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.443122386932373
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.160701274871826
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.467447757720947
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 8.878803253173828
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.319275856018066
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 12.464693069458008,
      "learning_rate": 1e-05,
      "loss": 7.7113,
      "step": 324
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 8.202373504638672
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 8.009256362915039
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 7.268817901611328
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 6.240869998931885
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.876509666442871
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.650528907775879
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 7.716962814331055
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.397171974182129
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.445191383361816
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.716911792755127
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 8.72461223602295
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 5.966598033905029
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.920076847076416
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.894033908843994
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 7.85788106918335
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 7.540774822235107
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 7.278809070587158,
      "learning_rate": 1e-05,
      "loss": 7.6518,
      "step": 328
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.3588995933532715
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.230800151824951
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 9.12582015991211
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 6.536040306091309
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.049560070037842
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.585072040557861
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 6.499294757843018
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.185570240020752
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 8.560257911682129
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.9662089347839355
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.9150896072387695
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.782442092895508
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 6.808966636657715
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 7.0312652587890625
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 8.256035804748535
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 5.9824957847595215
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 27.485139846801758,
      "learning_rate": 1e-05,
      "loss": 7.4296,
      "step": 332
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 7.996812343597412
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 8.52172565460205
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 6.970703601837158
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 8.100028991699219
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 6.653751850128174
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 6.863412857055664
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 8.512393951416016
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 8.101210594177246
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.219964981079102
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.555787086486816
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 6.899551868438721
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.902730941772461
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.157779693603516
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 7.2505292892456055
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.7435941696167
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.325323104858398
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 9.657209396362305,
      "learning_rate": 1e-05,
      "loss": 7.9235,
      "step": 336
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 10.248441696166992
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 9.168166160583496
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 6.4800028800964355
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 7.3843536376953125
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 7.292364597320557
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 7.466684341430664
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 8.085734367370605
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 6.877345085144043
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 7.6778740882873535
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 8.524276733398438
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 5.903565883636475
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 8.051684379577637
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 8.170751571655273
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 8.803227424621582
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 6.672534465789795
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 6.787819862365723
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 11.158797264099121,
      "learning_rate": 1e-05,
      "loss": 7.7247,
      "step": 340
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 8.05754566192627
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.328925609588623
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.325913429260254
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 10.541116714477539
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.610118865966797
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.334056854248047
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.565546035766602
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 8.530682563781738
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 7.741385459899902
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 8.623273849487305
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 8.464166641235352
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 7.518078327178955
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.212327003479004
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.203678131103516
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 6.548776149749756
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.018510818481445
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 12.790303230285645,
      "learning_rate": 1e-05,
      "loss": 7.9765,
      "step": 344
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 6.895763397216797
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 6.724978923797607
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.96275520324707
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.274966239929199
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 10.541850090026855
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 8.429983139038086
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 6.944403171539307
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.4602251052856445
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 7.646166801452637
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 8.556116104125977
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 7.953361988067627
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 10.278975486755371
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 7.50942325592041
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 8.2050199508667
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 8.435586929321289
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 7.581918239593506
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 8.359407424926758,
      "learning_rate": 1e-05,
      "loss": 8.0251,
      "step": 348
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.187657356262207
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 8.220602989196777
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.714519023895264
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 6.87969446182251
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 5.847953796386719
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.259443283081055
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.641909599304199
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.88498067855835
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.256746768951416
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 8.373141288757324
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.68518590927124
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 6.997099876403809
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 8.21064281463623
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 6.817462921142578
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 8.52826976776123
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 7.30765962600708
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 8.219947814941406,
      "learning_rate": 1e-05,
      "loss": 7.4883,
      "step": 352
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.689026355743408
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.617109775543213
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.739031791687012
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.603020668029785
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 6.514982223510742
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 7.027559280395508
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 7.732449054718018
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 8.451412200927734
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 7.511340141296387
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 8.37960147857666
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 8.750621795654297
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 9.927215576171875
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 6.567143440246582
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 8.477529525756836
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 7.915719032287598
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 6.968222141265869
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 38.21114730834961,
      "learning_rate": 1e-05,
      "loss": 7.8045,
      "step": 356
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.177818775177002
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 7.300991058349609
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.489441871643066
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.3603081703186035
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 8.500944137573242
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 8.282540321350098
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.954254150390625
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.281638145446777
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 8.158886909484863
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 8.06462287902832
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 7.806986331939697
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 7.653802394866943
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.382436275482178
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.428923606872559
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.367988109588623
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.514551639556885
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 8.67624568939209,
      "learning_rate": 1e-05,
      "loss": 7.3579,
      "step": 360
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.569649696350098
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.19333028793335
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.599008083343506
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 8.926871299743652
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 8.505086898803711
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 8.26209545135498
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 7.934647560119629
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 6.3065505027771
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.9667558670043945
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.28446102142334
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 8.471291542053223
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 6.672237396240234
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 8.080378532409668
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 8.164806365966797
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 7.377711772918701
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 9.048792839050293
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 8.28503704071045,
      "learning_rate": 1e-05,
      "loss": 7.8352,
      "step": 364
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.037072658538818
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 8.051529884338379
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.193315505981445
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 8.116616249084473
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 8.625510215759277
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.75101375579834
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 6.594483852386475
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.566758155822754
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 6.676406383514404
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 5.961306095123291
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 6.858781814575195
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 7.72995662689209
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 7.304133892059326
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 8.491721153259277
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 7.835065841674805
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 8.217796325683594
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 6.4822468757629395,
      "learning_rate": 1e-05,
      "loss": 7.5007,
      "step": 368
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 8.482872009277344
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 7.724673271179199
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 7.986774921417236
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 8.917150497436523
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.199212074279785
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.338844299316406
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 7.4475178718566895
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 7.746762752532959
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 8.658126831054688
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.789925575256348
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.565216541290283
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.844878673553467
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.409068584442139
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.5737810134887695
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 9.6215181350708
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.220926284790039
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 7.346324920654297,
      "learning_rate": 1e-05,
      "loss": 8.033,
      "step": 372
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 6.772524833679199
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 8.599050521850586
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 7.914992332458496
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 8.557478904724121
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 7.507643699645996
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 8.550390243530273
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 6.837709903717041
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 7.908990383148193
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 6.894425868988037
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.860786437988281
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 10.20931625366211
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.11106538772583
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 6.464744567871094
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 7.5650482177734375
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 6.485140323638916
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 8.6277494430542
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 6.913656234741211,
      "learning_rate": 1e-05,
      "loss": 7.7417,
      "step": 376
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.382986068725586
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.160738468170166
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 5.765810966491699
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 8.376479148864746
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.711132049560547
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.837111473083496
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.531510353088379
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.058841228485107
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.356544017791748
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.759592056274414
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 6.572195529937744
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.247291088104248
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 8.073227882385254
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 7.549444675445557
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 6.55302619934082
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 8.655436515808105
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 7.607263565063477,
      "learning_rate": 1e-05,
      "loss": 7.287,
      "step": 380
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.948608875274658
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 6.852585315704346
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.866298675537109
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.278505325317383
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.752599239349365
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 7.490969657897949
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.140195369720459
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.640631198883057
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 6.846495628356934
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.44287633895874
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.46273136138916
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.740181922912598
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.432460308074951
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 6.514998435974121
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.7037811279296875
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 8.572640419006348
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 34.64687728881836,
      "learning_rate": 1e-05,
      "loss": 7.2929,
      "step": 384
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.393444061279297
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 6.809739589691162
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 8.354009628295898
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.224018573760986
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 7.06647253036499
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 8.077034950256348
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.546486854553223
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.608115196228027
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 8.464731216430664
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.66046667098999
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.584188938140869
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.831490993499756
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.013270854949951
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 6.784848690032959
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 6.726080417633057
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.718171119689941
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 7.433840751647949,
      "learning_rate": 1e-05,
      "loss": 7.3664,
      "step": 388
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 6.544736862182617
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 8.203983306884766
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 7.465991020202637
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 8.106051445007324
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.131522178649902
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.356942176818848
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.284195899963379
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.43376636505127
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 6.587597370147705
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.629831790924072
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.891706943511963
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 8.281379699707031
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.473642349243164
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.356862545013428
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.434154510498047
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.545454025268555
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 8.564620971679688,
      "learning_rate": 1e-05,
      "loss": 7.733,
      "step": 392
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.493045806884766
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.530567646026611
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 6.9867377281188965
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.497313022613525
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.54642915725708
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.227397918701172
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 6.083568572998047
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.5797858238220215
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.190927505493164
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.804437637329102
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 6.25315523147583
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 8.14346981048584
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 8.10193920135498
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 8.656279563903809
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.551058769226074
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.372847557067871
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 8.214591026306152,
      "learning_rate": 1e-05,
      "loss": 7.4387,
      "step": 396
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.052064895629883
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.301991939544678
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.68660306930542
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.6993088722229
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.585814476013184
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 5.526637554168701
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 8.026785850524902
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.8537092208862305
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 7.7518157958984375
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 8.07005500793457
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 8.92111873626709
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 6.033825874328613
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 6.670497417449951
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 7.326803207397461
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 5.786319255828857
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 4.986851692199707
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 8.211631774902344,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 400
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 8.16402530670166
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 6.480024814605713
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 7.280276775360107
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 8.154717445373535
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.655904293060303
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.352047443389893
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 5.682362079620361
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 8.11989974975586
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 8.387131690979004
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 9.53703784942627
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 6.655045509338379
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 7.705557823181152
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 7.438412189483643
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 8.17999267578125
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 7.171300888061523
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 6.299173831939697
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 7.1993303298950195,
      "learning_rate": 1e-05,
      "loss": 7.5164,
      "step": 404
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.641836166381836
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.117701053619385
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 8.085641860961914
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 6.889163970947266
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 6.277770519256592
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.646060943603516
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.830228328704834
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 8.776656150817871
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 8.884283065795898
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.368470668792725
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 8.663987159729004
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.82201623916626
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 6.9849371910095215
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.868908882141113
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.82205867767334
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 6.70831823348999
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 8.574630737304688,
      "learning_rate": 1e-05,
      "loss": 7.6493,
      "step": 408
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.717168807983398
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.876805305480957
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.386096000671387
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 6.769972324371338
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.8474907875061035
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 7.430103302001953
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 8.60749340057373
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.915833473205566
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 6.886184215545654
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 7.472280502319336
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 8.016729354858398
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 6.445272445678711
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 7.496355056762695
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 6.299161434173584
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 8.038983345031738
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 7.582717418670654
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 7.722811222076416,
      "learning_rate": 1e-05,
      "loss": 7.3618,
      "step": 412
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 6.286516189575195
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.193554401397705
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 6.946856498718262
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.37235689163208
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.574662685394287
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 8.490656852722168
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.368077754974365
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.071966171264648
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 8.683417320251465
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.295628547668457
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.127044677734375
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.379115104675293
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 6.642946720123291
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 5.8854169845581055
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 7.564972400665283
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 8.172124862670898
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 9.949014663696289,
      "learning_rate": 1e-05,
      "loss": 7.316,
      "step": 416
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.642681121826172
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.551724433898926
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 8.745495796203613
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.428471565246582
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 7.171889781951904
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 5.769639492034912
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 7.432443618774414
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 8.923596382141113
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 6.7726359367370605
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 8.836204528808594
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 6.804697036743164
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 7.142188549041748
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 9.05344295501709
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 5.8570780754089355
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 8.59079360961914
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 7.467078685760498
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 10.086739540100098,
      "learning_rate": 1e-05,
      "loss": 7.5744,
      "step": 420
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.523290634155273
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 9.388203620910645
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.851738452911377
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 8.036615371704102
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 8.924742698669434
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 8.338311195373535
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 7.435419082641602
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 9.776920318603516
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.931642055511475
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 8.37321662902832
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 6.431545734405518
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.484476089477539
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.720913410186768
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.32297945022583
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 9.89228630065918
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.917448043823242
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 13.031113624572754,
      "learning_rate": 1e-05,
      "loss": 8.1469,
      "step": 424
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 7.06943416595459
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 7.262650966644287
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 8.45206069946289
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 6.875007629394531
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 7.519100189208984
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 7.1032538414001465
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 5.48206901550293
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 6.675551891326904
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 8.335246086120605
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 6.578075408935547
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 8.709754943847656
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 7.48773717880249
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 8.329510688781738
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.336324691772461
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.861161708831787
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.82258939743042
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 6.152872562408447,
      "learning_rate": 1e-05,
      "loss": 7.4312,
      "step": 428
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.716245651245117
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 6.747432708740234
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.629819869995117
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 8.541764259338379
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.039007186889648
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 8.476766586303711
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.546884536743164
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.051605224609375
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.005039215087891
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 8.787872314453125
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 6.768316745758057
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.395030498504639
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.614141941070557
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 7.476695537567139
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 8.085238456726074
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.956167697906494
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 7.150528430938721,
      "learning_rate": 1e-05,
      "loss": 7.4899,
      "step": 432
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 6.2686638832092285
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 6.817713737487793
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.953629016876221
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.653858661651611
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.211630821228027
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.571349620819092
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.674591541290283
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 8.07883071899414
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.360065937042236
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 8.174786567687988
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 8.172672271728516
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.628303050994873
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.551377296447754
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.402233600616455
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.185695171356201
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.558728218078613
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 7.155255317687988,
      "learning_rate": 1e-05,
      "loss": 7.5165,
      "step": 436
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.7870097160339355
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 6.7949018478393555
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.516891956329346
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.011209487915039
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 7.368472099304199
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 8.414665222167969
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 8.271146774291992
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 7.160569667816162
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.261310577392578
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 7.390544414520264
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.799598693847656
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.128928184509277
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 7.6385111808776855
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 7.949865341186523
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.722685813903809
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.048281669616699
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 9.056074142456055,
      "learning_rate": 1e-05,
      "loss": 7.579,
      "step": 440
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 7.325395584106445
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 6.809396743774414
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 7.93270206451416
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 5.876395225524902
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 8.715688705444336
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 8.271340370178223
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.611556053161621
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.576800346374512
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 7.465314865112305
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 6.9765095710754395
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 7.722802639007568
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 5.882603645324707
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.558326721191406
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 8.456668853759766
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.297536373138428
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 8.358458518981934
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 8.361883163452148,
      "learning_rate": 1e-05,
      "loss": 7.4898,
      "step": 444
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.301019668579102
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.562089920043945
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.377427577972412
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.837671756744385
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 8.031326293945312
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 7.766227722167969
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 8.1034574508667
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 6.900975704193115
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 6.8105316162109375
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.705560207366943
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.321799278259277
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.985131740570068
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 8.134916305541992
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.9979987144470215
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 8.813652038574219
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.7494001388549805
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 7.949248790740967,
      "learning_rate": 1e-05,
      "loss": 7.7124,
      "step": 448
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.6748366355896
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 9.430197715759277
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.950160503387451
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.608154296875
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.827464580535889
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.335095405578613
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 6.692455291748047
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.753864288330078
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.910593032836914
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 7.026668548583984
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.699347972869873
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 8.73515510559082
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 5.924276351928711
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 7.194492816925049
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 6.815627574920654
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 8.569952011108398
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 9.376100540161133,
      "learning_rate": 1e-05,
      "loss": 7.3218,
      "step": 452
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.071786403656006
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 8.500837326049805
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.616519927978516
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.41066312789917
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.498581409454346
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.662198066711426
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 8.533330917358398
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.826202869415283
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 5.781799793243408
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 7.796501636505127
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 7.433226585388184
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 8.162298202514648
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.141223907470703
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 8.053763389587402
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.651839256286621
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 7.9693827629089355
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 7.522998332977295,
      "learning_rate": 1e-05,
      "loss": 7.5069,
      "step": 456
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.886975288391113
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.430150032043457
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.33018159866333
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 6.763383388519287
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.151362895965576
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 8.488028526306152
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.582300662994385
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.7537641525268555
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.809164047241211
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.224438190460205
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.036888599395752
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 6.6881103515625
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 7.0610270500183105
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.495622634887695
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.982619285583496
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 8.471846580505371
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 7.548433780670166,
      "learning_rate": 1e-05,
      "loss": 7.3847,
      "step": 460
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.492752552032471
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.414783954620361
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 6.958139419555664
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.523969650268555
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 7.313895225524902
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 9.326557159423828
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 6.414394855499268
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 8.158541679382324
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 8.524713516235352
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 7.452775955200195
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 6.711016654968262
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 6.0021891593933105
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 7.351870059967041
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 8.341731071472168
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 6.927850246429443
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 8.103446960449219
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 8.838705062866211,
      "learning_rate": 1e-05,
      "loss": 7.5012,
      "step": 464
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 7.5359787940979
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 7.917109489440918
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 8.253795623779297
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 5.830416679382324
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 7.173283100128174
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 6.86555814743042
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 8.2816743850708
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 7.030264377593994
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.218795299530029
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 8.179518699645996
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 6.069608211517334
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.452979564666748
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.952457427978516
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.3262481689453125
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.294139862060547
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 8.272405624389648
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 8.8307466506958,
      "learning_rate": 1e-05,
      "loss": 7.4159,
      "step": 468
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.995546817779541
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.348943710327148
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.752555847167969
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.205842018127441
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 5.3938069343566895
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 8.541333198547363
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 6.5666656494140625
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 8.063101768493652
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.871635913848877
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 6.627650260925293
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.3707194328308105
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 8.60824203491211
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 7.370887756347656
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 8.135141372680664
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 6.571417808532715
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 8.569228172302246
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 5.495779514312744,
      "learning_rate": 1e-05,
      "loss": 7.4995,
      "step": 472
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.3555755615234375
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.581902027130127
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.661810398101807
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 8.451849937438965
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.544458389282227
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.733592510223389
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 8.055648803710938
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 6.366407871246338
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 8.399275779724121
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.604538440704346
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.138541221618652
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.221652030944824
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.910350322723389
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.670030117034912
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.891989707946777
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.760045051574707
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 7.496448040008545,
      "learning_rate": 1e-05,
      "loss": 7.5217,
      "step": 476
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 5.98838472366333
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 6.972364902496338
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 8.017916679382324
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 6.72564697265625
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.107974529266357
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.51851749420166
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 8.02220630645752
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 6.731963157653809
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 7.734429836273193
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 7.6751532554626465
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 8.270689964294434
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 8.36730670928955
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.998497009277344
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.339462280273438
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.456426620483398
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 6.2014360427856445
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 25.53122901916504,
      "learning_rate": 1e-05,
      "loss": 7.5705,
      "step": 480
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 6.7765889167785645
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 5.939384937286377
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 7.431704044342041
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 7.0311503410339355
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 7.85010290145874
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 8.657299995422363
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 7.776172637939453
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 8.400893211364746
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.41817045211792
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 5.291060447692871
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 6.7957353591918945
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.213968753814697
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.194128513336182
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 8.623279571533203
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.415239334106445
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.158830642700195
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 8.475302696228027,
      "learning_rate": 1e-05,
      "loss": 7.3109,
      "step": 484
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 6.010516166687012
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 6.4728193283081055
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.426761150360107
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.637828350067139
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.0229811668396
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.485072612762451
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.580550193786621
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 8.183653831481934
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 8.61182975769043
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.392693519592285
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.6934428215026855
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.78805685043335
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 6.823986530303955
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 5.321688652038574
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 8.120028495788574
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 6.619423866271973
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 9.314112663269043,
      "learning_rate": 1e-05,
      "loss": 7.262,
      "step": 488
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 6.464946746826172
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.431856632232666
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 6.49228572845459
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.5033674240112305
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 5.852793216705322
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 7.260061740875244
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 7.412514686584473
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 6.1333746910095215
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.116766452789307
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 6.125434398651123
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 6.8508477210998535
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.747311115264893
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 8.02690601348877
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.734346866607666
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.265091896057129
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.37477445602417
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 6.400746822357178,
      "learning_rate": 1e-05,
      "loss": 7.0495,
      "step": 492
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 7.530344009399414
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.486377716064453
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.711554527282715
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 8.266678810119629
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.165747165679932
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 6.487946033477783
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.897132396697998
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.752471923828125
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 8.042768478393555
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 7.82958984375
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 8.048827171325684
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 7.088045120239258
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.428301811218262
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.58991003036499
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.183356285095215
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.424529075622559
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 5.952877044677734,
      "learning_rate": 1e-05,
      "loss": 7.4333,
      "step": 496
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.634365558624268
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.395888328552246
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 6.1642680168151855
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 11.371374130249023
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.7658867835998535
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.91407585144043
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.287159442901611
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.620785713195801
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 6.069972515106201
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 6.981333255767822
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 8.453526496887207
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 7.676165580749512
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.617778778076172
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.226446628570557
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 6.792215347290039
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.699793338775635
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 6.784480571746826,
      "learning_rate": 1e-05,
      "loss": 7.6044,
      "step": 500
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.936556816101074
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 7.314278602600098
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.333626747131348
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 7.541306018829346
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.859274864196777
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.090661525726318
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.179004192352295
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.451000690460205
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 8.14928150177002
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 6.634354114532471
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 8.010232925415039
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 6.803582668304443
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.186071395874023
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.031855583190918
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 9.090021133422852
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.947539329528809
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 10.279557228088379,
      "learning_rate": 1e-05,
      "loss": 7.2224,
      "step": 504
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 7.197842121124268
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 5.550083637237549
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 7.927826881408691
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 8.098876953125
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 7.544885158538818
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 5.2315802574157715
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 6.434506416320801
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 7.8484296798706055
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 7.890492916107178
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 6.7303290367126465
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 8.096400260925293
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 8.270633697509766
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 6.146450519561768
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 6.086196422576904
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 7.130472183227539
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 9.620548248291016
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 11.04758358001709,
      "learning_rate": 1e-05,
      "loss": 7.2378,
      "step": 508
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.292830467224121
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.308938503265381
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 8.264904975891113
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.9970855712890625
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.1894073486328125
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.078858852386475
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.143139362335205
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.0466766357421875
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.1531219482421875
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.736265659332275
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.479001522064209
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.206132411956787
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 6.567325115203857
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 9.430864334106445
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 7.865579605102539
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 7.944159984588623
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 10.99915599822998,
      "learning_rate": 1e-05,
      "loss": 7.544,
      "step": 512
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.875278472900391
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.499728679656982
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.56744384765625
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 6.49501895904541
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.927114486694336
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.889367580413818
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.124241352081299
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.091158390045166
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.612726211547852
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.023258686065674
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.678445339202881
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 6.276734352111816
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.79238748550415
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.798552989959717
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 8.04211711883545
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.001851558685303
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 6.438345432281494,
      "learning_rate": 1e-05,
      "loss": 7.4185,
      "step": 516
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.143211841583252
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 9.173112869262695
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.394917011260986
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.45618200302124
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 7.865028381347656
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 8.293980598449707
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.5899176597595215
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.7772955894470215
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 7.769937992095947
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 7.875511169433594
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.373845100402832
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.6106672286987305
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 8.324528694152832
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 7.267722129821777
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 6.403731822967529
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 7.540292739868164
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 9.208507537841797,
      "learning_rate": 1e-05,
      "loss": 7.4287,
      "step": 520
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.188186168670654
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 7.074416637420654
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.974092483520508
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 7.678687572479248
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 6.949998378753662
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.662763595581055
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.934101581573486
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.031049728393555
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 8.691899299621582
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.6412129402160645
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.41140079498291
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.246796131134033
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.197312831878662
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.531804084777832
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 8.101283073425293
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 6.982879638671875
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 7.2206597328186035,
      "learning_rate": 1e-05,
      "loss": 7.3936,
      "step": 524
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 5.994927883148193
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.510817050933838
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.491668224334717
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 8.631285667419434
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.820464134216309
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 8.025981903076172
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 8.016201972961426
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.091838836669922
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.195727348327637
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 6.70258903503418
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.134093761444092
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.5409836769104
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.384543418884277
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 8.114914894104004
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.395365238189697
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.335023403167725
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 8.254216194152832,
      "learning_rate": 1e-05,
      "loss": 7.4617,
      "step": 528
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.49200439453125
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.4064531326293945
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.799133777618408
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.238255500793457
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.0656890869140625
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 6.918032169342041
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.360342979431152
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.714888095855713
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.256121635437012
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.048753261566162
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.488709449768066
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 8.110106468200684
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 6.803780555725098
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 7.6590895652771
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 8.994193077087402
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 7.455229759216309
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 8.605949401855469,
      "learning_rate": 1e-05,
      "loss": 7.4882,
      "step": 532
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.872522830963135
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.711930751800537
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 8.425943374633789
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.626667499542236
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.283633232116699
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.237564563751221
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.342020511627197
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.193355560302734
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 7.633035659790039
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 8.704571723937988
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 7.387469291687012
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 6.8588104248046875
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.158219814300537
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 5.944448471069336
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.678129196166992
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 6.531997203826904
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 12.98642635345459,
      "learning_rate": 1e-05,
      "loss": 7.2244,
      "step": 536
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.5984039306640625
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 7.342191219329834
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.002650737762451
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 7.219787120819092
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.468268394470215
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 8.031564712524414
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 6.4438252449035645
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.23739767074585
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.673415184020996
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 5.952277183532715
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.126577377319336
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.435783386230469
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.865754127502441
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.166076183319092
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 7.044633865356445
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 7.728236675262451
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 13.224288940429688,
      "learning_rate": 1e-05,
      "loss": 6.7086,
      "step": 540
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 6.528472900390625
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.1472487449646
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.960062503814697
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 8.45230484008789
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 5.881997108459473
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.3338236808776855
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 5.855220317840576
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.768827438354492
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 6.52283239364624
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.111107349395752
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 6.901499271392822
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.300691604614258
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 8.309020042419434
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 8.151947975158691
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 5.975563049316406
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 6.330888271331787
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 12.605628967285156,
      "learning_rate": 1e-05,
      "loss": 7.0957,
      "step": 544
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.967072010040283
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.832306861877441
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 8.231558799743652
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.048768520355225
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.960262298583984
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.344545364379883
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.109943389892578
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 4.733938694000244
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.238724708557129
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 8.260741233825684
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.376689910888672
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.577080249786377
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.346119403839111
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.154162883758545
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.60212516784668
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.365157127380371
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 9.049332618713379,
      "learning_rate": 1e-05,
      "loss": 7.3843,
      "step": 548
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.4785919189453125
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.48644495010376
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.324497222900391
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 6.539449214935303
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.7453484535217285
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 8.373583793640137
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 5.793534278869629
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.276280879974365
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 7.802945137023926
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 8.766831398010254
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 7.4768781661987305
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 9.5725736618042
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 8.261630058288574
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.059144496917725
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.958282470703125
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 6.153409481048584
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 11.152636528015137,
      "learning_rate": 1e-05,
      "loss": 7.5668,
      "step": 552
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.548931121826172
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.065240859985352
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 8.215862274169922
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.424259662628174
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 6.2460222244262695
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 5.549643039703369
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.507060527801514
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.211172103881836
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.626739025115967
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 7.4139251708984375
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.043968677520752
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 9.454302787780762
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 5.712962627410889
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.568973541259766
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 8.186714172363281
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.4905500411987305
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 8.562798500061035,
      "learning_rate": 1e-05,
      "loss": 7.0791,
      "step": 556
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.399631023406982
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 6.462893962860107
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.647140979766846
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 8.959585189819336
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.3586039543151855
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 6.308656215667725
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.304111003875732
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.803801536560059
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 6.349695205688477
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.3514580726623535
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.856917381286621
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 6.344872951507568
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 10.208879470825195
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 8.334113121032715
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 8.131044387817383
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 6.98945951461792
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 13.086893081665039,
      "learning_rate": 1e-05,
      "loss": 7.5507,
      "step": 560
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.109250068664551
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.0021820068359375
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 7.3011908531188965
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.9080095291137695
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 5.985989570617676
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.494702339172363
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.356332302093506
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 6.303308486938477
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.847715854644775
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 7.621277332305908
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 7.958037853240967
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.312685966491699
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 7.9227399826049805
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.91876745223999
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.980694770812988
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.127100944519043
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 9.478686332702637,
      "learning_rate": 1e-05,
      "loss": 6.8844,
      "step": 564
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.785765171051025
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.069398403167725
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.624000549316406
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.589402198791504
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.71028470993042
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.8040995597839355
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 8.297085762023926
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.994999885559082
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.133907318115234
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.354856491088867
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.07767391204834
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 7.313139915466309
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 5.891556739807129
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.023702621459961
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 6.9758782386779785
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.132079124450684
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 7.249563217163086,
      "learning_rate": 1e-05,
      "loss": 6.9861,
      "step": 568
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 5.720625877380371
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.588978290557861
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.318280220031738
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.8056640625
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 6.798443794250488
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.070276737213135
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.421245098114014
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 8.829109191894531
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.762360572814941
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 6.913220405578613
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.664241790771484
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.506215572357178
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 7.45806884765625
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 6.911649703979492
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 7.029008865356445
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 9.186979293823242
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 7.404519081115723,
      "learning_rate": 1e-05,
      "loss": 7.4365,
      "step": 572
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.69561767578125
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.253920078277588
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.224653244018555
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.014443874359131
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 6.856091499328613
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 7.475865840911865
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 8.294323921203613
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 6.822495937347412
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 8.247729301452637
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.257664680480957
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 6.6761603355407715
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.511137008666992
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 7.891904354095459
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.459043025970459
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.4446587562561035
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.46220064163208
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 13.324430465698242,
      "learning_rate": 1e-05,
      "loss": 7.2242,
      "step": 576
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.240333080291748
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 5.891350269317627
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 8.440022468566895
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.336248874664307
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 8.08369255065918
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 6.678980350494385
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 7.388444423675537
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 8.116167068481445
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.985495090484619
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.1554694175720215
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 5.734152793884277
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.836318492889404
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 8.015624046325684
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 7.644883155822754
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 6.862589359283447
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 7.435445308685303
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 7.489919185638428,
      "learning_rate": 1e-05,
      "loss": 7.3653,
      "step": 580
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 6.606504440307617
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 7.32516622543335
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 5.255186080932617
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 6.040641784667969
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.831211566925049
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 5.917975902557373
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 6.239455699920654
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.3138813972473145
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.090262413024902
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.045931816101074
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.861750602722168
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.141983032226562
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 8.104612350463867
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 6.6913981437683105
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 8.990403175354004
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 9.890838623046875
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 10.313264846801758,
      "learning_rate": 1e-05,
      "loss": 7.4592,
      "step": 584
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.043955326080322
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.812482833862305
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.216663837432861
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.2877197265625
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.716070652008057
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.508369445800781
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.194730758666992
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.643498420715332
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 6.741654872894287
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 5.544003009796143
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 5.179015636444092
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 8.52990436553955
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 6.550567150115967
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.098201274871826
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 6.233389854431152
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.521161079406738
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 8.909944534301758,
      "learning_rate": 1e-05,
      "loss": 7.0513,
      "step": 588
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.2927165031433105
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.610311031341553
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.310665130615234
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 8.091915130615234
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.787126541137695
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.051408767700195
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.773773193359375
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.184485912322998
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.855920791625977
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.103989601135254
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.841363430023193
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 9.239950180053711
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.608368873596191
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 7.081403732299805
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 7.458740711212158
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.932859897613525
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 7.619635105133057,
      "learning_rate": 1e-05,
      "loss": 7.5141,
      "step": 592
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.711199760437012
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 6.179352283477783
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.0197038650512695
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.366167068481445
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.081969738006592
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.563540935516357
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.957453727722168
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 6.4261298179626465
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.284909248352051
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.347063064575195
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.23387336730957
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.124420642852783
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 8.148276329040527
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.832248687744141
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.663163185119629
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.190361022949219
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 10.322369575500488,
      "learning_rate": 1e-05,
      "loss": 7.3206,
      "step": 596
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.5823822021484375
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.621179103851318
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.843857765197754
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.259695529937744
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 6.15716552734375
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 6.123781204223633
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.600485801696777
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.457756519317627
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 5.8051838874816895
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 8.342947959899902
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 6.8732991218566895
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 9.609371185302734
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 7.363448143005371
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 7.682497978210449
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 6.98959493637085
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 5.413798809051514
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 7.93290901184082,
      "learning_rate": 1e-05,
      "loss": 7.1079,
      "step": 600
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 5.863027095794678
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.1672043800354
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.288692951202393
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.979000091552734
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.516534328460693
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.801942825317383
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 6.100656032562256
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 6.618739128112793
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 7.000290870666504
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.902126789093018
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.2357048988342285
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.861132621765137
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 7.270315647125244
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 8.226339340209961
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 7.3134918212890625
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.79039192199707
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 9.5869140625,
      "learning_rate": 1e-05,
      "loss": 7.0585,
      "step": 604
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.229596138000488
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.62630033493042
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.256913185119629
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 5.7639288902282715
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 8.023152351379395
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 5.484198093414307
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 6.825581073760986
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 7.332122802734375
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.500748634338379
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.523744106292725
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 6.091108798980713
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.950115203857422
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.617984294891357
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.866483688354492
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 6.162181854248047
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 8.873619079589844
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 8.048406600952148,
      "learning_rate": 1e-05,
      "loss": 7.1955,
      "step": 608
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.955214023590088
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.716982841491699
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.862886905670166
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.912551403045654
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.8794050216674805
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.291535377502441
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.558544158935547
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.4672532081604
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.938068866729736
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.853997707366943
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 5.898573398590088
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.487909317016602
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.8547682762146
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.927160263061523
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.053619384765625
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.528899192810059
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 11.865911483764648,
      "learning_rate": 1e-05,
      "loss": 7.5117,
      "step": 612
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 6.785801410675049
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 7.2341461181640625
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 6.397313117980957
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 8.108588218688965
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 6.625888824462891
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 8.769024848937988
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 7.340517520904541
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 8.80515193939209
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 8.321527481079102
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.630622863769531
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.603785037994385
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.64886474609375
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.109368801116943
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 10.849748611450195
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 6.718138217926025
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.589798927307129
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 11.95450210571289,
      "learning_rate": 1e-05,
      "loss": 7.7211,
      "step": 616
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.257184982299805
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 6.896993637084961
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.137396812438965
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.252776145935059
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.8186421394348145
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.251648902893066
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.9130096435546875
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.634657859802246
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.703446865081787
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 7.668133735656738
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 5.454965591430664
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.864248752593994
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.6096296310424805
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 8.163223266601562
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.514528751373291
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.8950581550598145
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 9.179863929748535,
      "learning_rate": 1e-05,
      "loss": 7.1897,
      "step": 620
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.0427398681640625
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 8.806157112121582
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.0806779861450195
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.725435256958008
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.998117446899414
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 8.644993782043457
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 7.055511474609375
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.919378280639648
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.5139384269714355
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 6.738946437835693
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.804373264312744
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.803759574890137
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 6.615255832672119
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.634895324707031
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.8634233474731445
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 8.031856536865234
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 10.611557006835938,
      "learning_rate": 1e-05,
      "loss": 7.5175,
      "step": 624
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 7.690569877624512
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.589330196380615
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.584785461425781
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.5766706466674805
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.742151260375977
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.255079746246338
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 7.709362983703613
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 8.217350006103516
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.06015682220459
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 6.313155174255371
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.056081295013428
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.849941730499268
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.5791144371032715
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 8.137368202209473
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 7.19508171081543
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.661494731903076
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 7.693917274475098,
      "learning_rate": 1e-05,
      "loss": 7.0761,
      "step": 628
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 8.119651794433594
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 7.5288567543029785
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 7.959064960479736
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 9.150483131408691
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.391473770141602
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.91225528717041
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 4.802752494812012
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.874536037445068
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.54563045501709
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 8.387598991394043
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 6.7841949462890625
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.481412410736084
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 6.446166038513184
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 6.585354804992676
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 7.915628433227539
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 8.058070182800293
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 13.476020812988281,
      "learning_rate": 1e-05,
      "loss": 7.4964,
      "step": 632
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 5.982921123504639
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.5262346267700195
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.167222499847412
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.830445289611816
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.163572788238525
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.750284194946289
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 5.81481409072876
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 5.630077838897705
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.124134540557861
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.522799015045166
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.225833415985107
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 8.06759262084961
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 5.810555458068848
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 6.746077060699463
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 7.4301300048828125
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 6.782974720001221
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 10.01047420501709,
      "learning_rate": 1e-05,
      "loss": 6.9735,
      "step": 636
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.033430099487305
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.119344234466553
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 9.009607315063477
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 6.190314292907715
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 6.2979512214660645
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 6.006887912750244
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 5.69260835647583
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 7.93494176864624
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 8.463129997253418
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 8.12779712677002
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 6.717981338500977
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 6.579882621765137
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.652796745300293
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 6.9944000244140625
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.34970760345459
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.760429859161377
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 8.153667449951172,
      "learning_rate": 1e-05,
      "loss": 7.1832,
      "step": 640
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.8320088386535645
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.609013557434082
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.697287082672119
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.936979293823242
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 6.716914176940918
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.154397964477539
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.838075637817383
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.491228103637695
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.0453877449035645
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.26906156539917
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 6.503844738006592
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 6.591574668884277
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.147425174713135
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.488796710968018
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.209327220916748
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 5.4793829917907715
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 9.911272048950195,
      "learning_rate": 1e-05,
      "loss": 7.1257,
      "step": 644
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 6.690517902374268
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.453930854797363
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.430661678314209
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.411694526672363
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.168932914733887
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 8.349129676818848
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.541749954223633
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 8.19633674621582
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.347944736480713
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 8.2139253616333
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.517119407653809
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.3139801025390625
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.087775707244873
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 6.694464683532715
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.009782314300537
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 5.610466003417969
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 9.804287910461426,
      "learning_rate": 1e-05,
      "loss": 7.3149,
      "step": 648
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.101987361907959
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.421648025512695
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 8.165241241455078
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.517752647399902
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 7.045173645019531
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.186534404754639
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.8046159744262695
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 7.126046180725098
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 7.928966522216797
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 7.348383903503418
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.833086967468262
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.14984130859375
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 7.28413200378418
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 8.91409683227539
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 6.1575727462768555
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 7.604197978973389
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 8.240758895874023,
      "learning_rate": 1e-05,
      "loss": 7.0368,
      "step": 652
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 7.483431816101074
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.619241237640381
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.7724504470825195
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 7.779647350311279
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 8.029610633850098
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 7.847306251525879
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 8.167267799377441
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 5.850249290466309
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 8.200510025024414
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.976884841918945
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.122704029083252
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.784440994262695
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 7.086678981781006
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 7.878862380981445
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 6.911147594451904
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 8.810918807983398
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 8.949639320373535,
      "learning_rate": 1e-05,
      "loss": 7.5201,
      "step": 656
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 8.174928665161133
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 8.46475601196289
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 7.743515968322754
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 6.3208537101745605
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 9.431600570678711
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.73713493347168
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.477538585662842
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.694230556488037
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 6.77695369720459
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.611932277679443
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 8.679218292236328
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.213522434234619
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.131553649902344
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.900057315826416
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.052402496337891
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.303713321685791
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 9.742547035217285,
      "learning_rate": 1e-05,
      "loss": 7.6696,
      "step": 660
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 8.109513282775879
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 8.700970649719238
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 6.375213623046875
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 6.323480606079102
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 6.410595893859863
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 8.223325729370117
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 7.4896416664123535
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 7.41136360168457
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.093486785888672
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.962798595428467
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 6.783671855926514
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.9288811683654785
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 7.214912414550781
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 7.583855152130127
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 8.02242374420166
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 8.287652969360352
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 8.3956298828125,
      "learning_rate": 1e-05,
      "loss": 7.4951,
      "step": 664
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.655033111572266
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.859521865844727
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.403736114501953
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.7692694664001465
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.195956230163574
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.0312323570251465
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 6.801809787750244
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 5.832334518432617
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.629120826721191
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.128756046295166
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 6.115785598754883
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 6.762349605560303
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 6.528202533721924
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.133510589599609
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.177834510803223
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.630080699920654
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 7.629683494567871,
      "learning_rate": 1e-05,
      "loss": 6.9784,
      "step": 668
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.206544399261475
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.687783241271973
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 8.02108097076416
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.218125820159912
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 6.214282512664795
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 6.846371173858643
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.5454864501953125
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.466509819030762
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.416057586669922
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.269529819488525
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 8.064579963684082
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.235340118408203
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 8.030264854431152
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.418236255645752
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.922667503356934
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.7122650146484375
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 11.072211265563965,
      "learning_rate": 1e-05,
      "loss": 7.2672,
      "step": 672
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 8.938671112060547
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 7.6342668533325195
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 6.6158905029296875
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 8.289597511291504
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 7.555471897125244
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 7.190801620483398
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 6.536323070526123
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 9.616280555725098
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 8.075754165649414
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 5.615295886993408
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 7.979087829589844
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 8.457880020141602
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.58794641494751
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.5620036125183105
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 6.1433515548706055
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 6.961207866668701
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 8.521005630493164,
      "learning_rate": 1e-05,
      "loss": 7.5475,
      "step": 676
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.261760234832764
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.554717540740967
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.289490699768066
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.898194789886475
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 7.643047332763672
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.528102874755859
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.874150276184082
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 5.520148277282715
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 8.791017532348633
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.693221092224121
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 6.908621311187744
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.768990993499756
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.435976028442383
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.174536228179932
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.372771739959717
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 6.212368011474609
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 8.009778022766113,
      "learning_rate": 1e-05,
      "loss": 7.1204,
      "step": 680
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 8.101927757263184
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 8.021136283874512
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 6.8517165184021
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 6.456894397735596
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.120262145996094
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 5.69822359085083
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.33031177520752
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.184324264526367
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.660877227783203
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 6.408108234405518
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.114947319030762
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 6.465716361999512
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 6.893744945526123
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.693607807159424
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 8.601404190063477
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.1552958488464355
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 8.637632369995117,
      "learning_rate": 1e-05,
      "loss": 7.3599,
      "step": 684
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 8.449953079223633
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 6.092698574066162
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 7.23623514175415
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 6.218639373779297
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 7.042680740356445
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.683070182800293
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 7.148660182952881
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.5183491706848145
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 7.062304496765137
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 8.05009937286377
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.545003890991211
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.777060031890869
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 7.287845134735107
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 7.333927154541016
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.781002998352051
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.593295097351074
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 8.430625915527344,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 688
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.843874931335449
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 8.43603801727295
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.0038862228393555
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.914568901062012
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.252508163452148
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.488450050354004
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 8.247780799865723
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 5.2898736000061035
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.816662788391113
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 6.292922496795654
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.026313304901123
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 6.4099931716918945
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 6.112621784210205
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 6.309559345245361
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 8.057075500488281
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 5.60748291015625
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 9.589264869689941,
      "learning_rate": 1e-05,
      "loss": 7.0694,
      "step": 692
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 6.944591999053955
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.298734188079834
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.7446064949035645
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 8.947562217712402
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 8.578198432922363
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 6.776474952697754
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.689108848571777
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.707582950592041
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 6.9742536544799805
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.655471324920654
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 6.792566776275635
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.6118035316467285
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 6.983582019805908
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.595003604888916
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.40387487411499
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 6.533393859863281
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 8.779928207397461,
      "learning_rate": 1e-05,
      "loss": 7.4523,
      "step": 696
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 6.148072242736816
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 7.678252696990967
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 7.911110877990723
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 6.86262845993042
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 8.367439270019531
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 7.396990776062012
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 6.561540126800537
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 7.649150848388672
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.69673490524292
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 7.375219821929932
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 7.70353889465332
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.853096961975098
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 8.121430397033691
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.880465030670166
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 6.479558944702148
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.869632720947266
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 9.258282661437988,
      "learning_rate": 1e-05,
      "loss": 7.3472,
      "step": 700
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 7.030467987060547
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.8309831619262695
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.556358814239502
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 7.555022239685059
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 8.32147216796875
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.492121696472168
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 9.32041072845459
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.08317756652832
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 8.02802848815918
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 5.466261386871338
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 7.287408351898193
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 7.372259616851807
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.903319835662842
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.544984817504883
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.343652725219727
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.654857158660889
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 8.622983932495117,
      "learning_rate": 1e-05,
      "loss": 7.1744,
      "step": 704
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 7.812174320220947
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 5.909177780151367
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 5.8095245361328125
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 7.052878379821777
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 7.985577583312988
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 6.716677665710449
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 5.420307636260986
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 8.052311897277832
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.1189422607421875
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 6.5761003494262695
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 6.242554187774658
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.71758508682251
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 7.018841743469238
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.8804097175598145
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 8.242166519165039
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.773303508758545
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 8.638007164001465,
      "learning_rate": 1e-05,
      "loss": 6.958,
      "step": 708
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 5.959477424621582
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 7.246318340301514
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.894919395446777
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.691508769989014
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.414511203765869
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.229193687438965
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 6.448711395263672
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.0033111572265625
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 7.093571186065674
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 5.801601886749268
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 8.135292053222656
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 5.782474040985107
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 6.3200788497924805
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 5.94486141204834
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 7.074196815490723
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 7.28468656539917
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 11.58680534362793,
      "learning_rate": 1e-05,
      "loss": 6.7703,
      "step": 712
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.849686145782471
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 7.030930519104004
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.761579513549805
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 7.286937713623047
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 9.033803939819336
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.709596633911133
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.591322898864746
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.10847282409668
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 5.561008453369141
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 7.242018699645996
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 5.796981334686279
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 6.962856292724609
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.757320404052734
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.800143718719482
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 7.972363471984863
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 5.255859851837158
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 8.860085487365723,
      "learning_rate": 1e-05,
      "loss": 6.9826,
      "step": 716
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 7.4591965675354
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.7074995040893555
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.161533355712891
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.999730587005615
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 5.71613883972168
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.86704683303833
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.0736894607543945
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 7.124951362609863
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 6.697651386260986
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 8.009465217590332
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 7.249051094055176
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 6.208799839019775
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.2842583656311035
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.352067470550537
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.21425199508667
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.694626808166504
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 8.041057586669922,
      "learning_rate": 1e-05,
      "loss": 6.9262,
      "step": 720
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.710392475128174
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 5.673832416534424
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.350873947143555
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.452645301818848
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.4787726402282715
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.880707740783691
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.578438758850098
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 8.347559928894043
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 6.601321220397949
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 8.1145658493042
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 7.230293273925781
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 7.785109996795654
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 7.164449214935303
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 10.3302640914917
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.7654948234558105
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.416160583496094
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 17.19939422607422,
      "learning_rate": 1e-05,
      "loss": 7.4926,
      "step": 724
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.297775745391846
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.254092216491699
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 6.75316858291626
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.239148139953613
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.915564060211182
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 6.573504447937012
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.444130897521973
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 6.038114070892334
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.86410665512085
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.0940260887146
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.964905738830566
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 5.967769622802734
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.103727340698242
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.943150997161865
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 6.697291374206543
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 8.190081596374512
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 9.935938835144043,
      "learning_rate": 1e-05,
      "loss": 7.2088,
      "step": 728
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.934054851531982
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.397598743438721
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 6.877037048339844
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.371365547180176
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 6.468707084655762
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.584269046783447
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.97596549987793
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 5.887796401977539
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 6.6856255531311035
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.671935558319092
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.419550895690918
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.113692283630371
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.560783386230469
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.018510341644287
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.281029224395752
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 7.373250961303711
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 9.6453218460083,
      "learning_rate": 1e-05,
      "loss": 7.0388,
      "step": 732
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.59787130355835
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.1958088874816895
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 5.841987609863281
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 7.673733234405518
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.844405174255371
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.504095554351807
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.97158670425415
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.09932279586792
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.428367614746094
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 5.931997299194336
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 5.997110366821289
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.226871490478516
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 6.523054599761963
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 5.714462757110596
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 7.630645751953125
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 5.443638324737549
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 11.290186882019043,
      "learning_rate": 1e-05,
      "loss": 6.7891,
      "step": 736
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 7.464349269866943
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 5.307511329650879
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 6.255017280578613
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 6.451249599456787
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.7138566970825195
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 7.377171039581299
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 7.201420307159424
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.3959059715271
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 6.23306131362915
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 5.694160461425781
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 7.187521457672119
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 8.979655265808105
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 6.956150531768799
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 5.67344856262207
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 7.267752647399902
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 6.5684309005737305
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 10.0569429397583,
      "learning_rate": 1e-05,
      "loss": 6.7329,
      "step": 740
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.244104385375977
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 7.919495105743408
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 8.121923446655273
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.918493270874023
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.184842586517334
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.298986911773682
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 6.329951763153076
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 9.115906715393066
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.0547261238098145
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 8.008286476135254
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 5.8476457595825195
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.836241245269775
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 6.950997829437256
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 5.062143325805664
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.1118950843811035
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.521003246307373
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 12.965163230895996,
      "learning_rate": 1e-05,
      "loss": 7.1579,
      "step": 744
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 5.460243225097656
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.338834762573242
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.477792739868164
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.510186195373535
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 7.366612434387207
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 5.444656848907471
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 6.648079872131348
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 6.5677080154418945
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.497233867645264
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 7.339270114898682
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.455827236175537
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.312677383422852
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 7.146262168884277
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 9.07483959197998
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 6.557300090789795
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 5.9276041984558105
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 14.075063705444336,
      "learning_rate": 1e-05,
      "loss": 7.0078,
      "step": 748
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 8.006896018981934
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 7.5673909187316895
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 5.856039524078369
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 5.972984790802002
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.561403274536133
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.06856632232666
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.3454108238220215
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.717676639556885
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 7.056063652038574
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 6.363266468048096
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 7.54683256149292
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 6.342522144317627
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 8.633951187133789
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 8.731681823730469
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 7.0735764503479
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 5.569624423980713
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 13.974790573120117,
      "learning_rate": 1e-05,
      "loss": 7.1509,
      "step": 752
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.840026378631592
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.616765975952148
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 5.288039207458496
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.358343601226807
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.27634859085083
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.6418304443359375
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.2174506187438965
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 6.411586284637451
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.900961875915527
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.968139171600342
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 5.8199896812438965
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.450069904327393
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.636783599853516
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.720037937164307
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.309404373168945
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 5.681249618530273
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 12.393040657043457,
      "learning_rate": 1e-05,
      "loss": 6.7586,
      "step": 756
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 6.951699733734131
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.504156589508057
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.197661399841309
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.398370265960693
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 7.896425247192383
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 8.971856117248535
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 6.46296501159668
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 5.605593204498291
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 6.434176445007324
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.443437099456787
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 5.2601799964904785
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.065011024475098
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 6.256528854370117
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 6.231419086456299
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 8.050885200500488
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 7.220293998718262
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 9.28491497039795,
      "learning_rate": 1e-05,
      "loss": 6.9969,
      "step": 760
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 6.95959997177124
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 8.705526351928711
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 8.727036476135254
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 5.517380237579346
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 7.507992267608643
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 6.41192626953125
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 8.236661911010742
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 8.467326164245605
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.4355292320251465
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.225676536560059
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 6.937514305114746
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.482503414154053
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 5.641543865203857
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 6.729276180267334
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.264945983886719
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 8.20069408416748
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 8.33326530456543,
      "learning_rate": 1e-05,
      "loss": 7.3407,
      "step": 764
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.086641788482666
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.6157732009887695
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.25689172744751
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.3634867668151855
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 7.484757900238037
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 6.550750732421875
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 5.9314374923706055
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 7.975395679473877
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.568387508392334
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.632331371307373
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 8.138890266418457
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.667007923126221
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.61824893951416
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.937856674194336
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 8.544729232788086
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 6.40806245803833
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 8.572357177734375,
      "learning_rate": 1e-05,
      "loss": 7.4238,
      "step": 768
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.4458208084106445
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.594648838043213
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.095149517059326
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 5.969532489776611
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.8944926261901855
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 7.890063285827637
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 5.807255268096924
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.778004169464111
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.68062686920166
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 6.7813615798950195
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.005338668823242
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.582813262939453
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.20383882522583
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 6.341447353363037
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.3400654792785645
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.805850982666016
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 7.7894463539123535,
      "learning_rate": 1e-05,
      "loss": 7.076,
      "step": 772
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 7.550410747528076
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 9.259661674499512
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 6.415372371673584
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 7.1147541999816895
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 8.33450698852539
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.94950008392334
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 7.3309125900268555
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.3152642250061035
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 6.100374221801758
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.920892715454102
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.095296382904053
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.4029741287231445
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.7282586097717285
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.756066799163818
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.877449989318848
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 8.843703269958496
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 14.068558692932129,
      "learning_rate": 1e-05,
      "loss": 7.3122,
      "step": 776
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.154531002044678
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.661750316619873
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.3830156326293945
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.446633815765381
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.467200756072998
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.7809929847717285
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.532763481140137
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.069321632385254
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 6.666829586029053
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 6.729734897613525
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 8.047247886657715
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 7.445354461669922
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 8.056983947753906
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.447768211364746
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.610297679901123
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.414412975311279
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 8.444063186645508,
      "learning_rate": 1e-05,
      "loss": 7.3072,
      "step": 780
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 6.923367023468018
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 8.491064071655273
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 7.770700454711914
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 6.343165874481201
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 8.539068222045898
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 8.747261047363281
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 6.936436653137207
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 7.332629203796387
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.83334493637085
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.433723449707031
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 7.838510036468506
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.165923595428467
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.647995948791504
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.556159496307373
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.321399211883545
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 7.451937198638916
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 7.768092632293701,
      "learning_rate": 1e-05,
      "loss": 7.2083,
      "step": 784
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.1944146156311035
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 8.14492130279541
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 6.813396453857422
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.070866107940674
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.5733256340026855
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.310108661651611
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.646653175354004
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.8057990074157715
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.227962970733643
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.460399627685547
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 10.749659538269043
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.301640033721924
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.835189342498779
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.297918319702148
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 7.33172607421875
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 8.268495559692383
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 12.736845970153809,
      "learning_rate": 1e-05,
      "loss": 7.5645,
      "step": 788
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 7.2564849853515625
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 6.693613052368164
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 8.08535099029541
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 7.0054850578308105
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 6.573246479034424
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.443109512329102
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 6.466366767883301
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.204606056213379
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 6.855348110198975
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 4.441232681274414
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 7.399599075317383
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 8.294011116027832
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 6.7782511711120605
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 6.616808891296387
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 8.379448890686035
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 8.07263469696045
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 9.135390281677246,
      "learning_rate": 1e-05,
      "loss": 7.0978,
      "step": 792
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.490533828735352
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.667505741119385
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 6.516371726989746
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 6.871809005737305
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.892791271209717
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.648448467254639
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.065356731414795
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 5.8899312019348145
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 6.102813243865967
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.722466945648193
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.7273454666137695
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 6.9797468185424805
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.617501258850098
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 5.828641891479492
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.586007595062256
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.656508922576904
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 8.31425952911377,
      "learning_rate": 1e-05,
      "loss": 7.1415,
      "step": 796
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.667788505554199
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.039983749389648
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 8.142295837402344
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.74735689163208
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 7.005985260009766
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 5.8914337158203125
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 6.9917707443237305
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 7.970510482788086
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.457269668579102
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 6.707930088043213
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.463604927062988
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.081424236297607
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.173180103302002
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 8.315511703491211
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.111395359039307
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 7.675925254821777
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 13.885756492614746,
      "learning_rate": 1e-05,
      "loss": 7.2152,
      "step": 800
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.944310665130615
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.619216442108154
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.1233744621276855
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.6428070068359375
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 7.94877815246582
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 7.0549116134643555
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.682488918304443
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.743875026702881
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.513547420501709
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 7.396149158477783
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 8.035909652709961
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.492295265197754
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 8.638956069946289
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.584041595458984
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.426419734954834
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.097825527191162
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 7.657343864440918,
      "learning_rate": 1e-05,
      "loss": 7.3091,
      "step": 804
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.421038627624512
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 7.746757984161377
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 7.267167091369629
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.538160800933838
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.926204681396484
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.096925258636475
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.609364986419678
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.561868190765381
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 7.904791355133057
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.172348499298096
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.510406970977783
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 3.943934679031372
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.78411865234375
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 5.865633964538574
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.366123676300049
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.765300750732422
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 7.649136066436768,
      "learning_rate": 1e-05,
      "loss": 6.8425,
      "step": 808
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.673496246337891
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.912662982940674
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.4880218505859375
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.5267252922058105
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 7.249334335327148
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 7.383227825164795
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 6.732342720031738
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 6.841943264007568
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 8.066043853759766
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.152496337890625
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.930498123168945
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.643558979034424
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.635989189147949
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.804720878601074
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.8450469970703125
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 7.259118556976318
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 11.723189353942871,
      "learning_rate": 1e-05,
      "loss": 7.1966,
      "step": 812
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 8.049393653869629
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.0914154052734375
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 6.122917175292969
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.678223133087158
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 8.251885414123535
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.544054985046387
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 5.424549579620361
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.093045234680176
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 6.6615800857543945
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 6.85531759262085
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 7.89941930770874
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 8.123095512390137
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.062263011932373
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 5.183375358581543
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.723530292510986
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.928625106811523
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 10.438928604125977,
      "learning_rate": 1e-05,
      "loss": 7.1683,
      "step": 816
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 6.329004287719727
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.395474433898926
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.169943809509277
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.302660942077637
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 4.311930179595947
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 6.7956223487854
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 8.561283111572266
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 7.382592678070068
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.079009056091309
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.315610885620117
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 6.858888626098633
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.796277046203613
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 7.829658508300781
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 8.531761169433594
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 6.132144451141357
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 6.622104644775391
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 10.736778259277344,
      "learning_rate": 1e-05,
      "loss": 7.0884,
      "step": 820
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.063544750213623
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 6.15837287902832
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.392588138580322
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.8159050941467285
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 7.323587417602539
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.974554061889648
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 9.954418182373047
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.285775184631348
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 6.7255473136901855
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 5.315337657928467
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 5.433378219604492
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 9.515216827392578
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 8.122336387634277
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 7.4263410568237305
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 6.806458950042725
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 7.625648498535156
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 10.938361167907715,
      "learning_rate": 1e-05,
      "loss": 7.2462,
      "step": 824
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.0844244956970215
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.456233978271484
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 6.636801242828369
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.5240960121154785
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 8.554115295410156
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.113595485687256
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.418328762054443
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.812263488769531
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.303267478942871
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.643023490905762
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.4047346115112305
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.257386684417725
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.618614196777344
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.383209228515625
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.28087043762207
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.783229827880859
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 10.570295333862305,
      "learning_rate": 1e-05,
      "loss": 7.2046,
      "step": 828
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.066954135894775
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 6.017994403839111
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 5.889477252960205
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.422532558441162
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.714961528778076
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.765348434448242
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 8.227232933044434
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.908946990966797
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.873594284057617
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.139333248138428
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 7.538086414337158
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 7.394338607788086
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 5.5286993980407715
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.41973352432251
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.282325267791748
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 6.877228260040283
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 7.164116382598877,
      "learning_rate": 1e-05,
      "loss": 6.8792,
      "step": 832
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.014946937561035
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.261376857757568
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 7.785135746002197
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.2608208656311035
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.561666965484619
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.101681709289551
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.488606929779053
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.884815692901611
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.263153076171875
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.207045555114746
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 6.977006912231445
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 9.40546703338623
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.275892734527588
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.392833232879639
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.277686595916748
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.715944290161133
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 11.026599884033203,
      "learning_rate": 1e-05,
      "loss": 7.3046,
      "step": 836
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.373568058013916
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.282296180725098
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 8.989021301269531
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 8.298437118530273
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.262816429138184
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.362588882446289
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 6.636457920074463
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.431212902069092
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 7.652865886688232
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 5.918848991394043
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 7.0359721183776855
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 6.489068031311035
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.97586727142334
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 6.597050189971924
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.466129779815674
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.548438549041748
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 10.666437149047852,
      "learning_rate": 1e-05,
      "loss": 7.3325,
      "step": 840
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.647886753082275
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.946865081787109
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.2192254066467285
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.136842727661133
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.086144924163818
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 5.14063835144043
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.505218982696533
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.121418476104736
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 8.363995552062988
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 6.321098327636719
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 7.077032566070557
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 6.553358554840088
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.954373359680176
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.888192176818848
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 6.50044584274292
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 6.498714447021484
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 10.264307975769043,
      "learning_rate": 1e-05,
      "loss": 7.1226,
      "step": 844
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 7.122339248657227
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 10.349291801452637
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.962682247161865
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.65241003036499
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.716928482055664
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.524501323699951
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.878113746643066
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.864073753356934
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 7.688050270080566
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 6.648819923400879
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 5.486545562744141
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 6.570918083190918
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 5.990664005279541
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 8.49235725402832
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 7.460155010223389
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 6.888789176940918
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 12.738500595092773,
      "learning_rate": 1e-05,
      "loss": 7.081,
      "step": 848
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 5.939150810241699
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 7.700911045074463
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.2215189933776855
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.312709331512451
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.869744300842285
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.386552810668945
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 7.309284210205078
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.659444808959961
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 7.308578968048096
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 6.570850849151611
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 7.458028316497803
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 8.026230812072754
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.743609428405762
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.304820537567139
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 6.992911338806152
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.8466620445251465
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 7.237311363220215,
      "learning_rate": 1e-05,
      "loss": 7.0407,
      "step": 852
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.739663600921631
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.594604969024658
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 7.4628400802612305
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.074709892272949
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 5.920360088348389
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.624337196350098
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.200811386108398
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 5.6813764572143555
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.261751174926758
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 7.322147846221924
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.742356300354004
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.786734580993652
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 6.4852986335754395
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.360162734985352
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.615264892578125
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 6.292037010192871
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 10.1194486618042,
      "learning_rate": 1e-05,
      "loss": 6.7603,
      "step": 856
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.3731489181518555
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 7.642509937286377
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.940797328948975
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.354060649871826
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.625466346740723
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 7.004022598266602
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.887704849243164
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.381670951843262
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 7.742708206176758
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 5.806938171386719
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 9.25085163116455
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 8.006744384765625
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.139254093170166
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.4344868659973145
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 6.517035484313965
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.687093257904053
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 13.363895416259766,
      "learning_rate": 1e-05,
      "loss": 7.1122,
      "step": 860
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.885530948638916
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 5.295608997344971
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.261105537414551
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.543768882751465
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 6.631839752197266
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 8.036160469055176
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 7.843410015106201
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 7.620923042297363
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 6.896880149841309
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 7.265169620513916
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 6.748443126678467
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 7.148123741149902
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.159070014953613
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.454427242279053
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.603905200958252
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 6.6103315353393555
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 14.529662132263184,
      "learning_rate": 1e-05,
      "loss": 7.1878,
      "step": 864
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 6.908429145812988
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 4.97613000869751
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 8.182188034057617
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 6.001644134521484
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.8494768142700195
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.309213161468506
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 6.899145126342773
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.575263500213623
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 5.268899440765381
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.0977020263671875
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 8.327857971191406
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.346909046173096
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.880013942718506
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 6.058511257171631
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.517721652984619
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.46694278717041
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 7.738489627838135,
      "learning_rate": 1e-05,
      "loss": 7.0416,
      "step": 868
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 7.537184238433838
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.965476989746094
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.977582931518555
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.999542713165283
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.124688148498535
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.587009906768799
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.983232021331787
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.21464729309082
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.629908561706543
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 6.601301193237305
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 5.424865245819092
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.169931411743164
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.0615057945251465
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.941755771636963
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.539806842803955
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.41072940826416
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 12.689265251159668,
      "learning_rate": 1e-05,
      "loss": 7.1981,
      "step": 872
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 5.062407970428467
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 5.46240758895874
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 7.425811767578125
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 6.492099761962891
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 7.750023365020752
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 6.900044918060303
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 6.329041004180908
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 7.1648101806640625
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 8.222869873046875
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 7.404592990875244
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 6.908819675445557
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 6.736179351806641
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 7.546988487243652
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 6.58929967880249
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 6.964486598968506
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 8.306330680847168
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 10.150222778320312,
      "learning_rate": 1e-05,
      "loss": 6.9541,
      "step": 876
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 4.916464805603027
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 7.70362663269043
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 7.413809776306152
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 6.724100112915039
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.129174709320068
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.72393798828125
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.359467029571533
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 7.650168418884277
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.47109842300415
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 5.767757892608643
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 6.9738969802856445
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.568243026733398
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.924320697784424
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 8.216288566589355
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.06545877456665
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.254753112792969
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 10.832039833068848,
      "learning_rate": 1e-05,
      "loss": 6.9914,
      "step": 880
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 6.783562183380127
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.076141834259033
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.6703200340271
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.284185409545898
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.897521495819092
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 8.662328720092773
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.0982346534729
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.200083255767822
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 7.154634475708008
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 5.406583786010742
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 6.681096076965332
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 6.295332431793213
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.527408599853516
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.087545394897461
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.703707695007324
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.524468898773193
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 10.1476469039917,
      "learning_rate": 1e-05,
      "loss": 6.9408,
      "step": 884
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.715442657470703
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.3051934242248535
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 6.390124320983887
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.0039825439453125
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 6.599710941314697
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 7.457118034362793
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 7.729769229888916
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 5.300081253051758
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.241535186767578
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 6.716594696044922
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 10.763550758361816
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.097225189208984
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 6.860349178314209
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 5.056646347045898
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 6.769491195678711
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 7.8604888916015625
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 10.353286743164062,
      "learning_rate": 1e-05,
      "loss": 7.1167,
      "step": 888
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 4.667062282562256
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.999044418334961
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.802943229675293
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 5.5111613273620605
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 7.988242149353027
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.311324596405029
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.752025127410889
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 8.992109298706055
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.959993839263916
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 7.000390529632568
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.956244945526123
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.791262149810791
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.730249881744385
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 8.142066955566406
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.706309795379639
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 7.471441268920898
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 8.586185455322266,
      "learning_rate": 1e-05,
      "loss": 6.9239,
      "step": 892
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 7.10991096496582
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 9.815572738647461
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 6.91194486618042
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 5.406633377075195
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.5275492668151855
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 6.079318046569824
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.262707710266113
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.2677483558654785
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 8.208398818969727
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 6.665585994720459
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 5.156864166259766
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 6.864404201507568
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.2122602462768555
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.06465482711792
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.250428676605225
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.6371612548828125
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 7.234125137329102,
      "learning_rate": 1e-05,
      "loss": 7.0901,
      "step": 896
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.440708637237549
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.934587001800537
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.739702224731445
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.604779243469238
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 6.641813278198242
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 6.754274845123291
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.286022186279297
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 8.066521644592285
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.359044075012207
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 8.003812789916992
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 5.381690502166748
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.545904159545898
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.94319486618042
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.592953205108643
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 7.475757122039795
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 5.290265083312988
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 12.212139129638672,
      "learning_rate": 1e-05,
      "loss": 7.1288,
      "step": 900
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.401431083679199
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 7.175251007080078
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.627004623413086
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.592196464538574
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 8.014532089233398
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 8.453191757202148
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 6.473550319671631
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 5.163286209106445
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.889769077301025
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 7.036489009857178
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.779224395751953
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.895600318908691
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.134620189666748
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 8.013327598571777
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.4609761238098145
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 7.004875659942627
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 8.652647972106934,
      "learning_rate": 1e-05,
      "loss": 6.8822,
      "step": 904
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 5.638509750366211
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.867103576660156
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.349686145782471
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.487237453460693
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.321836948394775
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.190227508544922
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 7.496026992797852
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 7.660998344421387
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.9119291305542
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.554183006286621
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 6.828163146972656
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.23051929473877
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 6.412253379821777
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 8.449088096618652
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 7.166393280029297
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 5.820252895355225
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 15.45479965209961,
      "learning_rate": 1e-05,
      "loss": 7.274,
      "step": 908
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 8.545425415039062
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 6.9689106941223145
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 7.681533336639404
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 8.247234344482422
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 5.67457389831543
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 5.293242454528809
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 7.21744966506958
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 6.8160552978515625
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 8.147981643676758
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 5.1454997062683105
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 6.737396240234375
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 6.068857192993164
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.522346496582031
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 7.239411354064941
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.320777893066406
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 7.624478816986084
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 10.170125007629395,
      "learning_rate": 1e-05,
      "loss": 6.8907,
      "step": 912
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 8.306585311889648
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.89127254486084
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 6.6195783615112305
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.293664455413818
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 6.025571823120117
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.590465068817139
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 6.431428909301758
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.9945173263549805
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 7.4139084815979
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.611980438232422
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.585293769836426
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 7.0576372146606445
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.9786152839660645
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 8.03677749633789
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.17863130569458
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 6.563412666320801
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 9.507015228271484,
      "learning_rate": 1e-05,
      "loss": 7.2237,
      "step": 916
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 7.506018161773682
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 6.961234092712402
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 7.495973587036133
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 10.008259773254395
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 5.648875713348389
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.365983963012695
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.316587924957275
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 10.8967924118042
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.952255725860596
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.4896135330200195
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 6.710374355316162
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 8.04035472869873
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 7.045206546783447
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 7.0320916175842285
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.548027992248535
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.064159870147705
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 10.154857635498047,
      "learning_rate": 1e-05,
      "loss": 7.3801,
      "step": 920
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 8.215863227844238
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.244930744171143
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 7.521228790283203
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.889376640319824
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.395261287689209
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 8.156518936157227
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.503940582275391
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.132629871368408
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.810621738433838
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.033893585205078
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.136981964111328
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 7.134891033172607
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 5.800745964050293
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 7.12330436706543
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 5.94764518737793
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 7.088926315307617
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 15.699043273925781,
      "learning_rate": 1e-05,
      "loss": 6.7585,
      "step": 924
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 6.7463860511779785
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.379523277282715
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.026024341583252
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 6.146527290344238
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 6.631680488586426
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 5.587727069854736
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.874544143676758
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.26963996887207
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 6.96865177154541
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.387615203857422
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.577230930328369
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.907308578491211
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 7.070199012756348
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 5.672652721405029
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 5.533665657043457
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 8.239709854125977
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 10.684964179992676,
      "learning_rate": 1e-05,
      "loss": 6.9387,
      "step": 928
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 5.306830883026123
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.0807294845581055
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.104422092437744
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.069748878479004
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 7.462544918060303
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 5.234628677368164
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 6.4365339279174805
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 6.622644901275635
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 8.1332368850708
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 6.042628288269043
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 7.300492763519287
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 6.684426784515381
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 8.131669044494629
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 7.155475616455078
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 7.361519813537598
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.139490604400635
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 11.229714393615723,
      "learning_rate": 1e-05,
      "loss": 6.8292,
      "step": 932
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 8.513408660888672
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 7.47006368637085
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 8.397581100463867
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 6.862460613250732
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.429778575897217
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.159305572509766
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.055986404418945
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 6.316487789154053
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.5689377784729
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.640231132507324
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.580584526062012
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.678936004638672
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 7.801337242126465
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 5.979376316070557
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 6.328587532043457
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 7.152510643005371
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 8.050337791442871,
      "learning_rate": 1e-05,
      "loss": 7.3085,
      "step": 936
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.412740230560303
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.542388916015625
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.299499988555908
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 6.4343390464782715
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 5.112453937530518
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 5.56180477142334
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.815736770629883
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.620245456695557
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.05021333694458
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.488569736480713
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 8.58554458618164
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.182085990905762
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 7.535633087158203
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.3729329109191895
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.7463603019714355
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 7.912971019744873
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 10.25662899017334,
      "learning_rate": 1e-05,
      "loss": 6.9796,
      "step": 940
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 7.69617223739624
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 7.972501754760742
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 6.604824066162109
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 6.748988151550293
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 7.5925397872924805
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 6.980581283569336
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 5.221174240112305
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 8.271592140197754
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 8.285240173339844
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.198753356933594
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 8.651240348815918
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.786141872406006
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 7.848208904266357
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 6.1206889152526855
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 5.202738285064697
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 6.917944431304932
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 20.709840774536133,
      "learning_rate": 1e-05,
      "loss": 7.1937,
      "step": 944
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.641531467437744
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.21943998336792
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.054539680480957
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 7.983400821685791
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.18771505355835
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.0154500007629395
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.3548264503479
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 8.938995361328125
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.730776786804199
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.442458152770996
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 6.281740188598633
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.285667419433594
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.419149875640869
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.9907026290893555
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.841375827789307
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.770568370819092
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 11.56143569946289,
      "learning_rate": 1e-05,
      "loss": 7.2599,
      "step": 948
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.147912502288818
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 6.523369312286377
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.673999309539795
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 6.8589253425598145
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.6316914558410645
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.505194187164307
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.881287097930908
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.716148376464844
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 7.705357551574707
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 7.234653949737549
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 4.728617191314697
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 8.132536888122559
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.226592540740967
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.024975776672363
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.391762733459473
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.8909783363342285
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 7.193974018096924,
      "learning_rate": 1e-05,
      "loss": 6.7671,
      "step": 952
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.503868579864502
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 6.246789932250977
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.054797649383545
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 6.230038642883301
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 7.662936687469482
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 5.528838634490967
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 8.346099853515625
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 7.101447582244873
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 7.253212928771973
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.623206615447998
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.733837604522705
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 7.201745986938477
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 5.60967493057251
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 6.806154727935791
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 6.711158275604248
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 7.49789571762085
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 10.065765380859375,
      "learning_rate": 1e-05,
      "loss": 6.882,
      "step": 956
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.21502161026001
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.57265043258667
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 8.609515190124512
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.69774866104126
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.754255771636963
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.47467041015625
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 6.135778903961182
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 6.833698272705078
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 8.076727867126465
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.18071985244751
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 7.544920921325684
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.746448516845703
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 5.129591941833496
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 7.287917137145996
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 8.023297309875488
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 7.695043087005615
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 12.473396301269531,
      "learning_rate": 1e-05,
      "loss": 7.2486,
      "step": 960
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 6.8862624168396
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 8.018321990966797
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 5.963377952575684
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 7.080880165100098
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.9259724617004395
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.3260040283203125
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 7.495688438415527
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 7.327057361602783
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.417838096618652
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 6.969508171081543
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 6.5072021484375
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 5.406200408935547
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 8.404203414916992
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 6.724545001983643
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 8.785811424255371
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 7.580672264099121
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 14.45427131652832,
      "learning_rate": 1e-05,
      "loss": 7.1137,
      "step": 964
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.370247840881348
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 6.299409866333008
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 6.963869094848633
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.019730091094971
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.072245121002197
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.386733531951904
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 6.684301376342773
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 4.898955345153809
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 7.214037895202637
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.378685474395752
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 7.377138137817383
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.4468841552734375
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 7.723830223083496
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 7.185025215148926
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 6.125648498535156
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 5.005349636077881
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 9.891280174255371,
      "learning_rate": 1e-05,
      "loss": 6.697,
      "step": 968
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 5.908299446105957
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 7.348592281341553
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 7.623134136199951
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 5.8047685623168945
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 8.166605949401855
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 6.214479923248291
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.763429164886475
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.246802806854248
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 6.360289573669434
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 7.215107440948486
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 6.240716457366943
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 7.251758098602295
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 6.222929000854492
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.0632829666137695
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.167920112609863
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.49587869644165
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 16.57097816467285,
      "learning_rate": 1e-05,
      "loss": 6.9434,
      "step": 972
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 6.962765216827393
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 7.189886569976807
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 6.9198713302612305
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 7.37045431137085
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 7.1267571449279785
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 6.627594470977783
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 7.7014641761779785
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 8.065690994262695
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.901725769042969
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.292377471923828
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.605903148651123
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.1569037437438965
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.275448322296143
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 6.153111934661865
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.113094329833984
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.552021503448486
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 12.221220016479492,
      "learning_rate": 1e-05,
      "loss": 7.2509,
      "step": 976
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 5.582419395446777
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.608456611633301
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.077754497528076
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.377397537231445
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 8.005874633789062
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 6.943305492401123
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 5.039041996002197
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 5.3582987785339355
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 6.967587471008301
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.118664741516113
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.86191463470459
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.3010711669921875
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 7.054254531860352
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.0650224685668945
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.174571514129639
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 7.094186305999756
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 11.45328426361084,
      "learning_rate": 1e-05,
      "loss": 6.6019,
      "step": 980
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 7.299440383911133
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 7.498958587646484
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 5.643187522888184
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 6.710718631744385
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 4.989567279815674
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 7.3485188484191895
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.408294200897217
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.234899520874023
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.222224235534668
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 5.323256492614746
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.286306381225586
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.765387058258057
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 6.803764820098877
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 6.057846546173096
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 7.369040489196777
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 4.631045818328857
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 13.128499984741211,
      "learning_rate": 1e-05,
      "loss": 6.537,
      "step": 984
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.0165300369262695
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 9.636990547180176
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.92141056060791
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 8.328207969665527
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 8.655137062072754
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 5.962042331695557
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 7.139155387878418
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 5.951781272888184
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.242577075958252
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 6.192465305328369
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.066593170166016
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 5.052526473999023
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.169510841369629
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.320633411407471
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 7.460927963256836
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.767777919769287
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 13.682676315307617,
      "learning_rate": 1e-05,
      "loss": 7.0553,
      "step": 988
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.26161527633667
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.195328712463379
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 6.575880527496338
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.457463264465332
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.717080116271973
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.568975448608398
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.150315761566162
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 6.121634006500244
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 7.137089252471924
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 6.512327194213867
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 5.7186503410339355
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 7.857564449310303
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 6.544626235961914
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 5.669384956359863
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 8.197263717651367
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 7.903849124908447
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 9.18397045135498,
      "learning_rate": 1e-05,
      "loss": 7.0368,
      "step": 992
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 8.318384170532227
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.759368896484375
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 7.203463077545166
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.139601230621338
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.125643730163574
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.2910990715026855
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.852033615112305
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.999146938323975
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 6.983765602111816
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.62445068359375
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 6.96693229675293
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.5112762451171875
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 6.913211822509766
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.895196437835693
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.714592933654785
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 6.136826515197754
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 11.356926918029785,
      "learning_rate": 1e-05,
      "loss": 7.2772,
      "step": 996
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.39324951171875
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.965189456939697
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 4.856173038482666
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.796143531799316
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.012423038482666
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.791145324707031
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 6.089158535003662
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.546772003173828
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 7.113350868225098
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 6.909821033477783
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 5.086513996124268
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 5.281689167022705
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.155663013458252
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.97465181350708
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 6.3320536613464355
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.37673807144165
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 9.61492919921875,
      "learning_rate": 1e-05,
      "loss": 6.6675,
      "step": 1000
    },
    {
      "epoch": 0.21680216802168023,
      "eval_runtime": 475.941,
      "eval_samples_per_second": 4.307,
      "eval_steps_per_second": 4.307,
      "step": 1000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.200301170349121
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.18200159072876
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 6.448754787445068
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.443253517150879
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.204944133758545
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.956484317779541
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 8.278193473815918
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.354549407958984
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 7.706345558166504
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 6.88260555267334
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 7.415295600891113
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 9.46681022644043
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.28498649597168
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.753265380859375
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.21872615814209
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.794654846191406
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 11.087937355041504,
      "learning_rate": 1e-05,
      "loss": 7.1619,
      "step": 1004
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 8.103793144226074
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 6.696789264678955
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 7.202451229095459
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 6.795474052429199
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 7.07622766494751
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 5.795262813568115
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 5.790053367614746
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 7.690417766571045
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.886086940765381
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.402839660644531
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.679451942443848
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 8.481178283691406
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.219592094421387
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.535593032836914
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.372814655303955
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.237918853759766
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 10.041038513183594,
      "learning_rate": 1e-05,
      "loss": 7.2479,
      "step": 1008
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.08104944229126
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 7.404486179351807
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.244776248931885
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.723374366760254
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 7.370257377624512
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.3641252517700195
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.315952777862549
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.128642559051514
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.036430358886719
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 5.0449652671813965
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 5.234369277954102
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.228369235992432
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.7594709396362305
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 7.44128942489624
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.200995445251465
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 5.882501602172852
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 9.559895515441895,
      "learning_rate": 1e-05,
      "loss": 6.4663,
      "step": 1012
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 4.724367618560791
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 5.559926986694336
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 7.216229438781738
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 7.121089458465576
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.787789344787598
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.201678276062012
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.663787364959717
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 6.99213171005249
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 7.7690510749816895
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.907878398895264
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.482990741729736
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 7.241072654724121
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.2811713218688965
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.746082782745361
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.04698371887207
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 7.200124740600586
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 10.433239936828613,
      "learning_rate": 1e-05,
      "loss": 6.8089,
      "step": 1016
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 8.50014877319336
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 5.461256980895996
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 6.957305908203125
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 7.50396203994751
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 6.355972766876221
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 7.653649806976318
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 7.5602216720581055
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 8.097250938415527
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 6.60841703414917
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 5.600517272949219
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.712099075317383
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.394993782043457
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 4.237303256988525
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 5.942808151245117
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.009834289550781
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.011541843414307
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 13.654816627502441,
      "learning_rate": 1e-05,
      "loss": 6.8505,
      "step": 1020
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 5.7924089431762695
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.501516819000244
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.5411529541015625
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 8.314125061035156
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 6.455413818359375
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.414417743682861
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.0845866203308105
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.044702529907227
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.698920249938965
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.773724555969238
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.1285786628723145
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 7.847387313842773
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.4636735916137695
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.389291286468506
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 5.85585355758667
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.383887767791748
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 8.285881042480469,
      "learning_rate": 1e-05,
      "loss": 7.0431,
      "step": 1024
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.822943687438965
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 7.0572829246521
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.449740409851074
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 7.2519330978393555
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 6.51719331741333
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.338062763214111
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.16584587097168
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.173580646514893
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.56890344619751
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 8.209773063659668
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.2873430252075195
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.560556888580322
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.707864284515381
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 5.823641777038574
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.833600044250488
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 8.218318939208984
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 12.471418380737305,
      "learning_rate": 1e-05,
      "loss": 6.9992,
      "step": 1028
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 7.208240985870361
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 7.266573905944824
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 4.696186065673828
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 4.908297538757324
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 7.106673240661621
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.08116626739502
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.204023361206055
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.50902271270752
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.294286251068115
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.718829154968262
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 6.613008499145508
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 6.452101707458496
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.291262149810791
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 5.840047359466553
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.497626781463623
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 7.500667572021484
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 12.266073226928711,
      "learning_rate": 1e-05,
      "loss": 6.8868,
      "step": 1032
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.990556716918945
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 5.555000305175781
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 5.675119876861572
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 7.266265392303467
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.478727340698242
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 6.761635780334473
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.305683612823486
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.112029552459717
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 6.085116386413574
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.093148231506348
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 6.1049699783325195
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.65914249420166
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 6.601381778717041
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.438216686248779
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.827940940856934
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.400212287902832
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 11.598660469055176,
      "learning_rate": 1e-05,
      "loss": 6.8972,
      "step": 1036
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 6.105923175811768
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.433703422546387
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 6.329004287719727
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.754707336425781
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 5.259593486785889
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 5.191527843475342
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 7.280404567718506
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 6.775933742523193
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.614051342010498
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.991091251373291
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.857244491577148
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 8.396681785583496
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 6.568772315979004
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 8.086036682128906
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.344949245452881
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.0377888679504395
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 10.380792617797852,
      "learning_rate": 1e-05,
      "loss": 6.8767,
      "step": 1040
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 7.178520679473877
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.566020488739014
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.640204429626465
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 7.8094000816345215
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 5.617241382598877
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 6.784266471862793
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.283026218414307
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.685701847076416
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.1293230056762695
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.440011978149414
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.385231971740723
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.485172271728516
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.451851844787598
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.494245529174805
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.109187602996826
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.664866924285889
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 9.93511962890625,
      "learning_rate": 1e-05,
      "loss": 7.1703,
      "step": 1044
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.319580554962158
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.672450065612793
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.607071876525879
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 7.855536460876465
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.627927780151367
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 6.867918491363525
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.7895965576171875
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.668528079986572
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 7.69627571105957
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 6.788503170013428
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 5.816336154937744
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 7.832911491394043
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.302175998687744
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 6.334800720214844
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.404547214508057
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.6564531326293945
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 12.097260475158691,
      "learning_rate": 1e-05,
      "loss": 7.14,
      "step": 1048
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.3673624992370605
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 7.345746994018555
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.399808406829834
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.893328666687012
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.641597747802734
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.329206466674805
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.3025922775268555
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.347461700439453
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 6.975097179412842
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 7.418933868408203
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 8.513043403625488
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 7.2217535972595215
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 5.857076644897461
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 6.3144211769104
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 6.755739688873291
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 7.035030841827393
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 9.242719650268555,
      "learning_rate": 1e-05,
      "loss": 7.0449,
      "step": 1052
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 6.895745277404785
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 8.109973907470703
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 5.583008289337158
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 8.251558303833008
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 7.153377532958984
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 5.056821346282959
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 6.79836893081665
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 8.359689712524414
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 7.371549129486084
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 8.227720260620117
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 7.735574722290039
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 8.125937461853027
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 5.980463027954102
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.033767223358154
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 6.632011890411377
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.17477560043335
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 10.914666175842285,
      "learning_rate": 1e-05,
      "loss": 7.1556,
      "step": 1056
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 6.08601713180542
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.758199691772461
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 8.062204360961914
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.48990535736084
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.015381336212158
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 6.642941951751709
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.399835586547852
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.2063493728637695
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.959075450897217
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.305070400238037
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.004550933837891
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 5.697859764099121
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 8.242985725402832
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 5.253163814544678
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 6.840559005737305
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 6.0575852394104
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 10.77590560913086,
      "learning_rate": 1e-05,
      "loss": 6.6264,
      "step": 1060
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.509031772613525
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 8.184059143066406
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 6.0021891593933105
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.411652088165283
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.2971110343933105
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.750975131988525
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.984745979309082
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 5.132136821746826
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.457449436187744
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.116127967834473
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.153439521789551
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.290377140045166
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.057905673980713
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 6.842280387878418
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.3945770263671875
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 6.831722259521484
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 14.613655090332031,
      "learning_rate": 1e-05,
      "loss": 6.9635,
      "step": 1064
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.2669901847839355
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.934587001800537
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 7.670973777770996
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 7.986721515655518
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.8918046951293945
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.162020683288574
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 6.458258628845215
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.468177318572998
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.8539509773254395
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 7.622634410858154
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 7.5419206619262695
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.760746002197266
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 7.96550989151001
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 7.1270623207092285
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 5.981945037841797
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 6.8015594482421875
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 13.775136947631836,
      "learning_rate": 1e-05,
      "loss": 7.1559,
      "step": 1068
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.845040798187256
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 7.283699989318848
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 9.003326416015625
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.6761555671691895
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 6.57176399230957
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 7.327366352081299
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 7.818884372711182
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 8.759390830993652
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.509615898132324
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.089116096496582
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.983151912689209
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 4.655735492706299
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.780440330505371
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 7.716338157653809
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 5.1714396476745605
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.8689470291137695
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 8.392318725585938,
      "learning_rate": 1e-05,
      "loss": 6.9413,
      "step": 1072
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 5.517389297485352
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 8.977272987365723
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 6.809225559234619
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 7.463841915130615
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.447659015655518
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 7.50148344039917
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 5.934373378753662
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.661991596221924
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.052711009979248
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.569423198699951
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.451755046844482
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.64124870300293
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 4.964218616485596
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 6.876412391662598
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 5.77583646774292
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 7.141819477081299
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 9.439066886901855,
      "learning_rate": 1e-05,
      "loss": 6.8617,
      "step": 1076
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 4.808403968811035
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 7.526983261108398
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 9.496932029724121
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 8.357320785522461
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.576632022857666
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.371687889099121
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.517840385437012
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 6.106588840484619
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.547543048858643
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.974145889282227
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.242876052856445
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.707537651062012
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.446581840515137
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 6.468570709228516
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.283873081207275
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.450695991516113
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 9.613053321838379,
      "learning_rate": 1e-05,
      "loss": 7.1178,
      "step": 1080
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.3586225509643555
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.023218154907227
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 6.840522766113281
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.577470779418945
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.662054538726807
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.614591121673584
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.271722316741943
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.777010917663574
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.943894386291504
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 8.010992050170898
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 8.623173713684082
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.850984573364258
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.643616199493408
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.491645336151123
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.125601768493652
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 4.913458347320557
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 8.79106330871582,
      "learning_rate": 1e-05,
      "loss": 7.2955,
      "step": 1084
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.173964023590088
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.297435283660889
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.333782196044922
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 8.542112350463867
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.460133075714111
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 5.85720682144165
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.7959418296813965
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.539281368255615
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.20054817199707
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.184398651123047
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 5.156643390655518
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.109844207763672
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 6.975315570831299
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.874215126037598
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.840130805969238
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 5.93142557144165
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 7.796830177307129,
      "learning_rate": 1e-05,
      "loss": 6.9545,
      "step": 1088
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 6.182673454284668
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 6.938268661499023
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.1496076583862305
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.490072250366211
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 7.772456169128418
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 8.363109588623047
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 6.738237380981445
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 7.197934627532959
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.413341045379639
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 5.644072532653809
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.7704925537109375
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.999139785766602
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 5.939326763153076
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 7.018746376037598
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.948537349700928
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 7.390830039978027
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 10.31556224822998,
      "learning_rate": 1e-05,
      "loss": 6.9348,
      "step": 1092
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.706887245178223
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.8857269287109375
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 7.2255754470825195
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.717510223388672
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 8.182604789733887
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.68071174621582
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.467784404754639
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.960620880126953
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.501740455627441
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 7.283770561218262
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 7.546706199645996
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.932991981506348
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 6.975515365600586
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 6.783822059631348
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 5.917794227600098
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 8.070178031921387
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 8.253833770751953,
      "learning_rate": 1e-05,
      "loss": 7.1775,
      "step": 1096
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 6.7036356925964355
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 9.509157180786133
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 8.868484497070312
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 8.851401329040527
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.902122974395752
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.179004192352295
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.203049182891846
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.357303142547607
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.695252418518066
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.777548313140869
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.571182727813721
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 6.445879936218262
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 7.613519668579102
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.962215900421143
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.133123874664307
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 8.359917640686035
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 10.273412704467773,
      "learning_rate": 1e-05,
      "loss": 7.5083,
      "step": 1100
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.837104797363281
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.864569187164307
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 6.128785610198975
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.110869884490967
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.118786334991455
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 7.5314130783081055
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.923222541809082
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.041040897369385
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.838395118713379
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.815962791442871
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.22232723236084
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 6.440571308135986
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 6.829898834228516
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.788761138916016
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.312337398529053
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.062758445739746
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 10.478727340698242,
      "learning_rate": 1e-05,
      "loss": 7.1167,
      "step": 1104
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.387634754180908
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.007828235626221
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.022501468658447
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 8.141789436340332
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 8.145479202270508
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 7.367302417755127
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 6.961704730987549
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 7.268761157989502
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 7.133292198181152
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 8.854631423950195
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 5.868349552154541
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 6.791266918182373
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 7.084886074066162
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 8.723010063171387
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.407963275909424
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.553590297698975
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 17.42902946472168,
      "learning_rate": 1e-05,
      "loss": 7.295,
      "step": 1108
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 8.86971378326416
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 9.03045654296875
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 6.139096736907959
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 11.144896507263184
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 7.756535053253174
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.81001091003418
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.646246910095215
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 7.881695747375488
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.300922393798828
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 6.151089668273926
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 6.902618885040283
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.260714054107666
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 6.074488639831543
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 8.0690336227417
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 6.1732378005981445
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 5.205733299255371
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 12.11021900177002,
      "learning_rate": 1e-05,
      "loss": 7.3385,
      "step": 1112
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 6.859509468078613
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 7.189981460571289
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 5.786984443664551
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 7.397222995758057
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.497747898101807
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 8.272007942199707
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.611568927764893
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.451854228973389
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.055583477020264
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.863482475280762
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.29779577255249
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 6.202818870544434
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.964652061462402
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.111798286437988
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.046978950500488
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.631354808807373
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 9.776307106018066,
      "learning_rate": 1e-05,
      "loss": 7.2651,
      "step": 1116
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 7.081651210784912
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.910873889923096
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 8.817336082458496
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.902690410614014
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 6.952691078186035
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 7.0165510177612305
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 7.657864093780518
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 8.258048057556152
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 5.2499847412109375
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 8.84226131439209
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 6.827990531921387
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 6.969838619232178
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.397452354431152
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 6.739284992218018
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 6.4374518394470215
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.235106945037842
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 9.97748851776123,
      "learning_rate": 1e-05,
      "loss": 7.2061,
      "step": 1120
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.747211456298828
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.463890075683594
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.142908096313477
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.472138404846191
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 7.741088390350342
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 8.071581840515137
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 6.970482349395752
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 7.932971000671387
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.981753826141357
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.395490646362305
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 6.723923206329346
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.2320332527160645
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.999042510986328
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 7.415954113006592
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.993943691253662
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 7.559228897094727
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 11.001664161682129,
      "learning_rate": 1e-05,
      "loss": 7.4277,
      "step": 1124
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.23488712310791
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.027560234069824
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.145557880401611
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.09623384475708
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.6892170906066895
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 6.903112888336182
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 8.179741859436035
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.517232894897461
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.217609405517578
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 5.93771505355835
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.674633979797363
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 5.596259593963623
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 5.923689365386963
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 7.915530681610107
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 6.266429901123047
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 5.050105571746826
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 15.972588539123535,
      "learning_rate": 1e-05,
      "loss": 6.8985,
      "step": 1128
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 6.348333835601807
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.78695821762085
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.588552951812744
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 5.387616157531738
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 7.091767311096191
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 6.773248672485352
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 6.850849151611328
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 7.6522321701049805
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 5.444101333618164
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 6.200252056121826
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 7.116288185119629
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 6.438629627227783
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.565104961395264
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.404239177703857
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.34057092666626
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 6.69857931137085
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 9.367098808288574,
      "learning_rate": 1e-05,
      "loss": 6.8555,
      "step": 1132
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 7.002165794372559
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 6.947790145874023
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 5.140801429748535
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 6.022055625915527
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.279783725738525
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 6.91513729095459
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.324364185333252
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 6.624607086181641
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.415003776550293
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.767943859100342
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.033409595489502
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.40108585357666
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 7.511155128479004
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 5.5691304206848145
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 6.379209041595459
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 7.140615940093994
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 14.256927490234375,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1136
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 4.5922627449035645
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 8.785099029541016
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 5.730871200561523
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 7.395756244659424
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.6411614418029785
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 8.765597343444824
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.964103698730469
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.796679973602295
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.668231964111328
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.6248955726623535
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 7.353137493133545
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 7.2922043800354
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.626474380493164
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.110000133514404
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.288928031921387
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.043907165527344
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 8.873779296875,
      "learning_rate": 1e-05,
      "loss": 7.23,
      "step": 1140
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.286092758178711
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 6.954174995422363
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.6156229972839355
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.50462532043457
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 6.891541004180908
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 7.732840538024902
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 5.010836124420166
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 6.993682384490967
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.211600303649902
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.3390302658081055
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.563206195831299
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.9002885818481445
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.558388710021973
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.788573741912842
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 5.072666168212891
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.560475826263428
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 13.835308074951172,
      "learning_rate": 1e-05,
      "loss": 6.6865,
      "step": 1144
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 6.744873046875
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 8.118261337280273
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 6.340214252471924
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 7.968196868896484
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 6.053826808929443
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.1634135246276855
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 5.703420162200928
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.777592182159424
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 9.033265113830566
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 7.376434803009033
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.554478168487549
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.756682395935059
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 7.179178237915039
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 7.439974308013916
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 5.618058204650879
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 6.74072790145874
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 10.661195755004883,
      "learning_rate": 1e-05,
      "loss": 7.0355,
      "step": 1148
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 7.415238380432129
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.607497692108154
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 7.08932638168335
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.9643635749816895
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 7.01400089263916
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 6.17967414855957
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 8.01211929321289
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 5.8196539878845215
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.727422714233398
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 7.999715328216553
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 7.391567707061768
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.365152359008789
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.841097354888916
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.818741321563721
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.938418865203857
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.963674545288086
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 11.21865177154541,
      "learning_rate": 1e-05,
      "loss": 6.9467,
      "step": 1152
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.4520583152771
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.823613166809082
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 5.404146194458008
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.487601280212402
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 7.365325927734375
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 7.028332710266113
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 5.7311601638793945
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 5.662105083465576
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 6.568940162658691
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 6.9440765380859375
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 5.710484027862549
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 7.435183525085449
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 6.2931599617004395
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 7.414081573486328
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 9.005419731140137
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 5.952235698699951
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 14.655587196350098,
      "learning_rate": 1e-05,
      "loss": 6.8299,
      "step": 1156
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.423302173614502
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.1964263916015625
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.204663276672363
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.223381519317627
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.98484468460083
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 7.008689880371094
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 5.015990257263184
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.821047782897949
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.089737415313721
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 6.383350372314453
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.270363807678223
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.178927421569824
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 7.80311393737793
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.509444236755371
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.180529594421387
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 7.599409580230713
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 11.047155380249023,
      "learning_rate": 1e-05,
      "loss": 6.9308,
      "step": 1160
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.117722511291504
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.501136302947998
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.803651809692383
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.442080974578857
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 8.038607597351074
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 7.1581010818481445
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 6.982692241668701
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 7.1746087074279785
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.324269771575928
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.806074619293213
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.207949161529541
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 6.2532782554626465
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.651294231414795
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 5.190150737762451
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 5.7953619956970215
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.803294658660889
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 12.522839546203613,
      "learning_rate": 1e-05,
      "loss": 7.0781,
      "step": 1164
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 7.4406914710998535
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 4.573291778564453
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 6.900152683258057
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 6.560080051422119
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 5.820485591888428
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 6.599278926849365
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 6.974061489105225
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 7.717818260192871
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 7.473382472991943
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 7.549659252166748
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 5.05867338180542
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 6.784838676452637
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.417977333068848
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 6.3143839836120605
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.330992698669434
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.574884414672852
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 9.691932678222656,
      "learning_rate": 1e-05,
      "loss": 6.7557,
      "step": 1168
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 6.77662992477417
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.4967803955078125
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 6.533064365386963
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.251468181610107
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 6.018828392028809
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.261981010437012
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.590882778167725
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.463891983032227
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 6.906278610229492
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 6.782890796661377
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 5.343433380126953
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 7.055598735809326
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 6.450456619262695
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.461159706115723
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.000478744506836
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.56417989730835
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 12.962100982666016,
      "learning_rate": 1e-05,
      "loss": 6.9349,
      "step": 1172
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.13246488571167
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 7.09539270401001
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.872598648071289
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.749472141265869
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 5.364550590515137
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 5.177147388458252
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.134965896606445
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.518931865692139
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.903183937072754
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 7.078738689422607
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 7.381341934204102
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.422458648681641
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.336745738983154
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.6642961502075195
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.424095630645752
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.793268203735352
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 10.94908618927002,
      "learning_rate": 1e-05,
      "loss": 6.5031,
      "step": 1176
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.43558931350708
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.163074970245361
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.519376754760742
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 5.832204818725586
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 6.510299205780029
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 4.278517246246338
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 7.555248260498047
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 8.092315673828125
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 8.844780921936035
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.8191633224487305
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 8.341814994812012
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.23590612411499
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.56386137008667
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.186968803405762
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.001454830169678
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 6.793973445892334
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 9.312244415283203,
      "learning_rate": 1e-05,
      "loss": 7.0109,
      "step": 1180
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 5.4906158447265625
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.540551662445068
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.7183074951171875
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.339768409729004
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 6.320673942565918
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 8.197400093078613
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 5.233614921569824
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 7.218170166015625
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 4.234715461730957
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 7.325976371765137
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 5.98531436920166
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 5.788468837738037
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.40206241607666
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.339653015136719
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 6.313889026641846
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 6.6097846031188965
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 8.467920303344727,
      "learning_rate": 1e-05,
      "loss": 6.6287,
      "step": 1184
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.63134765625
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.8272318840026855
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.162258148193359
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 8.674830436706543
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.109567165374756
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.421520709991455
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 8.160548210144043
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 7.455776691436768
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 6.420004367828369
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.29403829574585
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 5.6046013832092285
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.489034175872803
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.136376857757568
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 6.963831424713135
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.355506420135498
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.515232563018799
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 11.724323272705078,
      "learning_rate": 1e-05,
      "loss": 6.8264,
      "step": 1188
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 6.033239841461182
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 4.48406457901001
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 7.264068126678467
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 8.6771879196167
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 7.208704471588135
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 8.120426177978516
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 6.957635879516602
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 6.636363983154297
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.148530960083008
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 5.273324012756348
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 6.88399600982666
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.364534854888916
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 7.548931121826172
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.850070476531982
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.5268683433532715
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.222479820251465
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 9.511556625366211,
      "learning_rate": 1e-05,
      "loss": 6.825,
      "step": 1192
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 6.714018821716309
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 7.807712078094482
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 7.41910982131958
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 5.798588275909424
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.615363597869873
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.158195972442627
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.69288444519043
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 5.74700927734375
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.784201622009277
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.223033428192139
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.455259799957275
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 7.301753997802734
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 8.444475173950195
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.213710784912109
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.037543773651123
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.432907581329346
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 12.161343574523926,
      "learning_rate": 1e-05,
      "loss": 7.0529,
      "step": 1196
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 7.6507368087768555
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 6.043193817138672
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 7.209499359130859
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 6.354310989379883
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 6.846682071685791
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.209164142608643
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 5.90344762802124
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.13301420211792
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 7.744858264923096
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.225622653961182
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 7.490937232971191
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.8980393409729
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 5.608983993530273
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.820324420928955
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.090577125549316
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.783963680267334
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 21.024232864379883,
      "learning_rate": 1e-05,
      "loss": 6.7508,
      "step": 1200
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 7.310865879058838
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 7.290266513824463
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 6.54668664932251
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 6.961453437805176
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.960027694702148
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 5.793966293334961
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.493354320526123
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.544284820556641
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 4.545827865600586
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 8.087959289550781
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 6.832186222076416
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 7.036766529083252
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 5.860195159912109
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 8.513921737670898
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.195688724517822
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.112873077392578
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 12.02924633026123,
      "learning_rate": 1e-05,
      "loss": 6.8179,
      "step": 1204
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 7.0836896896362305
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.9020094871521
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 7.006012439727783
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.341948986053467
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.273036956787109
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.720661163330078
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 7.189017295837402
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 7.507137298583984
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.583150386810303
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.950738906860352
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.556240081787109
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.301921844482422
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.896803379058838
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 7.059595584869385
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.461666584014893
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 7.826074123382568
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 11.116612434387207,
      "learning_rate": 1e-05,
      "loss": 6.9162,
      "step": 1208
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.442145347595215
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.006429672241211
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.307609558105469
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.538306713104248
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.71394157409668
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 5.6306843757629395
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.643363952636719
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.408754825592041
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 6.284713268280029
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 8.127771377563477
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 8.222599983215332
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 7.12717342376709
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 7.229034423828125
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 7.639761924743652
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.236316204071045
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.9902472496032715
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 12.58489990234375,
      "learning_rate": 1e-05,
      "loss": 7.2218,
      "step": 1212
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.4343109130859375
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 6.683711528778076
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.106671333312988
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 6.827040195465088
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 7.5085554122924805
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 6.699880599975586
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 6.732553482055664
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 5.975132465362549
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 7.537517547607422
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 6.29100227355957
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 6.629748344421387
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 7.53555965423584
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.030120372772217
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.901857852935791
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 5.911977291107178
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.630941390991211
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 10.77598762512207,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 1216
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.552185535430908
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.420846939086914
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 8.370104789733887
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 6.470491409301758
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 6.05180549621582
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 5.329020023345947
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.885063648223877
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.591660976409912
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 6.759666919708252
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 4.971434116363525
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 7.4828901290893555
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 6.880161285400391
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.736654758453369
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.319893836975098
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.946145534515381
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 7.134067058563232
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 9.612577438354492,
      "learning_rate": 1e-05,
      "loss": 6.8689,
      "step": 1220
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 6.826146602630615
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.397317886352539
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.595570087432861
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 6.639533519744873
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 6.947284698486328
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 8.150077819824219
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 6.278438091278076
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 5.025537014007568
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 5.157489776611328
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 7.755422592163086
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 5.737362384796143
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 6.784856796264648
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 6.959603786468506
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.3351898193359375
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.8290181159973145
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 5.880040645599365
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 10.695844650268555,
      "learning_rate": 1e-05,
      "loss": 6.7687,
      "step": 1224
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 4.799084663391113
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.671602725982666
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 7.413687705993652
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.597569942474365
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 6.580340385437012
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 6.912883281707764
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 7.479572296142578
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 8.901373863220215
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.327216625213623
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.238004207611084
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.1274895668029785
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.632993698120117
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 6.511383056640625
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.0075812339782715
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.63429594039917
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 8.311016082763672
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 10.96148681640625,
      "learning_rate": 1e-05,
      "loss": 7.0091,
      "step": 1228
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 6.845617294311523
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 7.200986862182617
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 6.3055100440979
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 7.109217166900635
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.220516681671143
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.481105804443359
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 4.863149642944336
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 8.252840995788574
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.092360973358154
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 6.8711628913879395
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.700523853302002
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.846122741699219
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 8.060955047607422
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.129741668701172
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 7.361046314239502
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.358890056610107
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 11.890700340270996,
      "learning_rate": 1e-05,
      "loss": 7.0437,
      "step": 1232
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.636552333831787
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 7.303225040435791
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 7.946299076080322
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.062041759490967
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 6.9236602783203125
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.498474597930908
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.441540718078613
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 6.918821334838867
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 7.993755340576172
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.9297566413879395
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 8.009839057922363
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.498166561126709
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.161844730377197
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.106975078582764
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.207282066345215
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.765815734863281
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 9.107978820800781,
      "learning_rate": 1e-05,
      "loss": 7.0878,
      "step": 1236
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 5.849630355834961
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.371317386627197
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.191779136657715
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.019562721252441
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 5.905892372131348
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 6.11245059967041
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.026834964752197
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.0406646728515625
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.710328578948975
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.472901344299316
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.818559646606445
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 5.996326923370361
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 7.8045759201049805
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 3.9682514667510986
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 8.040393829345703
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 4.91656494140625
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 11.770962715148926,
      "learning_rate": 1e-05,
      "loss": 6.3279,
      "step": 1240
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 8.293050765991211
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.436187744140625
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.488138198852539
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.439058303833008
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 8.052603721618652
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 6.454054355621338
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 7.150751113891602
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 6.504859447479248
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 5.96629524230957
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.078763008117676
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 6.796637535095215
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.0615692138671875
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.777421474456787
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.386005878448486
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 7.266984462738037
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 5.806630611419678
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 12.314183235168457,
      "learning_rate": 1e-05,
      "loss": 6.9974,
      "step": 1244
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 8.345503807067871
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 6.465434551239014
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 6.599984645843506
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 7.862142086029053
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.071755409240723
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.470027446746826
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 6.132706165313721
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.22521448135376
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.572267055511475
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 6.919271469116211
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.696727275848389
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.447562217712402
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.081358432769775
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.83201265335083
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.250486373901367
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.757256031036377
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 10.899338722229004,
      "learning_rate": 1e-05,
      "loss": 7.2956,
      "step": 1248
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 6.986612319946289
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.579284191131592
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.088249206542969
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.446885585784912
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.8246283531188965
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 5.281497955322266
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.333003520965576
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 7.709160327911377
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.3298821449279785
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.8211469650268555
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 6.6198906898498535
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 6.729305267333984
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 6.483108997344971
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 7.198068141937256
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 8.65249252319336
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 7.09812068939209
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 11.298846244812012,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 1252
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.12604284286499
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.181591510772705
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 6.7035064697265625
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 5.914373397827148
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.364342212677002
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.50762939453125
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 6.099970817565918
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.483819484710693
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 5.518998622894287
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.171819686889648
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 8.104472160339355
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.613284587860107
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 7.07728910446167
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 8.95007038116455
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 8.343103408813477
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 7.3635406494140625
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 12.329615592956543,
      "learning_rate": 1e-05,
      "loss": 7.2202,
      "step": 1256
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.354058742523193
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.0200276374816895
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 5.931285858154297
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 6.522705078125
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.487317085266113
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 6.218979358673096
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.860516548156738
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.272420883178711
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 7.958540439605713
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 6.471065044403076
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 5.800843238830566
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 7.129497528076172
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 6.205568313598633
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.711884498596191
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.568289279937744
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.252121448516846
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 9.190211296081543,
      "learning_rate": 1e-05,
      "loss": 6.9853,
      "step": 1260
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.832106590270996
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 7.264849662780762
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 7.250072479248047
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.596536636352539
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.22640323638916
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 6.633586406707764
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 8.099926948547363
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.192752838134766
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.716578006744385
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.886001110076904
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 8.187666893005371
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.324018955230713
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 9.200560569763184
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.916619777679443
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 7.176090240478516
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.026546478271484
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 12.51988697052002,
      "learning_rate": 1e-05,
      "loss": 7.3456,
      "step": 1264
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 7.486380100250244
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 5.772532939910889
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 6.485970497131348
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 7.2397780418396
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 5.14519739151001
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 7.872018814086914
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.7055792808532715
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.634101390838623
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 6.431583404541016
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.227764129638672
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 6.4657368659973145
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.430370330810547
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.364115238189697
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.20124626159668
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.181028842926025
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 6.1012349128723145
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 10.627791404724121,
      "learning_rate": 1e-05,
      "loss": 6.7965,
      "step": 1268
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.191298961639404
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.381189823150635
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 7.455287456512451
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.20367431640625
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.4449357986450195
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 6.958320140838623
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.434619903564453
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.020669460296631
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.991962432861328
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 6.396665573120117
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.261954307556152
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 5.437427043914795
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.299627780914307
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.944193363189697
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.804725646972656
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 7.047982215881348
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 8.057882308959961,
      "learning_rate": 1e-05,
      "loss": 6.8297,
      "step": 1272
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.779261112213135
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.536351680755615
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 6.552501678466797
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 5.787855625152588
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 7.11043643951416
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 7.0965895652771
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.974819183349609
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.270234107971191
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 7.228073596954346
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 7.212876319885254
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.7968645095825195
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.230345726013184
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 4.732089042663574
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.295330047607422
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.884389400482178
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 5.155648708343506
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 12.547107696533203,
      "learning_rate": 1e-05,
      "loss": 6.4777,
      "step": 1276
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.297157287597656
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.182052135467529
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.6082000732421875
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.382030010223389
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.652834415435791
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.359307765960693
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.649117469787598
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 5.324779510498047
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 6.535033702850342
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.647547245025635
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 6.056833744049072
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.196736812591553
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 7.4795966148376465
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.748595714569092
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 8.200261116027832
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.433755874633789
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 10.212786674499512,
      "learning_rate": 1e-05,
      "loss": 6.9221,
      "step": 1280
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 8.760422706604004
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.414706707000732
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.854870796203613
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.5017991065979
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.2580766677856445
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.865551948547363
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 6.386936187744141
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 6.07961368560791
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 8.077245712280273
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 7.739903926849365
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 7.058558464050293
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 8.166058540344238
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.019698143005371
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.248083114624023
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 6.623615741729736
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.137217998504639
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 11.437820434570312,
      "learning_rate": 1e-05,
      "loss": 7.387,
      "step": 1284
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 6.342959880828857
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.3350830078125
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.493411064147949
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.449929237365723
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.68914794921875
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.920162677764893
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.010358810424805
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 4.9592437744140625
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.716350555419922
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 7.166469097137451
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.167840957641602
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 7.282151699066162
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 4.951041221618652
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 4.914383888244629
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 5.4579596519470215
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 7.177332878112793
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 11.151546478271484,
      "learning_rate": 1e-05,
      "loss": 6.4396,
      "step": 1288
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 8.484631538391113
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 7.470307350158691
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 7.935141563415527
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 6.906927108764648
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.911740779876709
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.860156059265137
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 6.713443756103516
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.7943501472473145
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 4.562552452087402
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.292029857635498
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 6.060975551605225
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.09102725982666
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 6.987026214599609
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.813706874847412
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.327681541442871
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 6.3886823654174805
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 9.337859153747559,
      "learning_rate": 1e-05,
      "loss": 6.7875,
      "step": 1292
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.765022277832031
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 7.07673978805542
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.924882888793945
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.3975510597229
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.109369277954102
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.5706257820129395
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 6.070343017578125
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.163394451141357
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.371772289276123
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 6.317259311676025
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.800706386566162
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.093240261077881
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.069164276123047
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 6.665639400482178
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.378109931945801
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 4.296513557434082
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 10.117953300476074,
      "learning_rate": 1e-05,
      "loss": 6.8169,
      "step": 1296
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.673667907714844
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.871021747589111
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 7.358849048614502
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.731753826141357
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.8480377197265625
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 7.191940784454346
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.602221488952637
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 7.805811405181885
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.199617862701416
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.7522993087768555
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 8.09804630279541
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.324092864990234
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 7.170595169067383
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 6.018655776977539
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 7.029481887817383
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 8.119815826416016
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 11.177828788757324,
      "learning_rate": 1e-05,
      "loss": 7.1747,
      "step": 1300
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 5.457675457000732
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 7.0447916984558105
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 4.75991153717041
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 7.154180526733398
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.460989475250244
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.905231475830078
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 7.632200717926025
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 7.022422790527344
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.462324142456055
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.509594917297363
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.52032470703125
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.169502258300781
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 7.371226787567139
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.745395660400391
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 7.349982738494873
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.67792272567749
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 10.226861000061035,
      "learning_rate": 1e-05,
      "loss": 6.8902,
      "step": 1304
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.015510559082031
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.3602118492126465
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.850759506225586
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.6882734298706055
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.742886543273926
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.813981533050537
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.784328937530518
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.833166599273682
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 5.909376621246338
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 6.2184247970581055
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 7.927612781524658
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 8.04352855682373
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 7.735752105712891
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 5.815115451812744
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 6.124726295471191
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 5.417520999908447
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 13.038675308227539,
      "learning_rate": 1e-05,
      "loss": 6.7676,
      "step": 1308
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.480415344238281
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.557668685913086
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.134233474731445
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.072049617767334
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.505715847015381
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.311806678771973
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 5.496044635772705
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.561929702758789
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 7.156541347503662
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 5.872918128967285
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.320230484008789
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.041848659515381
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.231814861297607
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 4.949749946594238
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.612771987915039
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.236177921295166
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 11.853654861450195,
      "learning_rate": 1e-05,
      "loss": 6.5339,
      "step": 1312
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.232537746429443
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.563932418823242
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.863009929656982
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 7.182210445404053
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 6.934208869934082
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.005838871002197
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 6.401938438415527
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.347143650054932
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 4.7606892585754395
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 7.073276042938232
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 9.667080879211426
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 5.672601222991943
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 7.4445343017578125
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.6006550788879395
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 5.335942268371582
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.92181921005249
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 11.346056938171387,
      "learning_rate": 1e-05,
      "loss": 6.7505,
      "step": 1316
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 6.487114429473877
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 7.314789295196533
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 3.908294677734375
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 7.076794147491455
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.004128932952881
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.6794657707214355
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 8.069730758666992
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 6.201910495758057
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.755342960357666
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.571633815765381
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 5.47178840637207
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.042029857635498
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 7.956708908081055
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 7.140070915222168
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 6.5484795570373535
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 6.887307167053223
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 11.570833206176758,
      "learning_rate": 1e-05,
      "loss": 6.8822,
      "step": 1320
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 6.982356548309326
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 6.907086372375488
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.332438945770264
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.667771339416504
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.753884315490723
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 7.441008567810059
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.996374130249023
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 5.1016106605529785
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.035507678985596
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.753342151641846
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 6.813234329223633
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.146568298339844
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 7.718034267425537
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 7.454529285430908
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 6.284377098083496
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 6.2020769119262695
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 12.721735954284668,
      "learning_rate": 1e-05,
      "loss": 6.9744,
      "step": 1324
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.726288795471191
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 6.038079738616943
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.662810325622559
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 6.4698309898376465
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 7.829518795013428
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 4.62178897857666
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 4.727189064025879
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 5.835144996643066
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.082463264465332
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.051929473876953
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 6.762205123901367
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.034854412078857
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.242672443389893
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 6.980411529541016
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 6.1357855796813965
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.259984970092773
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 10.993103981018066,
      "learning_rate": 1e-05,
      "loss": 6.6538,
      "step": 1328
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.100094318389893
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.4005632400512695
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.480111598968506
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 7.13843297958374
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.69028377532959
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 4.571645736694336
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 7.5363450050354
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.548077583312988
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.861446857452393
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.1627197265625
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.439582347869873
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 7.292685508728027
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 7.195518970489502
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 5.6012282371521
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 6.921881198883057
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 7.73827600479126
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 10.853577613830566,
      "learning_rate": 1e-05,
      "loss": 6.6049,
      "step": 1332
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 6.523479461669922
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 6.7955217361450195
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 7.789525508880615
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 5.979030132293701
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 7.096251010894775
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.96211576461792
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 7.657576084136963
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.919751167297363
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 8.351832389831543
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.949306964874268
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.74993896484375
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 7.515335559844971
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 5.78221321105957
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.265688419342041
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.246512413024902
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.708591938018799
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 11.682160377502441,
      "learning_rate": 1e-05,
      "loss": 7.0808,
      "step": 1336
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 6.824801445007324
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 7.007654666900635
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 7.415992736816406
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 8.032255172729492
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 5.8042778968811035
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.809566974639893
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 5.978530406951904
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.352853298187256
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.228477954864502
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 5.611088752746582
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 5.435359477996826
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.7648138999938965
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 6.945111274719238
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.901163101196289
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.842253684997559
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.210491180419922
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 10.606245994567871,
      "learning_rate": 1e-05,
      "loss": 6.7603,
      "step": 1340
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 6.311218738555908
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 5.217872142791748
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 8.194448471069336
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 7.878105163574219
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 11.6558837890625
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.836665153503418
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.588724613189697
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 5.229272365570068
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 5.667141914367676
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 7.593451976776123
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.007673740386963
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.426514148712158
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 7.989998817443848
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.5093231201171875
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 5.574617385864258
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.440223217010498
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 13.454280853271484,
      "learning_rate": 1e-05,
      "loss": 6.8826,
      "step": 1344
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 8.032011032104492
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 9.014286994934082
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 7.4970221519470215
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 6.96103572845459
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.952782154083252
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 7.255221366882324
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 7.723636150360107
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.618013858795166
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 5.940160751342773
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 6.145793914794922
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 7.313621997833252
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 7.170774459838867
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 7.1098151206970215
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.732095241546631
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.519405841827393
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 8.106035232543945
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 13.475750923156738,
      "learning_rate": 1e-05,
      "loss": 7.1932,
      "step": 1348
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.935546875
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.606312274932861
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.7403883934021
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 7.593976974487305
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 7.560909748077393
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 8.948156356811523
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 6.484177112579346
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 7.438445091247559
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.189422607421875
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.638350963592529
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.381503105163574
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 6.686741828918457
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 7.805331707000732
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 5.934804439544678
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 5.950994491577148
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 6.139565467834473
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 14.636817932128906,
      "learning_rate": 1e-05,
      "loss": 7.0647,
      "step": 1352
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.852400779724121
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.8775177001953125
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.238414764404297
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 4.532907485961914
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.531967639923096
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.753028392791748
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.0324273109436035
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.654078483581543
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 8.870500564575195
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 7.951671123504639
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 5.697400093078613
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 6.8311967849731445
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 7.4813551902771
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.989306449890137
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.480403900146484
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 5.853555202484131
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 9.848360061645508,
      "learning_rate": 1e-05,
      "loss": 6.9143,
      "step": 1356
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 7.160450458526611
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 9.265316009521484
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 5.6797590255737305
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 7.725698471069336
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.271605014801025
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.655755043029785
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 7.683396339416504
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 7.1551361083984375
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.465001583099365
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.998783588409424
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.759510040283203
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.5449676513671875
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.773847579956055
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 7.480836868286133
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.200096130371094
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.834667205810547
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 7.9464874267578125,
      "learning_rate": 1e-05,
      "loss": 6.8534,
      "step": 1360
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.602974891662598
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.837883472442627
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.169592380523682
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.7832794189453125
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 5.615916728973389
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 6.291788101196289
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 7.007497787475586
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 7.599608898162842
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.297563552856445
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.689769744873047
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 7.952274322509766
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.103635787963867
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 6.959481716156006
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 7.166902542114258
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 8.091177940368652
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 4.3638129234313965
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 12.043890953063965,
      "learning_rate": 1e-05,
      "loss": 6.6583,
      "step": 1364
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.939330101013184
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.968878269195557
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.055292129516602
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 5.49023962020874
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 6.542580604553223
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 6.122120380401611
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 5.168903350830078
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 4.701756477355957
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.169476509094238
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.45853853225708
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.703368186950684
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.924389839172363
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 7.750249862670898
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.013692855834961
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.545732498168945
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.221172332763672
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 14.070898056030273,
      "learning_rate": 1e-05,
      "loss": 6.9235,
      "step": 1368
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 7.032261848449707
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 9.290417671203613
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 6.489999294281006
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 5.302497863769531
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 8.262619972229004
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 5.9532790184021
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.153698921203613
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.275404453277588
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 5.103616714477539
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 7.458254337310791
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 6.560132026672363
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 7.353178977966309
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 4.871937274932861
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.661942958831787
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 7.106959819793701
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.760831832885742
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 10.26778793334961,
      "learning_rate": 1e-05,
      "loss": 6.6648,
      "step": 1372
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 7.379616737365723
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 6.360511302947998
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 5.747204780578613
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 4.835597515106201
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 5.931272506713867
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.717137336730957
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 7.324906826019287
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.816966533660889
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 6.761072635650635
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 5.644361972808838
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 7.094188690185547
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 6.382448673248291
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.319972038269043
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.047584056854248
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 6.7177228927612305
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.689339637756348
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 10.375510215759277,
      "learning_rate": 1e-05,
      "loss": 6.6106,
      "step": 1376
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.2303900718688965
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.533024787902832
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.985009670257568
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.577392101287842
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 7.68538761138916
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 7.098452568054199
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 5.7065534591674805
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 6.18903923034668
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 7.139894008636475
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 4.7996015548706055
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.853250503540039
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.596813678741455
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 8.015609741210938
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.114710330963135
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.386952877044678
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 7.221292495727539
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 13.035935401916504,
      "learning_rate": 1e-05,
      "loss": 6.7583,
      "step": 1380
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.888249397277832
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.865950107574463
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.622276306152344
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 5.8083086013793945
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 7.886032581329346
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 5.771447658538818
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 7.9803466796875
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 6.093290328979492
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.862654685974121
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 7.529066562652588
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 7.836895942687988
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.23337459564209
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 6.84740686416626
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 5.606608867645264
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 8.038259506225586
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 5.8188982009887695
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 9.299691200256348,
      "learning_rate": 1e-05,
      "loss": 6.9806,
      "step": 1384
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 7.616774559020996
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 6.244619369506836
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 5.866208553314209
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 7.60125207901001
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 6.241443157196045
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.052032947540283
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.680506706237793
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 6.904367446899414
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 6.455206394195557
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 7.218884468078613
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 8.393888473510742
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 6.222735404968262
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 8.01293659210205
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 6.5095977783203125
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 8.192197799682617
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 7.696662902832031
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 12.596505165100098,
      "learning_rate": 1e-05,
      "loss": 7.1193,
      "step": 1388
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 6.2796735763549805
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 5.602993965148926
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 7.855165958404541
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 6.567735195159912
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 6.039257526397705
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 6.491916656494141
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.013677597045898
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.5685296058654785
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 5.985456943511963
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 7.040940284729004
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 6.3586835861206055
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 7.661607265472412
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.847589492797852
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.510908126831055
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.205118656158447
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 6.293356895446777
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 11.011350631713867,
      "learning_rate": 1e-05,
      "loss": 6.8327,
      "step": 1392
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 7.618500232696533
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 8.685961723327637
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 6.603429317474365
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 6.9169511795043945
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 8.606983184814453
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 6.6667890548706055
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 5.856312274932861
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 6.630408763885498
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.250106334686279
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.8623833656311035
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 6.544172286987305
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.752744674682617
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 5.4434003829956055
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 7.390340328216553
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 5.347696304321289
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 7.026673316955566
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 15.36958122253418,
      "learning_rate": 1e-05,
      "loss": 7.0127,
      "step": 1396
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 6.992153644561768
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 5.949040412902832
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 6.59953498840332
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 7.455568790435791
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 7.74334716796875
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 6.146509647369385
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 8.783475875854492
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 6.798627853393555
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 8.551275253295898
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 7.110095977783203
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 6.742674827575684
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 6.710419654846191
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 5.862302780151367
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 6.996175289154053
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.88018274307251
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.524196624755859
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 14.112421035766602,
      "learning_rate": 1e-05,
      "loss": 7.1153,
      "step": 1400
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 5.992388725280762
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 7.210493564605713
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.33189058303833
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.761303424835205
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.101006031036377
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 6.568661689758301
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.2070512771606445
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.6570587158203125
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.533921718597412
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.505926609039307
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 7.503663539886475
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.131793022155762
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.460870742797852
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 5.904085159301758
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.846635341644287
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.712334632873535
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 12.288518905639648,
      "learning_rate": 1e-05,
      "loss": 6.7143,
      "step": 1404
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.981381416320801
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 7.826851844787598
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.63201379776001
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 7.499067306518555
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.203002452850342
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 7.393346309661865
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.631730556488037
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.927936553955078
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 7.355736255645752
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 6.747978687286377
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 6.135331153869629
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 7.348727226257324
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 7.295470714569092
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 6.6481032371521
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 7.4384765625
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 6.247075080871582
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 12.232086181640625,
      "learning_rate": 1e-05,
      "loss": 6.957,
      "step": 1408
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 7.263253688812256
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 6.4109787940979
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 8.182090759277344
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 6.4318084716796875
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 5.708280086517334
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.526701927185059
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.864998817443848
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 5.122880935668945
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.540657043457031
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.9968180656433105
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 8.707228660583496
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 7.4557061195373535
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.013806343078613
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.476376056671143
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 6.754803657531738
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.189484119415283
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 15.738781929016113,
      "learning_rate": 1e-05,
      "loss": 7.0404,
      "step": 1412
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 8.292672157287598
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 6.799072265625
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.051100730895996
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.666577339172363
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 6.065462112426758
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 7.1452813148498535
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 7.011553764343262
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 6.85711145401001
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 7.57594633102417
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 8.051820755004883
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 6.132729530334473
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 6.795983791351318
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 5.390820026397705
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.186334609985352
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.031548023223877
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 6.626773834228516
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 8.29749584197998,
      "learning_rate": 1e-05,
      "loss": 6.98,
      "step": 1416
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.3575873374938965
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.601925373077393
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 7.452220439910889
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.345230579376221
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.616097927093506
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 5.663667678833008
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.896946907043457
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.977719783782959
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 4.830687999725342
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.663933277130127
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.446840763092041
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 8.051241874694824
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.585836887359619
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 5.4851155281066895
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.524850368499756
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 7.347733974456787
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 8.4509916305542,
      "learning_rate": 1e-05,
      "loss": 6.8655,
      "step": 1420
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 7.645861625671387
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.119181156158447
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 7.079891681671143
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.737837791442871
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.554779052734375
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.771641254425049
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.011497974395752
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 7.1961445808410645
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 7.413820266723633
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 6.713561058044434
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 8.936905860900879
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 8.005341529846191
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 6.128911018371582
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 5.007757663726807
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 6.544111251831055
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 7.572416305541992
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 16.256132125854492,
      "learning_rate": 1e-05,
      "loss": 6.9025,
      "step": 1424
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.195235729217529
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.7447123527526855
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.836194038391113
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.735487937927246
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 7.170560359954834
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.743103981018066
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 5.835610389709473
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.476447105407715
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 5.512821197509766
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 7.240321636199951
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 5.621574878692627
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 7.67365837097168
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.055909633636475
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.57295036315918
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.571426868438721
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 6.748170375823975
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 9.082473754882812,
      "learning_rate": 1e-05,
      "loss": 6.9209,
      "step": 1428
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 8.947654724121094
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.081883430480957
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.192691802978516
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.025510787963867
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.269570350646973
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 6.722842693328857
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.818140506744385
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.122247219085693
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 6.903950214385986
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 8.25213623046875
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 5.994275093078613
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 7.0283613204956055
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.533544063568115
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 6.837000846862793
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.196258068084717
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 6.8776631355285645
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 9.4660062789917,
      "learning_rate": 1e-05,
      "loss": 6.9877,
      "step": 1432
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.038643836975098
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 7.3765058517456055
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.340682506561279
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.101675987243652
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 4.499532699584961
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 7.082478046417236
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 8.09986400604248
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 6.814929485321045
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.543132781982422
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 6.280941009521484
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 6.535815715789795
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.031418323516846
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 6.725866794586182
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 7.475240230560303
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 7.3349504470825195
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 5.909976482391357
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 10.810956001281738,
      "learning_rate": 1e-05,
      "loss": 6.6995,
      "step": 1436
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.327454090118408
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.766556262969971
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.3729658126831055
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 7.490363597869873
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.372931957244873
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.885336875915527
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 5.889808177947998
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.572299480438232
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 5.253669738769531
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 4.77220344543457
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 7.3003249168396
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 7.25508451461792
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.625039100646973
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 5.542277812957764
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 6.593131065368652
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.420450210571289
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 11.003679275512695,
      "learning_rate": 1e-05,
      "loss": 6.715,
      "step": 1440
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 7.196735858917236
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 7.127843379974365
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 5.491322040557861
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 6.404238224029541
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 7.79624080657959
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.137145519256592
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 7.42323112487793
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.737654685974121
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 5.563323497772217
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.361433506011963
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.376361846923828
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.301097393035889
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 6.715987682342529
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.577033519744873
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.312084197998047
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.304893493652344
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 13.495100021362305,
      "learning_rate": 1e-05,
      "loss": 6.9267,
      "step": 1444
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.1951680183410645
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.3375115394592285
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.264612197875977
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.721154689788818
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.054561138153076
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 8.185332298278809
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.015085220336914
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.509518146514893
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.873014450073242
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 6.107041835784912
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.505728721618652
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.5776686668396
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.2978949546813965
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.468022346496582
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 8.27284049987793
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.04074239730835
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 12.019756317138672,
      "learning_rate": 1e-05,
      "loss": 7.4016,
      "step": 1448
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.734931468963623
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 5.778878688812256
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.201860427856445
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.017196178436279
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.406534671783447
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.5368170738220215
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.59942626953125
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 8.965322494506836
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.203126430511475
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 6.716803073883057
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.687141418457031
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.343740463256836
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 5.106528282165527
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 6.790108680725098
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 7.62070894241333
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 4.73736047744751
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 10.74943733215332,
      "learning_rate": 1e-05,
      "loss": 7.0279,
      "step": 1452
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 6.954719543457031
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 7.396217346191406
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 5.288550853729248
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 6.314439296722412
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 3.955944061279297
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 7.0616679191589355
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 6.139200687408447
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 6.722194194793701
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 7.280148029327393
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 7.2019243240356445
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 6.922427654266357
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 8.802499771118164
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 6.642228603363037
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 8.066892623901367
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.02841854095459
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.496020317077637
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 12.797309875488281,
      "learning_rate": 1e-05,
      "loss": 6.8296,
      "step": 1456
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 7.205528736114502
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 7.247345924377441
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 6.55270528793335
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 8.708107948303223
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 6.092589378356934
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 7.735528469085693
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 5.23781681060791
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 8.45285701751709
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.191458702087402
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.621027946472168
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.36692476272583
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 6.306450843811035
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 7.202468395233154
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 7.312747478485107
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 6.302276611328125
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 5.934462070465088
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 11.152758598327637,
      "learning_rate": 1e-05,
      "loss": 7.0294,
      "step": 1460
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 5.056506156921387
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 7.494815826416016
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 8.361584663391113
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 6.1710333824157715
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.0336408615112305
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.116451740264893
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.633138179779053
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.489717483520508
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 7.376957893371582
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 8.62699031829834
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 6.946261405944824
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 8.145672798156738
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.755641937255859
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.857307434082031
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 7.743814945220947
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 5.142797946929932
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 10.66592788696289,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 1464
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.058120250701904
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 4.814089298248291
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.53451681137085
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 8.563786506652832
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 6.0748772621154785
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 8.209442138671875
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 7.295759201049805
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 6.6278977394104
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 8.239376068115234
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 6.1779327392578125
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 5.073973655700684
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 6.142087936401367
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 6.691007137298584
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.284801006317139
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 4.924217700958252
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.509249210357666
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 12.7317533493042,
      "learning_rate": 1e-05,
      "loss": 6.7638,
      "step": 1468
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.121735572814941
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 6.5940728187561035
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 6.694380760192871
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.148159980773926
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.489004135131836
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 8.725561141967773
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.553973197937012
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.739065170288086
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 8.40593433380127
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 5.018245697021484
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 6.7906904220581055
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 5.705933094024658
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.093111991882324
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 6.2558112144470215
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.459235668182373
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.077858924865723
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 8.393448829650879,
      "learning_rate": 1e-05,
      "loss": 6.867,
      "step": 1472
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 6.8415846824646
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.41372537612915
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.768524646759033
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 8.140840530395508
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.01556396484375
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.273509979248047
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.372488021850586
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.0927019119262695
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 7.0975117683410645
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.357959747314453
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.45302677154541
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 5.716552257537842
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 5.994971752166748
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.062013626098633
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.632230281829834
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.090371608734131
    },
    {
      "epoch": 0.32,
      "grad_norm": 12.0968599319458,
      "learning_rate": 1e-05,
      "loss": 6.8327,
      "step": 1476
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.816427707672119
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 6.530907154083252
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.095999240875244
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.457586288452148
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.580542087554932
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.009202003479004
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 6.824170112609863
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.42884635925293
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.802205562591553
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 7.146924018859863
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.786345481872559
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.708792686462402
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 4.88918924331665
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.047660827636719
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.239055633544922
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 6.109212875366211
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 10.484597206115723,
      "learning_rate": 1e-05,
      "loss": 6.9046,
      "step": 1480
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 7.297184467315674
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 5.630864143371582
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 8.033440589904785
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 7.6650238037109375
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.907752990722656
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.331697463989258
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 5.866756439208984
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.63816499710083
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 7.371243476867676
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 6.892439842224121
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 7.002565860748291
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 5.080848693847656
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.895777702331543
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.072266101837158
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 8.462347030639648
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 6.593554973602295
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 14.166592597961426,
      "learning_rate": 1e-05,
      "loss": 6.9214,
      "step": 1484
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 8.11735725402832
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 6.722397804260254
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.57069206237793
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.467612266540527
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 5.948267459869385
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 8.04246997833252
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 7.620236873626709
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 6.42311429977417
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.935181140899658
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 6.865161895751953
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.7514495849609375
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.156529426574707
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 5.679855823516846
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 7.2400898933410645
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 6.466557502746582
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 9.490569114685059
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 13.826894760131836,
      "learning_rate": 1e-05,
      "loss": 7.2811,
      "step": 1488
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 5.125198841094971
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.309676647186279
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.505801677703857
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.370552062988281
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.993795394897461
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 7.037813663482666
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.305518627166748
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 7.9220967292785645
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.260763645172119
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.439047813415527
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 7.565482139587402
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 7.141110897064209
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.279150009155273
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.92315149307251
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 5.8375935554504395
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.845547676086426
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 10.373464584350586,
      "learning_rate": 1e-05,
      "loss": 6.6164,
      "step": 1492
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 8.311989784240723
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 6.872740268707275
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 7.22636079788208
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 7.666040897369385
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 7.332926273345947
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 6.362229347229004
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 5.685070991516113
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 7.823200225830078
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.93397855758667
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.794132709503174
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.21920919418335
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 7.527624607086182
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 8.047450065612793
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 6.52551794052124
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 4.773271083831787
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 7.087333679199219
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 15.861369132995605,
      "learning_rate": 1e-05,
      "loss": 6.9493,
      "step": 1496
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 7.071812152862549
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 7.896045207977295
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 5.830113410949707
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 5.0348124504089355
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.281698703765869
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.696913242340088
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.104727268218994
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.897377967834473
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 8.172799110412598
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 7.417323589324951
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.35821533203125
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.7873101234436035
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 7.561492919921875
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 6.3956618309021
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 7.135068416595459
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 6.371339797973633
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 9.242025375366211,
      "learning_rate": 1e-05,
      "loss": 7.0008,
      "step": 1500
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 6.9870405197143555
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 7.008581161499023
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 5.829616546630859
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 7.8617844581604
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 5.29081392288208
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 7.914795875549316
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 7.074616432189941
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 6.704603672027588
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 6.480629920959473
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.285736083984375
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.554870128631592
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.025040626525879
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.271465301513672
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.406000137329102
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.4595537185668945
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.405961513519287
    },
    {
      "epoch": 0.32607046070460705,
      "grad_norm": 18.385435104370117,
      "learning_rate": 1e-05,
      "loss": 7.0351,
      "step": 1504
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 6.858046531677246
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 6.147611141204834
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 5.9822468757629395
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 5.231870174407959
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.503199577331543
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.242766380310059
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 5.9149603843688965
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.793839931488037
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.993989944458008
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.539967060089111
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.328751087188721
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.197627067565918
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 7.157642841339111
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 6.266626834869385
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 8.21352481842041
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 5.5008015632629395
    },
    {
      "epoch": 0.3269376693766938,
      "grad_norm": 17.788345336914062,
      "learning_rate": 1e-05,
      "loss": 6.7421,
      "step": 1508
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 7.062148094177246
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.850600242614746
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.880724906921387
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.792598247528076
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.780336380004883
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.148305892944336
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 5.5271711349487305
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 6.399451732635498
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.282221794128418
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.109012603759766
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.672455787658691
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.301733016967773
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.009026050567627
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.372262001037598
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 5.762768268585205
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.1008195877075195
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 11.720280647277832,
      "learning_rate": 1e-05,
      "loss": 7.1907,
      "step": 1512
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 6.699037075042725
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.970765590667725
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.061394691467285
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.119976043701172
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.770598411560059
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.281170845031738
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.9546003341674805
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 6.812594413757324
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.05815315246582
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.810143947601318
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.385339736938477
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.910335063934326
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 4.030887126922607
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.9736456871032715
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.5106706619262695
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 5.8004961013793945
    },
    {
      "epoch": 0.32867208672086723,
      "grad_norm": 11.75008487701416,
      "learning_rate": 1e-05,
      "loss": 6.6344,
      "step": 1516
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 5.847898960113525
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 6.596672534942627
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 7.069430351257324
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 7.311767101287842
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.209158420562744
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.241708755493164
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 6.202678680419922
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 6.324984550476074
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 6.4890313148498535
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.615678310394287
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.173210144042969
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.010451793670654
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.767827987670898
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 6.500355243682861
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.51600456237793
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.432053089141846
    },
    {
      "epoch": 0.32953929539295396,
      "grad_norm": 10.331283569335938,
      "learning_rate": 1e-05,
      "loss": 6.9568,
      "step": 1520
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 6.078956127166748
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.132768630981445
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.675037384033203
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.107254981994629
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.258859634399414
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 7.496452331542969
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 8.5132474899292
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.895040512084961
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 4.6646342277526855
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 7.30001974105835
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 7.714179039001465
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 6.715773582458496
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 6.371140003204346
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 6.728188514709473
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 7.476552486419678
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 5.8777995109558105
    },
    {
      "epoch": 0.33040650406504063,
      "grad_norm": 10.392309188842773,
      "learning_rate": 1e-05,
      "loss": 6.8754,
      "step": 1524
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.9671311378479
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 7.414606094360352
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.438645362854004
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.493244647979736
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.343078136444092
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.191317081451416
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 5.954232215881348
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.810855865478516
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 6.043963432312012
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 7.046950340270996
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 7.3906025886535645
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 6.786735534667969
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 9.355414390563965
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 7.250055313110352
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 7.848613262176514
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 6.576263904571533
    },
    {
      "epoch": 0.33127371273712736,
      "grad_norm": 13.070449829101562,
      "learning_rate": 1e-05,
      "loss": 7.1195,
      "step": 1528
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.76240873336792
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 7.7436323165893555
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 7.4753570556640625
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.6861772537231445
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.829360008239746
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 6.9325761795043945
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.954930782318115
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.40125036239624
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 6.129059791564941
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.280336380004883
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.927753448486328
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.320268154144287
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 7.363841533660889
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 5.218011379241943
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.860517501831055
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.838335037231445
    },
    {
      "epoch": 0.3321409214092141,
      "grad_norm": 9.68878173828125,
      "learning_rate": 1e-05,
      "loss": 7.1077,
      "step": 1532
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 8.250575065612793
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.8633809089660645
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.274353504180908
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 5.089649677276611
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 6.992873191833496
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 5.908535480499268
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 7.387166500091553
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 6.716615676879883
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.7142462730407715
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.9289469718933105
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.108893394470215
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 8.054829597473145
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 6.932031631469727
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 4.591732025146484
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 5.89019250869751
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 7.435198783874512
    },
    {
      "epoch": 0.3330081300813008,
      "grad_norm": 12.044297218322754,
      "learning_rate": 1e-05,
      "loss": 6.8212,
      "step": 1536
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.8325347900390625
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.242559909820557
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 5.244423866271973
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.540383815765381
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.3226094245910645
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.693676471710205
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.9794816970825195
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 7.2742533683776855
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.821539402008057
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.504179000854492
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.894129276275635
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.614343166351318
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.146452903747559
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.386776447296143
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 6.861644744873047
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.391119480133057
    },
    {
      "epoch": 0.33387533875338754,
      "grad_norm": 12.59222412109375,
      "learning_rate": 1e-05,
      "loss": 6.7344,
      "step": 1540
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.398303031921387
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 5.799543857574463
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.848348140716553
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.192043304443359
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 5.643686294555664
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 5.376526355743408
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 7.166069984436035
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 7.020016670227051
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 6.507603168487549
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.805343151092529
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 9.50865364074707
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.089278221130371
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.386650562286377
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 6.522400379180908
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.703763008117676
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 4.697107315063477
    },
    {
      "epoch": 0.33474254742547427,
      "grad_norm": 10.08260726928711,
      "learning_rate": 1e-05,
      "loss": 6.9166,
      "step": 1544
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 5.851428031921387
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 6.99343204498291
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 6.854610443115234
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 7.744635105133057
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.019931316375732
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 8.106963157653809
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.513197898864746
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.5076375007629395
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.798973560333252
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 7.572307109832764
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.490939617156982
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 4.500717639923096
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.069121360778809
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.903626441955566
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 7.16799259185791
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.62802267074585
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 12.796228408813477,
      "learning_rate": 1e-05,
      "loss": 6.8577,
      "step": 1548
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.72029447555542
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.408884525299072
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.488094329833984
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 8.130730628967285
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.633065700531006
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 7.972618579864502
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 7.070233345031738
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.4056396484375
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 8.094992637634277
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 7.548053741455078
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 6.680094242095947
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 7.6496195793151855
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 7.0262651443481445
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 5.00564432144165
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 6.87119722366333
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 7.358814716339111
    },
    {
      "epoch": 0.3364769647696477,
      "grad_norm": 13.501379013061523,
      "learning_rate": 1e-05,
      "loss": 7.1915,
      "step": 1552
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.104362964630127
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 7.489148139953613
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.853282928466797
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.563688278198242
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.3281402587890625
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.469242095947266
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 5.026288032531738
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.45184850692749
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 7.425402641296387
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.7374267578125
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 5.835734844207764
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 4.527021408081055
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.1122636795043945
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.2383246421813965
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.6551289558410645
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 6.775114059448242
    },
    {
      "epoch": 0.3373441734417344,
      "grad_norm": 12.31808853149414,
      "learning_rate": 1e-05,
      "loss": 6.7245,
      "step": 1556
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.701052188873291
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.389093399047852
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.539053440093994
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 7.020531177520752
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 6.138461589813232
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 6.635265350341797
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 7.7618021965026855
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 5.263593673706055
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.0194573402404785
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 5.306835174560547
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.172009468078613
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 8.10030746459961
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.301654815673828
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.814505100250244
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 6.61742639541626
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 8.071049690246582
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 12.947366714477539,
      "learning_rate": 1e-05,
      "loss": 6.7408,
      "step": 1560
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.612431526184082
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 7.2637457847595215
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 7.1085076332092285
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.9591383934021
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.798898696899414
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.053314685821533
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.361542224884033
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 6.123429775238037
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.044321060180664
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.545816421508789
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 7.247438907623291
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 7.304589748382568
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.16622257232666
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.697808742523193
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 8.21203327178955
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.143420696258545
    },
    {
      "epoch": 0.33907859078590785,
      "grad_norm": 13.075034141540527,
      "learning_rate": 1e-05,
      "loss": 6.9152,
      "step": 1564
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.412783622741699
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 7.630626678466797
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.651858806610107
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 7.3929643630981445
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 8.311470031738281
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 6.354746341705322
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 5.63199520111084
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 5.585002899169922
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.487421035766602
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.605375289916992
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.794691562652588
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 7.170222282409668
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 5.710525989532471
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 5.8284149169921875
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 7.7029547691345215
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 6.093373775482178
    },
    {
      "epoch": 0.3399457994579946,
      "grad_norm": 10.254435539245605,
      "learning_rate": 1e-05,
      "loss": 6.6478,
      "step": 1568
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 4.610118389129639
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 6.554502487182617
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 7.037563800811768
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 6.858814716339111
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 5.282252311706543
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 6.8920722007751465
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 7.908417224884033
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 7.07145881652832
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 5.308371067047119
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.9295654296875
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.477464199066162
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 7.070078372955322
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.079158306121826
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.655366897583008
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.749014854431152
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 7.0263872146606445
    },
    {
      "epoch": 0.3408130081300813,
      "grad_norm": 10.380596160888672,
      "learning_rate": 1e-05,
      "loss": 6.5319,
      "step": 1572
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 8.197425842285156
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 7.594279766082764
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 7.081794261932373
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 6.936027526855469
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.78994607925415
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 8.01774787902832
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.670179843902588
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.506396770477295
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 7.705448150634766
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.891811370849609
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.872387886047363
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.741433143615723
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 7.06976318359375
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 7.059040069580078
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 4.955383777618408
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 6.56139612197876
    },
    {
      "epoch": 0.34168021680216804,
      "grad_norm": 9.031493186950684,
      "learning_rate": 1e-05,
      "loss": 6.9782,
      "step": 1576
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.774226665496826
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.293191909790039
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 7.434230327606201
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 5.874380111694336
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 8.362016677856445
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 7.577723503112793
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 9.221121788024902
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 7.639347553253174
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.460571765899658
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.282898902893066
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.098132133483887
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.683923721313477
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 7.946771621704102
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.706377029418945
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 7.130889892578125
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.769467830657959
    },
    {
      "epoch": 0.34254742547425476,
      "grad_norm": 12.698612213134766,
      "learning_rate": 1e-05,
      "loss": 7.2035,
      "step": 1580
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 7.129439353942871
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 7.097682476043701
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.86043119430542
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.893394947052002
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 8.221287727355957
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 8.8204927444458
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 7.209373474121094
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 7.988851070404053
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 5.507349491119385
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 8.085285186767578
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 8.149514198303223
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 6.638140678405762
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 6.0998334884643555
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 5.75364875793457
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 7.1528706550598145
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 3.833508014678955
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 12.17536735534668,
      "learning_rate": 1e-05,
      "loss": 6.9651,
      "step": 1584
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 6.993448257446289
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.0467071533203125
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.8126115798950195
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 6.853119850158691
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.062319278717041
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 7.264142990112305
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 7.4881205558776855
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.536728382110596
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.311653137207031
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.404109477996826
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 7.878824234008789
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.816279888153076
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 8.290351867675781
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 5.647396564483643
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 7.8939714431762695
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 6.794314384460449
    },
    {
      "epoch": 0.34428184281842816,
      "grad_norm": 8.81733512878418,
      "learning_rate": 1e-05,
      "loss": 7.0059,
      "step": 1588
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 7.613370895385742
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 6.314619064331055
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 7.996423721313477
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 5.794291973114014
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 6.913082599639893
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 8.110983848571777
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 5.436304092407227
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 5.136867046356201
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.487758636474609
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 5.761600971221924
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.882430076599121
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 7.352480411529541
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 7.196094989776611
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 5.742993354797363
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.06826114654541
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.634592056274414
    },
    {
      "epoch": 0.3451490514905149,
      "grad_norm": 8.9036865234375,
      "learning_rate": 1e-05,
      "loss": 6.5901,
      "step": 1592
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.34094762802124
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.263111114501953
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 6.544160842895508
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 4.843978404998779
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.418364524841309
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 6.770902156829834
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.350747108459473
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.3594512939453125
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.075109958648682
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 7.966270923614502
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 7.481382369995117
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.650601387023926
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 6.341209888458252
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 4.554572105407715
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 7.636709690093994
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 6.9654221534729
    },
    {
      "epoch": 0.3460162601626016,
      "grad_norm": 11.944253921508789,
      "learning_rate": 1e-05,
      "loss": 6.7852,
      "step": 1596
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.499706268310547
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 5.19542932510376
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 8.227527618408203
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.394549369812012
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 7.531883716583252
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.767837047576904
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.702320575714111
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 7.746737480163574
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.0282135009765625
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 4.102729797363281
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.401953220367432
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.156118869781494
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.717785358428955
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 5.777705669403076
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.043062686920166
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 7.474100589752197
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 13.530964851379395,
      "learning_rate": 1e-05,
      "loss": 6.798,
      "step": 1600
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 6.9119873046875
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 6.736239433288574
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 5.269401550292969
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 7.128897190093994
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.029704570770264
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.243692398071289
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.906351089477539
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 5.828474521636963
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 6.633449077606201
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 7.098293304443359
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 5.785239219665527
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 8.005455017089844
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.607629776000977
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.02903938293457
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 6.93367338180542
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.358798503875732
    },
    {
      "epoch": 0.3477506775067751,
      "grad_norm": 16.171613693237305,
      "learning_rate": 1e-05,
      "loss": 6.7191,
      "step": 1604
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.532411575317383
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.609661102294922
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 6.7671074867248535
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.63854455947876
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.057258129119873
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 5.449775218963623
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 5.281497001647949
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.420844554901123
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 7.128045558929443
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 5.868773937225342
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 6.903678894042969
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 7.197463512420654
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 4.6735920906066895
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 7.2936110496521
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 6.832991600036621
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 7.010087013244629
    },
    {
      "epoch": 0.3486178861788618,
      "grad_norm": 9.512726783752441,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 1608
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 7.335170269012451
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.929201126098633
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.1051154136657715
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.739389896392822
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 4.331240653991699
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 5.600114822387695
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 6.337505340576172
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 6.787952899932861
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.362746715545654
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.412031650543213
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 7.123015403747559
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.0087504386901855
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.055446147918701
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.996683120727539
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 6.548735618591309
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.080217361450195
    },
    {
      "epoch": 0.34948509485094853,
      "grad_norm": 13.095880508422852,
      "learning_rate": 1e-05,
      "loss": 6.5471,
      "step": 1612
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 4.842983722686768
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 8.068650245666504
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 7.146355152130127
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 6.971560478210449
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.439585208892822
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 4.611536502838135
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 7.543732643127441
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.968500137329102
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.3407301902771
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 6.693871021270752
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.525429725646973
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.0880279541015625
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 7.441201686859131
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.301263809204102
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.671921253204346
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.9919538497924805
    },
    {
      "epoch": 0.35035230352303526,
      "grad_norm": 12.578733444213867,
      "learning_rate": 1e-05,
      "loss": 6.7905,
      "step": 1616
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.268984317779541
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.391542434692383
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 6.698020935058594
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 5.779595851898193
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.253486633300781
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 7.6002607345581055
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.269921779632568
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 7.523846626281738
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 5.42051887512207
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.881711959838867
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 7.1694746017456055
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.344405651092529
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.747634410858154
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.9038848876953125
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.870635509490967
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.426238536834717
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 17.87577247619629,
      "learning_rate": 1e-05,
      "loss": 6.7219,
      "step": 1620
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.248898506164551
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.631362438201904
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.502667427062988
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 4.632380962371826
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 7.547854900360107
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.281871318817139
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 7.384843349456787
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.8378705978393555
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.186596870422363
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.537238121032715
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 6.94361686706543
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.4248857498168945
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.282595634460449
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.450291156768799
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.845824241638184
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 7.138287544250488
    },
    {
      "epoch": 0.35208672086720866,
      "grad_norm": 15.487622261047363,
      "learning_rate": 1e-05,
      "loss": 6.9298,
      "step": 1624
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 6.491145133972168
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 8.138792991638184
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 7.0929036140441895
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 6.250836372375488
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 7.1605634689331055
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 5.519092082977295
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 6.260827541351318
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 8.101604461669922
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 4.602090358734131
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 7.5394158363342285
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 4.895697116851807
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 7.775718688964844
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 7.580375671386719
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 5.8121137619018555
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 7.494195938110352
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 8.090353965759277
    },
    {
      "epoch": 0.3529539295392954,
      "grad_norm": 13.086752891540527,
      "learning_rate": 1e-05,
      "loss": 6.8004,
      "step": 1628
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 7.035274028778076
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 7.388011932373047
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.629570484161377
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.784739017486572
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.432830810546875
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 7.864521026611328
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 7.372235298156738
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.416447639465332
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 7.228590488433838
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 7.656662464141846
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 6.448760032653809
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 5.6397318840026855
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.292810916900635
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.2669677734375
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 4.271528720855713
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.430130481719971
    },
    {
      "epoch": 0.3538211382113821,
      "grad_norm": 11.75705623626709,
      "learning_rate": 1e-05,
      "loss": 6.8224,
      "step": 1632
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.547144889831543
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 7.017606258392334
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 7.144464492797852
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.67789888381958
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.026703357696533
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.343909740447998
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.966641426086426
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 6.86928129196167
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.571976184844971
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.141619682312012
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 6.123843669891357
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.867520332336426
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.1538543701171875
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 8.19681167602539
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 6.10245418548584
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.75191593170166
    },
    {
      "epoch": 0.35468834688346884,
      "grad_norm": 14.453248977661133,
      "learning_rate": 1e-05,
      "loss": 7.1565,
      "step": 1636
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 5.722966194152832
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.962075233459473
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 8.005974769592285
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.193337440490723
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 5.213945388793945
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 7.278277397155762
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 6.423695087432861
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 6.756588935852051
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 6.581984043121338
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 7.6014084815979
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 5.850255966186523
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 7.2173004150390625
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 5.480970859527588
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 6.006164073944092
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 7.573928356170654
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 7.344253063201904
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 11.045234680175781,
      "learning_rate": 1e-05,
      "loss": 6.6383,
      "step": 1640
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.222402095794678
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.513901233673096
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 6.329756259918213
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.079931735992432
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 7.507558822631836
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.7917704582214355
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.300641059875488
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 5.931935787200928
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.298342227935791
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 5.78041934967041
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.451823711395264
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 6.83962869644165
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.786454677581787
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.9477643966674805
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.290524959564209
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.1234540939331055
    },
    {
      "epoch": 0.3564227642276423,
      "grad_norm": 12.099395751953125,
      "learning_rate": 1e-05,
      "loss": 7.0123,
      "step": 1644
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 8.634918212890625
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.078726768493652
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.770812511444092
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 6.364267349243164
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 6.038773536682129
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 7.167923927307129
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 6.789864540100098
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 7.056997776031494
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 7.289417266845703
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 5.742889881134033
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.813155651092529
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.993440628051758
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.620208740234375
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 7.8773274421691895
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 6.8481059074401855
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.884833812713623
    },
    {
      "epoch": 0.357289972899729,
      "grad_norm": 19.162572860717773,
      "learning_rate": 1e-05,
      "loss": 6.8732,
      "step": 1648
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 8.394376754760742
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.077556133270264
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.646731376647949
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.986873149871826
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 8.119180679321289
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 7.600971221923828
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 8.020804405212402
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 7.9661478996276855
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 6.9196062088012695
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 7.59690523147583
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 6.639510154724121
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 4.5969462394714355
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.346288204193115
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 6.954309463500977
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.221027374267578
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.677178859710693
    },
    {
      "epoch": 0.3581571815718157,
      "grad_norm": 11.13240909576416,
      "learning_rate": 1e-05,
      "loss": 7.1728,
      "step": 1652
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.175774097442627
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 6.139369010925293
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.230066776275635
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.309122562408447
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 7.990145683288574
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 6.96368932723999
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 7.15973424911499
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 6.784533977508545
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.5659027099609375
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 7.981420040130615
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 7.315033912658691
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.883950233459473
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 4.630131244659424
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 7.492422103881836
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 4.306910037994385
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 7.1662983894348145
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 11.326271057128906,
      "learning_rate": 1e-05,
      "loss": 6.8184,
      "step": 1656
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 8.891188621520996
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 6.691408634185791
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 7.112861633300781
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 7.149021625518799
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 5.639978885650635
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 6.218925952911377
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.559835433959961
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.812148094177246
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 6.570540428161621
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.25718879699707
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.025303840637207
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.104306221008301
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.15133810043335
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.43947696685791
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 6.746843338012695
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.8758440017700195
    },
    {
      "epoch": 0.35989159891598915,
      "grad_norm": 17.59665870666504,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 1660
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 5.940486431121826
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 7.498424053192139
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 7.15302038192749
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 8.146894454956055
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.749403476715088
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.5777106285095215
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.142855167388916
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.335160732269287
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.061380386352539
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 6.468817710876465
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.43763542175293
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.822184085845947
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 6.984842777252197
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.837283134460449
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.584733009338379
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.959263801574707
    },
    {
      "epoch": 0.3607588075880759,
      "grad_norm": 13.841094970703125,
      "learning_rate": 1e-05,
      "loss": 7.1063,
      "step": 1664
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.0689005851745605
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.562760353088379
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.28458833694458
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.129278182983398
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 6.687686443328857
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.302489757537842
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.127472877502441
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.249658584594727
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.987952709197998
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 7.6695556640625
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.395518779754639
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 7.275481700897217
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 5.9360246658325195
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.6552839279174805
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.443770408630371
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.88043737411499
    },
    {
      "epoch": 0.3616260162601626,
      "grad_norm": 14.938139915466309,
      "learning_rate": 1e-05,
      "loss": 6.9786,
      "step": 1668
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.14177131652832
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 4.512699604034424
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.601744174957275
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.55983829498291
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 7.8302321434021
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 7.6451497077941895
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.1439008712768555
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.526185989379883
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 5.2877116203308105
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 4.654654502868652
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.419631004333496
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.440070152282715
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 5.846433162689209
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.788661003112793
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.497644424438477
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 7.351506233215332
    },
    {
      "epoch": 0.36249322493224934,
      "grad_norm": 14.49915885925293,
      "learning_rate": 1e-05,
      "loss": 6.453,
      "step": 1672
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 8.317204475402832
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 7.511528968811035
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 6.9377288818359375
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 6.518052577972412
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 7.148252964019775
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 6.573976516723633
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 5.07976770401001
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 7.899874210357666
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.115024566650391
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.924779891967773
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.100345611572266
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.68431282043457
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.6276021003723145
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.781975746154785
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.838397979736328
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.928897857666016
    },
    {
      "epoch": 0.36336043360433606,
      "grad_norm": 14.006051063537598,
      "learning_rate": 1e-05,
      "loss": 6.8117,
      "step": 1676
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.027317523956299
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.256129264831543
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.039803504943848
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 6.903931617736816
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 7.4183149337768555
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 5.382026195526123
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 5.5647358894348145
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 4.679791450500488
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 7.907369613647461
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.923351287841797
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.99993371963501
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.4318928718566895
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 7.665497303009033
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 8.006265640258789
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 8.021444320678711
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 7.540765762329102
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 11.739519119262695,
      "learning_rate": 1e-05,
      "loss": 6.7355,
      "step": 1680
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 5.89285945892334
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 7.71382474899292
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.816633224487305
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.009468078613281
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.325674057006836
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.932051181793213
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 7.128702163696289
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 7.036040782928467
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 6.925948143005371
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.4384918212890625
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.715723514556885
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 9.318169593811035
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 5.673376560211182
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.8608598709106445
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 7.029080867767334
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.8649187088012695
    },
    {
      "epoch": 0.36509485094850946,
      "grad_norm": 13.151961326599121,
      "learning_rate": 1e-05,
      "loss": 6.9801,
      "step": 1684
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.157257080078125
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 8.566368103027344
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.524257183074951
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 8.08071517944336
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 7.033411502838135
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 6.791049957275391
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 7.074739933013916
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 5.667596817016602
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 7.234703063964844
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 7.219968318939209
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 3.918245792388916
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 6.888444423675537
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 5.995876312255859
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.779206275939941
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 5.1513237953186035
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.223159313201904
    },
    {
      "epoch": 0.3659620596205962,
      "grad_norm": 8.749671936035156,
      "learning_rate": 1e-05,
      "loss": 6.7066,
      "step": 1688
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 8.856369972229004
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 6.505768299102783
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 7.277553081512451
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 7.324300765991211
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 7.546770095825195
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 7.09352970123291
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 8.01525592803955
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 6.872211456298828
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.442684173583984
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.18049955368042
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.748684883117676
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.794894695281982
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 7.881214141845703
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.8969316482543945
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.557491302490234
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 7.487244606018066
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 8.011662483215332,
      "learning_rate": 1e-05,
      "loss": 7.1551,
      "step": 1692
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 6.588006496429443
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 6.732091903686523
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 7.349502086639404
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 9.306407928466797
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.503824710845947
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.0495429039001465
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.346329689025879
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.9621100425720215
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 8.094799995422363
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 5.726823806762695
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 7.4876627922058105
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 7.250857830047607
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.597093105316162
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.966362953186035
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.746964454650879
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 7.432823657989502
    },
    {
      "epoch": 0.36769647696476965,
      "grad_norm": 10.307558059692383,
      "learning_rate": 1e-05,
      "loss": 7.1338,
      "step": 1696
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.5038299560546875
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 8.636213302612305
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 7.070670127868652
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.8507890701293945
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 7.37852144241333
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.4799580574035645
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.959658145904541
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 4.461940288543701
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 5.715892791748047
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 6.834873676300049
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 5.191016674041748
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 7.281731128692627
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.169281005859375
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 7.587100982666016
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.744732856750488
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.832852363586426
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 12.867781639099121,
      "learning_rate": 1e-05,
      "loss": 6.6687,
      "step": 1700
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.531707763671875
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.8858323097229
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 5.185482025146484
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.865591526031494
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.259845733642578
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 6.449408531188965
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.5872344970703125
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.218433856964111
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 9.245667457580566
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 6.874835014343262
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 8.685964584350586
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 7.121236324310303
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.378653049468994
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.127613067626953
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 6.230998992919922
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.13712215423584
    },
    {
      "epoch": 0.3694308943089431,
      "grad_norm": 11.50800609588623,
      "learning_rate": 1e-05,
      "loss": 7.1116,
      "step": 1704
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 5.903327941894531
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.428694725036621
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.799359321594238
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 7.20259952545166
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.261512279510498
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 6.09173059463501
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.01450252532959
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.6902666091918945
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 6.994780540466309
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.652253150939941
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.710801124572754
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.156329154968262
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 7.452376842498779
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 6.810173511505127
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 4.978171348571777
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 7.969327449798584
    },
    {
      "epoch": 0.37029810298102983,
      "grad_norm": 13.130169868469238,
      "learning_rate": 1e-05,
      "loss": 6.9448,
      "step": 1708
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.230527877807617
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.0443806648254395
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.306171417236328
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 5.743917465209961
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.201673984527588
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 7.694666862487793
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.663734436035156
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 5.878359794616699
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 7.137277126312256
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 7.021060466766357
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.786210536956787
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 5.751482963562012
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 7.497482776641846
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.6252665519714355
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.6860246658325195
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.861536502838135
    },
    {
      "epoch": 0.37116531165311656,
      "grad_norm": 30.83160400390625,
      "learning_rate": 1e-05,
      "loss": 6.7581,
      "step": 1712
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.0784454345703125
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 7.03397274017334
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 7.045401573181152
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.993930339813232
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 5.765843391418457
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 6.54266357421875
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 7.390023231506348
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 7.791756629943848
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.1250691413879395
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.620538234710693
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 6.48237943649292
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.7695631980896
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 6.921669006347656
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.8871870040893555
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.199451446533203
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.728370666503906
    },
    {
      "epoch": 0.37203252032520323,
      "grad_norm": 18.980958938598633,
      "learning_rate": 1e-05,
      "loss": 7.086,
      "step": 1716
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 7.409213542938232
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 8.447017669677734
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 4.826411724090576
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 5.995752334594727
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.494748115539551
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.784407615661621
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 7.1160359382629395
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.276813983917236
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 7.82333517074585
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 7.617834568023682
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.923198223114014
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.599327087402344
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.4899821281433105
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 7.964627265930176
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.857070446014404
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 5.7394537925720215
    },
    {
      "epoch": 0.37289972899728996,
      "grad_norm": 10.738057136535645,
      "learning_rate": 1e-05,
      "loss": 6.8353,
      "step": 1720
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 8.47487735748291
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 7.118866920471191
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 5.380270481109619
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 7.277040481567383
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 6.398674011230469
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 9.168145179748535
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 7.083852767944336
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 5.4466657638549805
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.245082855224609
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.567190170288086
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.432550430297852
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.65767765045166
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 8.891239166259766
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 6.689113140106201
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 7.853936195373535
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 7.461939334869385
    },
    {
      "epoch": 0.3737669376693767,
      "grad_norm": 20.250947952270508,
      "learning_rate": 1e-05,
      "loss": 6.9467,
      "step": 1724
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.925532817840576
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 4.236240863800049
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.791351795196533
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.691317558288574
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 7.389069557189941
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 5.629187107086182
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 6.685420513153076
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 8.544066429138184
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 6.203817844390869
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 9.076380729675293
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 8.063587188720703
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 7.295113563537598
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 9.71883773803711
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 6.988846778869629
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 6.228342056274414
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 5.842641353607178
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 20.20690155029297,
      "learning_rate": 1e-05,
      "loss": 7.0194,
      "step": 1728
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 7.396588325500488
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 4.955908298492432
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 6.851317882537842
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 5.459029197692871
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 7.495425224304199
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 7.071164608001709
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 5.212894439697266
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 6.320476055145264
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 7.938924312591553
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 7.6005988121032715
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 5.764976978302002
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 6.761519432067871
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 6.120822429656982
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.032553672790527
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 5.128564357757568
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.02775239944458
    },
    {
      "epoch": 0.37550135501355014,
      "grad_norm": 14.133788108825684,
      "learning_rate": 1e-05,
      "loss": 6.5087,
      "step": 1732
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 4.583703517913818
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 6.081716537475586
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 5.408565044403076
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 7.2709431648254395
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 5.697758197784424
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.498837947845459
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 5.823853969573975
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.879730224609375
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.914913177490234
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 7.645260810852051
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 7.082399368286133
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.838405132293701
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 5.909290790557861
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 7.015606880187988
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 7.289491176605225
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.892923355102539
    },
    {
      "epoch": 0.37636856368563687,
      "grad_norm": 10.175223350524902,
      "learning_rate": 1e-05,
      "loss": 6.6146,
      "step": 1736
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 6.8888750076293945
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 4.6204423904418945
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 6.216033935546875
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 7.054903507232666
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.818185329437256
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 5.811028480529785
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 6.911950588226318
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.307302474975586
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.138708114624023
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.2269368171691895
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.485801696777344
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 5.7592573165893555
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 7.8889875411987305
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 5.8979105949401855
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 6.945556640625
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 8.737360000610352
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 12.42385482788086,
      "learning_rate": 1e-05,
      "loss": 6.8568,
      "step": 1740
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 8.335494995117188
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 7.493769645690918
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 6.338626384735107
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 7.3475847244262695
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.042696952819824
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.067044258117676
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.672798156738281
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.403214454650879
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 8.011043548583984
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.2248454093933105
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 7.13377046585083
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.758477210998535
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 7.724386692047119
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.823332786560059
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.935319423675537
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.815923690795898
    },
    {
      "epoch": 0.3781029810298103,
      "grad_norm": 16.19879722595215,
      "learning_rate": 1e-05,
      "loss": 6.9455,
      "step": 1744
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.7710676193237305
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 7.72130012512207
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.907829761505127
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 7.989322185516357
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.965546131134033
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.932740688323975
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 7.500763416290283
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 7.771521091461182
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 8.103300094604492
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 8.415884971618652
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 5.694822788238525
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 5.554482460021973
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 6.951190948486328
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 6.283847332000732
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 5.114089012145996
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 7.377551078796387
    },
    {
      "epoch": 0.378970189701897,
      "grad_norm": 12.03246021270752,
      "learning_rate": 1e-05,
      "loss": 7.0035,
      "step": 1748
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 6.602658271789551
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 9.55819320678711
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 7.183732032775879
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 7.6955885887146
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.5928449630737305
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 8.012932777404785
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.7899394035339355
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 7.5177130699157715
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 6.105759620666504
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 6.436723709106445
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 8.856595039367676
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 7.466708183288574
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 7.971372127532959
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 5.152716636657715
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 7.785099506378174
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 4.552044868469238
    },
    {
      "epoch": 0.3798373983739837,
      "grad_norm": 12.0724458694458,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 1752
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.573974609375
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.946788311004639
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 7.351075172424316
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.7664899826049805
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 6.829586505889893
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 8.20547103881836
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 7.633475303649902
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 6.138539791107178
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 7.245657444000244
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.929986476898193
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 7.314255237579346
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.410684108734131
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.023324012756348
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.60191535949707
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 6.761828899383545
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.751462459564209
    },
    {
      "epoch": 0.38070460704607045,
      "grad_norm": 12.534482955932617,
      "learning_rate": 1e-05,
      "loss": 7.0928,
      "step": 1756
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.145917892456055
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 5.998371601104736
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.989431858062744
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.9270710945129395
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 7.830364227294922
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 5.4314284324646
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 6.539542198181152
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 6.499624729156494
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.559450149536133
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.231332778930664
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.739800930023193
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.1511030197143555
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 7.642301082611084
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.3157806396484375
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.448112964630127
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.239654064178467
    },
    {
      "epoch": 0.3815718157181572,
      "grad_norm": 15.675093650817871,
      "learning_rate": 1e-05,
      "loss": 6.7931,
      "step": 1760
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 6.62177038192749
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 6.588141441345215
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 7.193245887756348
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 7.248629570007324
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 9.107278823852539
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 8.830804824829102
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.539987564086914
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.340684413909912
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 6.339652061462402
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 5.128067493438721
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 8.175581932067871
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 7.30057954788208
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 8.215672492980957
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 4.427018165588379
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 7.434797286987305
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 7.475817680358887
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 18.552806854248047,
      "learning_rate": 1e-05,
      "loss": 7.0605,
      "step": 1764
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 6.897288799285889
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.231258869171143
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.927798748016357
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.259566783905029
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.394286155700684
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.078211784362793
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.496518611907959
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.188920021057129
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 5.646343231201172
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.739194393157959
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.171651363372803
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.099861145019531
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 7.913536071777344
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 5.36080265045166
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.705816268920898
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.440118312835693
    },
    {
      "epoch": 0.38330623306233064,
      "grad_norm": 13.6639986038208,
      "learning_rate": 1e-05,
      "loss": 7.0344,
      "step": 1768
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 4.6651835441589355
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.7520527839660645
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.5689005851745605
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 7.218936443328857
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 4.742941856384277
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 8.224800109863281
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 7.452648639678955
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 5.753363609313965
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.110918045043945
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.76862907409668
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.710855484008789
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 6.52162504196167
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 6.728224277496338
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 4.925248146057129
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 8.26810073852539
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 4.982512474060059
    },
    {
      "epoch": 0.38417344173441736,
      "grad_norm": 15.092267990112305,
      "learning_rate": 1e-05,
      "loss": 6.5872,
      "step": 1772
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 5.838324069976807
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.333170413970947
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.447743892669678
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.126394748687744
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.409499645233154
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.463222026824951
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.415781497955322
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.41391658782959
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 8.288098335266113
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 7.360701084136963
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 5.284470558166504
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 5.657751083374023
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 7.265291690826416
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.196761608123779
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.319493293762207
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.509529113769531
    },
    {
      "epoch": 0.3850406504065041,
      "grad_norm": 12.000161170959473,
      "learning_rate": 1e-05,
      "loss": 6.6456,
      "step": 1776
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.015764236450195
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.987449645996094
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.262732982635498
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.196621417999268
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.9996490478515625
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 8.538321495056152
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.310145854949951
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.864439964294434
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.299463748931885
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 8.066932678222656
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 5.502048492431641
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.683802127838135
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 5.685250282287598
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 5.941386699676514
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 7.373898506164551
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 8.137752532958984
    },
    {
      "epoch": 0.38590785907859076,
      "grad_norm": 11.688458442687988,
      "learning_rate": 1e-05,
      "loss": 6.8666,
      "step": 1780
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 7.976484298706055
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 6.572079181671143
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 9.011366844177246
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 8.122790336608887
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 7.47820520401001
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.5671916007995605
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.57891845703125
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.627481460571289
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.8312668800354
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 7.671876907348633
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 7.659461975097656
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.4945549964904785
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 7.397139072418213
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.855741024017334
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.953240394592285
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 4.584287643432617
    },
    {
      "epoch": 0.3867750677506775,
      "grad_norm": 11.714080810546875,
      "learning_rate": 1e-05,
      "loss": 7.0864,
      "step": 1784
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 7.111969470977783
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 6.158313751220703
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 8.213050842285156
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 7.230630874633789
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 7.266404151916504
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.203005313873291
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 6.351623058319092
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 6.268675327301025
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 8.393134117126465
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.740728378295898
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.854189872741699
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 7.071226596832275
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 7.8813276290893555
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 8.3281888961792
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.0343756675720215
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.307174205780029
    },
    {
      "epoch": 0.3876422764227642,
      "grad_norm": 14.50039291381836,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 1788
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 7.976319313049316
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.645056247711182
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 7.511394500732422
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.5417985916137695
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.215287685394287
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 5.7668232917785645
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.287937641143799
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.101224422454834
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 5.846227169036865
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.35775899887085
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 7.474123954772949
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.220396041870117
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.388857364654541
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.5723652839660645
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 6.1657209396362305
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 6.761062145233154
    },
    {
      "epoch": 0.38850948509485095,
      "grad_norm": 12.748668670654297,
      "learning_rate": 1e-05,
      "loss": 6.8645,
      "step": 1792
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.272033214569092
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.4339599609375
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 4.9245991706848145
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.352509021759033
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 6.316900730133057
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 5.453239917755127
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 7.300760269165039
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 5.927396774291992
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.758857250213623
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 7.318098545074463
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.375629425048828
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.910512447357178
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 6.533486843109131
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 6.960281848907471
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.139842510223389
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.464033126831055
    },
    {
      "epoch": 0.3893766937669377,
      "grad_norm": 13.828165054321289,
      "learning_rate": 1e-05,
      "loss": 6.7151,
      "step": 1796
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 6.684892177581787
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 5.739161491394043
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.0712056159973145
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.457199573516846
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.514532566070557
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.857333660125732
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 7.476129531860352
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.354600429534912
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.936346530914307
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 5.738579750061035
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.630368232727051
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.724833965301514
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 6.581324100494385
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 7.498419284820557
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 5.45848274230957
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 7.225796222686768
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 12.12498950958252,
      "learning_rate": 1e-05,
      "loss": 6.6843,
      "step": 1800
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.729139804840088
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 5.80200719833374
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.160938739776611
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.987941741943359
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 7.82224178314209
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 6.845699787139893
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 5.907830715179443
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 6.2790398597717285
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.749516487121582
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 5.8204779624938965
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 6.808022499084473
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.222045421600342
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 5.502439975738525
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 4.578292369842529
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 6.527950286865234
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 6.489729404449463
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 11.94982624053955,
      "learning_rate": 1e-05,
      "loss": 6.6396,
      "step": 1804
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.790987968444824
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 7.022675037384033
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.575833797454834
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.598699569702148
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 4.583224296569824
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 7.92532205581665
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 6.0115814208984375
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 8.834277153015137
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 6.460102558135986
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.4692583084106445
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 6.232648849487305
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.362189769744873
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 6.48718786239624
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 8.168778419494629
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 7.380178928375244
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 8.485404968261719
    },
    {
      "epoch": 0.39197831978319786,
      "grad_norm": 11.802701950073242,
      "learning_rate": 1e-05,
      "loss": 6.7743,
      "step": 1808
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 6.934202194213867
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 6.858664035797119
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 8.014054298400879
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 5.790808200836182
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.110710144042969
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.479976177215576
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 5.903993129730225
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.71138858795166
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 7.130838394165039
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 5.652756690979004
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 6.402455806732178
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 7.345299243927002
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 5.420858383178711
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 8.52608585357666
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 7.941612720489502
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 6.912479400634766
    },
    {
      "epoch": 0.39284552845528453,
      "grad_norm": 13.284111976623535,
      "learning_rate": 1e-05,
      "loss": 6.946,
      "step": 1812
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 7.185894966125488
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 7.12203311920166
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 5.610137939453125
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 6.8565673828125
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 8.63602066040039
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 6.764125347137451
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 5.794399261474609
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 7.864560127258301
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 8.38438606262207
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 5.550642490386963
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.8981099128723145
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.4067277908325195
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 5.053300857543945
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 5.773242950439453
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 6.958186626434326
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 7.147613525390625
    },
    {
      "epoch": 0.39371273712737126,
      "grad_norm": 12.64732551574707,
      "learning_rate": 1e-05,
      "loss": 6.7504,
      "step": 1816
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.4965009689331055
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.825068473815918
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 6.1767754554748535
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.576098918914795
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.222804546356201
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.080185890197754
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 8.037071228027344
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.127996921539307
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 5.19455099105835
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 6.606066703796387
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 7.438028335571289
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 6.336576461791992
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.924776554107666
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.158502101898193
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.826911449432373
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 4.567472457885742
    },
    {
      "epoch": 0.394579945799458,
      "grad_norm": 11.792019844055176,
      "learning_rate": 1e-05,
      "loss": 6.4122,
      "step": 1820
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 7.956368446350098
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 8.184361457824707
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 7.5145416259765625
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 6.627597808837891
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 6.757842063903809
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 6.67906379699707
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 7.164046287536621
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 4.533711910247803
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 6.661974906921387
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.044585704803467
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.552823543548584
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.3324079513549805
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 8.255638122558594
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 7.3350043296813965
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 6.9027276039123535
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 6.271425724029541
    },
    {
      "epoch": 0.3954471544715447,
      "grad_norm": 10.723921775817871,
      "learning_rate": 1e-05,
      "loss": 7.0484,
      "step": 1824
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 4.590123176574707
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.510931491851807
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.372846603393555
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.702863693237305
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 7.108877182006836
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 8.013314247131348
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 7.461329936981201
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 6.355632781982422
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 5.421542167663574
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 7.7091827392578125
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 6.456087112426758
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 7.254866600036621
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.996821880340576
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 7.180839538574219
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.939302444458008
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 9.969782829284668
    },
    {
      "epoch": 0.39631436314363144,
      "grad_norm": 15.115302085876465,
      "learning_rate": 1e-05,
      "loss": 7.1278,
      "step": 1828
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.237140655517578
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 6.605502128601074
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 5.959279537200928
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.661234378814697
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.672307968139648
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.581669330596924
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.970946788787842
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 8.141255378723145
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 6.3864617347717285
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 7.101136684417725
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 6.99885892868042
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 7.488223075866699
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 10.41711139678955
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 6.75139045715332
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.723464012145996
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.378602504730225
    },
    {
      "epoch": 0.39718157181571817,
      "grad_norm": 18.38558578491211,
      "learning_rate": 1e-05,
      "loss": 7.2547,
      "step": 1832
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.640625476837158
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.065558910369873
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.850683689117432
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 6.737874984741211
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.505273342132568
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 4.914711952209473
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.023563385009766
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 7.170594692230225
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 5.847465515136719
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.268424034118652
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.996109962463379
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.152270317077637
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 5.5727972984313965
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 7.93475341796875
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.5782976150512695
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.864645957946777
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 18.051366806030273,
      "learning_rate": 1e-05,
      "loss": 6.6327,
      "step": 1836
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 8.365279197692871
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.800463676452637
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.46727991104126
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.490615367889404
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 6.1424336433410645
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.517714500427246
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 4.674238204956055
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.077371597290039
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 6.610633850097656
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 5.685612678527832
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 6.372512340545654
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 7.052853107452393
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.129977703094482
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 6.839815616607666
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.09061336517334
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.087224006652832
    },
    {
      "epoch": 0.3989159891598916,
      "grad_norm": 11.506441116333008,
      "learning_rate": 1e-05,
      "loss": 6.7128,
      "step": 1840
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.550500392913818
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.226983547210693
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 6.876450538635254
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.145572185516357
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.253406047821045
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.942151069641113
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 6.523910999298096
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 7.143002986907959
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 7.204346179962158
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 4.099836349487305
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 6.578270435333252
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 7.070333480834961
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 7.033196926116943
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 4.5437164306640625
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.935102939605713
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.723697185516357
    },
    {
      "epoch": 0.3997831978319783,
      "grad_norm": 11.042301177978516,
      "learning_rate": 1e-05,
      "loss": 6.4907,
      "step": 1844
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 7.165626049041748
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 6.035970211029053
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 6.42431640625
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 8.1741943359375
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 6.911184310913086
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 6.595042705535889
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 7.025390148162842
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 7.393616199493408
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.1543450355529785
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.194869041442871
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 8.316924095153809
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 7.058084964752197
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 8.285669326782227
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 6.717796802520752
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 7.5489912033081055
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 7.303476333618164
    },
    {
      "epoch": 0.400650406504065,
      "grad_norm": 11.109623908996582,
      "learning_rate": 1e-05,
      "loss": 7.0816,
      "step": 1848
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.893130779266357
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.010936260223389
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.923093795776367
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 7.647600173950195
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.308612823486328
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 6.429826259613037
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 6.371376037597656
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.301163673400879
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.977085590362549
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 8.22973346710205
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.007702827453613
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.800291538238525
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 5.537547588348389
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 8.26563549041748
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 7.36803674697876
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 8.525911331176758
    },
    {
      "epoch": 0.40151761517615175,
      "grad_norm": 16.693214416503906,
      "learning_rate": 1e-05,
      "loss": 7.0374,
      "step": 1852
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 5.719332695007324
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.488830089569092
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.7235426902771
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 7.136974811553955
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.248893737792969
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.060917854309082
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.51327657699585
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 7.96453332901001
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 7.333781719207764
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 5.533329963684082
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 6.328481197357178
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 5.761972427368164
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.813888072967529
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 8.245430946350098
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 7.029170036315918
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.464137554168701
    },
    {
      "epoch": 0.4023848238482385,
      "grad_norm": 11.961882591247559,
      "learning_rate": 1e-05,
      "loss": 6.6479,
      "step": 1856
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.189012050628662
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 5.820988655090332
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.997926235198975
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.629144668579102
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 5.956190586090088
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.071488380432129
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.158321857452393
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 7.008922576904297
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 7.972340106964111
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 5.066133975982666
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 5.830930709838867
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 7.319993019104004
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 5.784542560577393
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.3943963050842285
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.41094970703125
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 8.207623481750488
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 12.640978813171387,
      "learning_rate": 1e-05,
      "loss": 6.4887,
      "step": 1860
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 6.982181549072266
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 8.009668350219727
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 5.8114447593688965
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 5.490384578704834
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.747119426727295
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.426513195037842
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.782283306121826
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.98311710357666
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 7.813360691070557
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 7.366974830627441
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.2428131103515625
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.895580291748047
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.624913692474365
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 7.30287504196167
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.859025001525879
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.830589294433594
    },
    {
      "epoch": 0.40411924119241194,
      "grad_norm": 12.75561237335205,
      "learning_rate": 1e-05,
      "loss": 6.8231,
      "step": 1864
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 6.19089412689209
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.30990743637085
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.393222332000732
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.623100757598877
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 6.828075885772705
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.946479320526123
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 4.728020668029785
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.688373565673828
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 5.266350269317627
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.405632495880127
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 6.200445652008057
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.005291938781738
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.319213390350342
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.807861328125
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 7.005979061126709
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 7.42253303527832
    },
    {
      "epoch": 0.40498644986449867,
      "grad_norm": 10.527029991149902,
      "learning_rate": 1e-05,
      "loss": 6.8213,
      "step": 1868
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.297486782073975
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.818125247955322
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.02869176864624
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.753157138824463
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 7.932427883148193
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 4.807745933532715
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 6.337674140930176
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 6.303576469421387
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.1000447273254395
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.04230260848999
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.383580684661865
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 8.577484130859375
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.535900592803955
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.714801788330078
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.746295928955078
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 7.28424072265625
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 10.588761329650879,
      "learning_rate": 1e-05,
      "loss": 6.7915,
      "step": 1872
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 5.052041053771973
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 6.7975053787231445
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 8.095544815063477
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 7.827065467834473
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.503422260284424
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.768783092498779
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 4.6531291007995605
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 6.810931205749512
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 6.688874244689941
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 6.94431209564209
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.779850959777832
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.747437000274658
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 5.9736647605896
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.549195289611816
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.829215049743652
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 7.014966011047363
    },
    {
      "epoch": 0.40672086720867207,
      "grad_norm": 13.86168098449707,
      "learning_rate": 1e-05,
      "loss": 6.8772,
      "step": 1876
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.500443935394287
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.329555988311768
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 6.283778667449951
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 5.295026779174805
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.256815433502197
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.584316730499268
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.102174282073975
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 6.085043907165527
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 4.777405738830566
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.250609874725342
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 6.975409984588623
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.075806140899658
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 8.218644142150879
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 6.866625785827637
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 6.410890579223633
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 7.301613807678223
    },
    {
      "epoch": 0.4075880758807588,
      "grad_norm": 12.762906074523926,
      "learning_rate": 1e-05,
      "loss": 6.8321,
      "step": 1880
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 6.353157043457031
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.417308807373047
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 6.474914073944092
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.285009860992432
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.104901313781738
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.058778285980225
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.14928674697876
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 6.646563529968262
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 5.495670795440674
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.833701133728027
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.344050884246826
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 7.233257293701172
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.738831520080566
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.391574382781982
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 6.449222087860107
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.641183853149414
    },
    {
      "epoch": 0.4084552845528455,
      "grad_norm": 18.488468170166016,
      "learning_rate": 1e-05,
      "loss": 6.9136,
      "step": 1884
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 7.845779895782471
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 7.720578670501709
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.838652610778809
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.251973628997803
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 5.5964179039001465
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.382522106170654
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.339742183685303
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.457535266876221
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.303336143493652
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.992598533630371
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 7.0442681312561035
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 8.125143051147461
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 7.815236568450928
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 7.896109104156494
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 6.741309642791748
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 8.633638381958008
    },
    {
      "epoch": 0.40932249322493225,
      "grad_norm": 13.929642677307129,
      "learning_rate": 1e-05,
      "loss": 7.2491,
      "step": 1888
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 7.418387413024902
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 6.003640174865723
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 5.897780418395996
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 8.545174598693848
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 8.546612739562988
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 6.9504241943359375
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 6.89384126663208
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 5.641729831695557
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.996703624725342
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.687174320220947
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.430656433105469
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.940340518951416
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.971424102783203
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.718386173248291
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 5.6631245613098145
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 7.376100063323975
    },
    {
      "epoch": 0.410189701897019,
      "grad_norm": 11.093749046325684,
      "learning_rate": 1e-05,
      "loss": 7.1051,
      "step": 1892
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.893421173095703
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.197078704833984
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.295474529266357
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 6.739606857299805
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 5.868967533111572
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 7.32244348526001
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 4.48317813873291
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 6.545616626739502
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.727317810058594
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.61076545715332
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 6.215098857879639
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.444573879241943
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.0021843910217285
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 7.451939582824707
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.4332170486450195
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.838639259338379
    },
    {
      "epoch": 0.4110569105691057,
      "grad_norm": 13.854036331176758,
      "learning_rate": 1e-05,
      "loss": 6.8168,
      "step": 1896
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 6.013025760650635
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 7.325876712799072
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 6.640594005584717
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 5.888084888458252
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 8.735339164733887
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.662420272827148
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 6.850462436676025
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.181193828582764
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 3.688311815261841
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.0135064125061035
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 8.031797409057617
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.285133361816406
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.989424228668213
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.258533477783203
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.7671098709106445
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 5.597353935241699
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 11.021623611450195,
      "learning_rate": 1e-05,
      "loss": 6.933,
      "step": 1900
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 7.121721267700195
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.383524417877197
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.434305667877197
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.960063934326172
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.615703105926514
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 7.8913702964782715
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 8.506864547729492
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.381888389587402
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 6.889255046844482
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 7.8379621505737305
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 7.407572269439697
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 5.817511081695557
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 7.4382710456848145
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 6.669683933258057
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 8.769148826599121
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 5.655536651611328
    },
    {
      "epoch": 0.41279132791327916,
      "grad_norm": 11.375889778137207,
      "learning_rate": 1e-05,
      "loss": 7.0488,
      "step": 1904
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 8.88162612915039
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 7.511412620544434
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 7.900771617889404
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 8.277113914489746
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 5.520416259765625
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 6.221579074859619
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 7.139410495758057
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 7.423389911651611
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.270284175872803
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.472412109375
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.826138496398926
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 6.1350250244140625
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.319952964782715
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.837907314300537
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.1874566078186035
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.805717468261719
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 13.564123153686523,
      "learning_rate": 1e-05,
      "loss": 7.1082,
      "step": 1908
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 8.735082626342773
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 7.322756767272949
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 6.731095314025879
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 6.985803127288818
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 6.697235107421875
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 5.441770076751709
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 7.8661322593688965
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 7.5420756340026855
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 7.726109027862549
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.84493350982666
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.883676528930664
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.091180324554443
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 6.519358158111572
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 6.739306926727295
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 7.859296798706055
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 7.813914775848389
    },
    {
      "epoch": 0.41452574525745256,
      "grad_norm": 16.095975875854492,
      "learning_rate": 1e-05,
      "loss": 7.1125,
      "step": 1912
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 8.065293312072754
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 6.563775539398193
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 7.439987659454346
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 5.613483905792236
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 6.86297607421875
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.875993728637695
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.460069179534912
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 6.443350791931152
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 5.9850687980651855
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 7.7412590980529785
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 6.945632457733154
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 6.994552135467529
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.17425012588501
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.968750476837158
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 8.462943077087402
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 7.948525428771973
    },
    {
      "epoch": 0.4153929539295393,
      "grad_norm": 10.507119178771973,
      "learning_rate": 1e-05,
      "loss": 7.0966,
      "step": 1916
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.413222312927246
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 4.715931415557861
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.629273891448975
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 6.479766845703125
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.805432319641113
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.097076892852783
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.917294025421143
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.919555187225342
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 7.312130928039551
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 5.845226287841797
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 7.298004150390625
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 6.82334566116333
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.255473613739014
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.163650989532471
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 7.125606536865234
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 7.612839698791504
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 16.654434204101562,
      "learning_rate": 1e-05,
      "loss": 6.5259,
      "step": 1920
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 5.9556145668029785
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 6.470804214477539
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 7.27482271194458
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 6.32291316986084
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 6.321402549743652
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 4.991641998291016
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.973714828491211
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.014674186706543
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.398632049560547
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.174102783203125
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.3336181640625
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 6.5655646324157715
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 6.168823719024658
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 5.889039993286133
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 7.5314531326293945
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 6.874566555023193
    },
    {
      "epoch": 0.41712737127371274,
      "grad_norm": 8.861069679260254,
      "learning_rate": 1e-05,
      "loss": 6.7038,
      "step": 1924
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.946407318115234
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 7.376439094543457
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 7.543226718902588
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.751337051391602
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 7.005956172943115
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.83931303024292
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.895480155944824
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 7.595380783081055
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.995596408843994
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.204065799713135
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 5.238842964172363
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.124108791351318
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 8.171958923339844
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 7.071220397949219
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 4.9354095458984375
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 7.535356521606445
    },
    {
      "epoch": 0.41799457994579947,
      "grad_norm": 16.097341537475586,
      "learning_rate": 1e-05,
      "loss": 7.0144,
      "step": 1928
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 4.874967575073242
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.6744160652160645
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.308837890625
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.987208366394043
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.639956951141357
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 6.864324569702148
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.360225200653076
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.0563836097717285
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 8.198180198669434
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.972628593444824
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 4.953065395355225
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.735562801361084
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 7.097900867462158
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 6.8009867668151855
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 5.873628616333008
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 6.565676212310791
    },
    {
      "epoch": 0.4188617886178862,
      "grad_norm": 11.986502647399902,
      "learning_rate": 1e-05,
      "loss": 6.8727,
      "step": 1932
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 5.998910427093506
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.419452667236328
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 8.14021110534668
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.406551361083984
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 6.400725364685059
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 5.920090198516846
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 7.444692611694336
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 8.126631736755371
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.241993427276611
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 6.441658020019531
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.149231433868408
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.095348834991455
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 9.788049697875977
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.61066198348999
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.145210266113281
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.3247785568237305
    },
    {
      "epoch": 0.4197289972899729,
      "grad_norm": 13.691765785217285,
      "learning_rate": 1e-05,
      "loss": 7.2909,
      "step": 1936
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 6.853979110717773
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 4.185957908630371
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 8.564746856689453
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 8.607500076293945
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.194698333740234
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.338704109191895
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 7.567476749420166
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.680437088012695
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 6.902261257171631
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 8.195412635803223
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.306352138519287
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.043200969696045
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 5.1203532218933105
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 6.700787544250488
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.085653305053711
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.807476043701172
    },
    {
      "epoch": 0.4205962059620596,
      "grad_norm": 10.451992988586426,
      "learning_rate": 1e-05,
      "loss": 7.3222,
      "step": 1940
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 7.867525577545166
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 7.829272270202637
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 6.678114414215088
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 6.996206283569336
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 7.625606536865234
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.65707540512085
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.985065460205078
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.782149791717529
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.075211524963379
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 6.900926113128662
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.269252300262451
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.79475736618042
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.642531394958496
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 5.0019683837890625
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 5.917761325836182
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.730395317077637
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 15.386128425598145,
      "learning_rate": 1e-05,
      "loss": 6.9221,
      "step": 1944
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.443520545959473
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 7.32811975479126
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.523280620574951
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.642639636993408
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.290378093719482
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 4.975742816925049
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.881274223327637
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.151573657989502
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 6.532840251922607
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 6.237017631530762
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 7.07658576965332
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 7.761266231536865
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 7.303860664367676
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 7.252689361572266
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 8.320611000061035
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 6.927450656890869
    },
    {
      "epoch": 0.42233062330623306,
      "grad_norm": 17.67192268371582,
      "learning_rate": 1e-05,
      "loss": 6.7906,
      "step": 1948
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.624355792999268
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 6.422626972198486
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 6.142302989959717
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.038931369781494
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 7.559001445770264
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.604286193847656
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.675896167755127
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.52841854095459
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.397129535675049
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.168642997741699
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 7.577730178833008
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.841401100158691
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.105597019195557
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.413871765136719
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 6.278578281402588
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.306751728057861
    },
    {
      "epoch": 0.4231978319783198,
      "grad_norm": 13.590761184692383,
      "learning_rate": 1e-05,
      "loss": 6.8553,
      "step": 1952
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.374983310699463
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.561798572540283
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.379948616027832
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 6.370915412902832
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 6.595465183258057
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.41229772567749
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.02300500869751
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.013139247894287
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 7.351168632507324
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.783641815185547
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.59029483795166
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.31036376953125
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 7.154048442840576
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 4.541780948638916
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 6.256210803985596
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 5.983292102813721
    },
    {
      "epoch": 0.4240650406504065,
      "grad_norm": 17.856430053710938,
      "learning_rate": 1e-05,
      "loss": 6.7314,
      "step": 1956
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 6.971498012542725
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 7.170068264007568
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 6.919621467590332
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 8.217477798461914
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 7.411377429962158
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.993001461029053
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.57179594039917
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 7.422266960144043
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 7.746294021606445
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 6.105112075805664
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 6.962817192077637
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 5.1677069664001465
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 8.667131423950195
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 6.767185688018799
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 7.4697113037109375
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 7.979172229766846
    },
    {
      "epoch": 0.42493224932249324,
      "grad_norm": 15.37397289276123,
      "learning_rate": 1e-05,
      "loss": 7.1589,
      "step": 1960
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 6.983154773712158
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 5.882586479187012
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 6.503383159637451
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 7.089922904968262
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 7.263847351074219
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.3130598068237305
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.571686744689941
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.898287773132324
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.403120040893555
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.763805389404297
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.71752405166626
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 7.504560947418213
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 7.0814385414123535
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.498963832855225
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 7.172513484954834
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.693256378173828
    },
    {
      "epoch": 0.42579945799457997,
      "grad_norm": 15.651253700256348,
      "learning_rate": 1e-05,
      "loss": 6.7713,
      "step": 1964
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 7.026828289031982
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 8.342308044433594
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 8.198749542236328
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 6.7507734298706055
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.750661849975586
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.066956996917725
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.043732643127441
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 8.29996109008789
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.9204230308532715
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 8.187813758850098
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 4.72829532623291
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.3525238037109375
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 7.213558673858643
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 4.701779842376709
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 7.079111576080322
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 8.880973815917969
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 10.618593215942383,
      "learning_rate": 1e-05,
      "loss": 7.159,
      "step": 1968
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 6.579076766967773
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.4573469161987305
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 8.305821418762207
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.151198387145996
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.881896495819092
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.3196282386779785
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 8.036487579345703
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 5.304418087005615
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 4.559196472167969
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 8.081337928771973
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 7.021636486053467
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 7.767545223236084
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.316423416137695
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.6828932762146
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.745388507843018
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 6.191034317016602
    },
    {
      "epoch": 0.42753387533875337,
      "grad_norm": 11.665891647338867,
      "learning_rate": 1e-05,
      "loss": 7.0251,
      "step": 1972
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 7.936312675476074
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 6.955715656280518
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 8.38556957244873
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 5.923854827880859
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 7.645789623260498
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 7.23798131942749
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 5.679558753967285
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 6.594541549682617
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.224583148956299
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.335134029388428
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 9.031699180603027
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.1317949295043945
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.165164947509766
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 5.137516021728516
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.757380485534668
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.9374847412109375
    },
    {
      "epoch": 0.4284010840108401,
      "grad_norm": 10.788152694702148,
      "learning_rate": 1e-05,
      "loss": 7.005,
      "step": 1976
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 5.954747676849365
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 7.710104465484619
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 8.561812400817871
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 7.082159996032715
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.62467622756958
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.250877857208252
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.678908824920654
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 7.935847759246826
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 8.234628677368164
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 6.941432476043701
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 6.2896504402160645
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 5.495303153991699
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 6.510032653808594
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 4.867443084716797
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 7.465303897857666
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 7.058575630187988
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 9.85473346710205,
      "learning_rate": 1e-05,
      "loss": 6.8538,
      "step": 1980
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.7476701736450195
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.509133815765381
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 4.123628616333008
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 6.655900001525879
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.669153690338135
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 8.09969711303711
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.3873467445373535
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.78691291809082
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 6.474372386932373
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 6.360513210296631
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 7.359066009521484
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 8.183147430419922
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 7.824506759643555
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 4.4300217628479
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 7.524045467376709
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 6.31549072265625
    },
    {
      "epoch": 0.43013550135501355,
      "grad_norm": 15.7307710647583,
      "learning_rate": 1e-05,
      "loss": 6.7782,
      "step": 1984
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 6.6879072189331055
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.03000020980835
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.4502763748168945
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.038091659545898
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 7.109073638916016
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 8.37456226348877
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 7.430029392242432
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 6.720231533050537
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.25650691986084
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.41398286819458
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.511202335357666
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 7.144651889801025
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.860373020172119
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 4.5792155265808105
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.831054210662842
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 5.506758689880371
    },
    {
      "epoch": 0.4310027100271003,
      "grad_norm": 11.347823143005371,
      "learning_rate": 1e-05,
      "loss": 6.7465,
      "step": 1988
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.926590442657471
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.852100372314453
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 8.31456470489502
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.915981769561768
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 4.174050807952881
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 7.759764671325684
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 6.598691463470459
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 6.864677906036377
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 6.910934925079346
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 7.448502063751221
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 7.696954727172852
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 6.857715606689453
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.754056453704834
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.856870174407959
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.503749847412109
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.55587100982666
    },
    {
      "epoch": 0.431869918699187,
      "grad_norm": 12.2257719039917,
      "learning_rate": 1e-05,
      "loss": 6.8744,
      "step": 1992
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 5.4187092781066895
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 8.052759170532227
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.97600793838501
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.721277713775635
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 7.309533596038818
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 6.469273090362549
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 7.42769193649292
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 6.989121437072754
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.927725791931152
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 7.3562912940979
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.560801982879639
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.854404926300049
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.963420867919922
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.9125518798828125
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.3697075843811035
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 7.212264060974121
    },
    {
      "epoch": 0.43273712737127373,
      "grad_norm": 11.187137603759766,
      "learning_rate": 1e-05,
      "loss": 6.9076,
      "step": 1996
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.0475053787231445
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 6.562124252319336
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 5.170841217041016
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.37847900390625
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 6.926302433013916
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 8.096616744995117
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 7.417169570922852
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 6.49241828918457
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 6.823380947113037
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.091014385223389
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.034282207489014
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.916765213012695
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.817077159881592
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 6.454538822174072
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.3396897315979
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.636676788330078
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 10.755325317382812,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 2000
    },
    {
      "epoch": 0.43360433604336046,
      "eval_runtime": 474.0073,
      "eval_samples_per_second": 4.325,
      "eval_steps_per_second": 4.325,
      "step": 2000
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 5.918861389160156
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 9.85246753692627
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 4.512612819671631
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 7.267844200134277
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 5.902722358703613
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 7.44755220413208
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 6.023986339569092
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 7.051623821258545
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.841248512268066
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.575092792510986
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.860210418701172
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 7.016875743865967
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.048031330108643
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 8.598430633544922
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.8741068840026855
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 7.0770087242126465
    },
    {
      "epoch": 0.43447154471544713,
      "grad_norm": 18.030179977416992,
      "learning_rate": 1e-05,
      "loss": 6.8668,
      "step": 2004
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 7.05330228805542
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 6.261926174163818
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 5.952279567718506
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 5.0234293937683105
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 6.324465751647949
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 5.868946552276611
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 7.7310004234313965
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 6.657653331756592
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 7.042103290557861
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 6.5453009605407715
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 7.1388726234436035
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 7.350517749786377
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 6.876318454742432
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 7.497003555297852
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 6.3268723487854
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 7.48751163482666
    },
    {
      "epoch": 0.43533875338753386,
      "grad_norm": 11.210491180419922,
      "learning_rate": 1e-05,
      "loss": 6.6961,
      "step": 2008
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 11.240562438964844
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 3.836951971054077
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 5.78495979309082
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 6.3249125480651855
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 6.704930782318115
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 6.398032188415527
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 7.839704990386963
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 6.962482929229736
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 6.398404121398926
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 5.335338592529297
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 7.466944217681885
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 5.739468574523926
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 5.583563804626465
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 6.108413219451904
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 7.495165824890137
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 7.106810092926025
    },
    {
      "epoch": 0.4362059620596206,
      "grad_norm": 12.633461952209473,
      "learning_rate": 1e-05,
      "loss": 6.6454,
      "step": 2012
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 6.174156665802002
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 6.7156758308410645
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 6.0259199142456055
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 5.669301986694336
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 5.546318054199219
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 4.71903133392334
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 7.438973426818848
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 7.186887741088867
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 6.705123424530029
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 6.18578577041626
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 8.779276847839355
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 7.918334484100342
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 6.499548435211182
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 6.5636396408081055
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 7.308687686920166
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 7.286022186279297
    },
    {
      "epoch": 0.4370731707317073,
      "grad_norm": 13.56058120727539,
      "learning_rate": 1e-05,
      "loss": 6.6702,
      "step": 2016
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 6.765225887298584
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 5.314556121826172
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 6.742745399475098
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 6.83982515335083
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 7.7035722732543945
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 5.4919586181640625
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 6.928928852081299
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 6.542724609375
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 5.676627159118652
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 6.553335189819336
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 7.144928932189941
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 7.346505641937256
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 7.155454635620117
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 7.451202392578125
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 6.842846870422363
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 6.459995269775391
    },
    {
      "epoch": 0.43794037940379404,
      "grad_norm": 17.45388412475586,
      "learning_rate": 1e-05,
      "loss": 6.685,
      "step": 2020
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 4.13609504699707
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 7.271722316741943
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 7.407220840454102
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 7.240621566772461
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.025007247924805
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.369734764099121
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.282633304595947
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.834190845489502
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 6.4812469482421875
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 6.8594512939453125
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 7.331337928771973
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 9.04451847076416
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 6.7956624031066895
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 5.9203267097473145
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 7.214666366577148
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 6.217211723327637
    },
    {
      "epoch": 0.4388075880758808,
      "grad_norm": 11.845840454101562,
      "learning_rate": 1e-05,
      "loss": 6.9645,
      "step": 2024
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 4.529310703277588
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 7.436314105987549
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 7.376768112182617
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 6.819875240325928
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 7.893923759460449
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 6.290761470794678
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 5.6455864906311035
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 5.656461238861084
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 8.101836204528809
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 7.360912322998047
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 6.907203197479248
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 5.898280143737793
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 8.42658519744873
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 7.108557224273682
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 7.062076568603516
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 6.515585899353027
    },
    {
      "epoch": 0.4396747967479675,
      "grad_norm": 12.913022994995117,
      "learning_rate": 1e-05,
      "loss": 6.8144,
      "step": 2028
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 8.228590965270996
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 6.633208751678467
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 6.07586669921875
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 5.827147960662842
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 6.8737287521362305
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 5.572175979614258
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 6.613647937774658
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 8.054043769836426
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 4.797590255737305
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 6.659666061401367
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 7.682240009307861
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 9.841906547546387
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 6.751793384552002
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 6.28228235244751
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 7.260073661804199
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 6.688211441040039
    },
    {
      "epoch": 0.44054200542005423,
      "grad_norm": 14.985793113708496,
      "learning_rate": 1e-05,
      "loss": 6.8651,
      "step": 2032
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 6.133472442626953
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 6.79282283782959
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 7.059307098388672
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 6.962420463562012
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 8.27832317352295
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 7.410504341125488
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 5.950091361999512
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 7.072184085845947
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 6.846389293670654
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 6.088312149047852
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 6.8515143394470215
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 7.143558025360107
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 8.41576862335205
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 7.550912380218506
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 6.474722862243652
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 6.685776233673096
    },
    {
      "epoch": 0.4414092140921409,
      "grad_norm": 16.24106216430664,
      "learning_rate": 1e-05,
      "loss": 6.9823,
      "step": 2036
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 7.0143632888793945
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 7.226716995239258
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 6.66057014465332
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 6.6094465255737305
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 7.505297660827637
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 5.192534446716309
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 5.494925498962402
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 6.01582670211792
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 6.269710063934326
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 6.142772197723389
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 7.059281826019287
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 6.264628887176514
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 5.9045939445495605
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 6.274190425872803
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 6.9312005043029785
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 6.391717910766602
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 10.006145477294922,
      "learning_rate": 1e-05,
      "loss": 6.4349,
      "step": 2040
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 7.1392822265625
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 8.03895092010498
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 7.462964057922363
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 6.512342929840088
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 7.465098857879639
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 7.440260887145996
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 7.277509689331055
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 6.9109883308410645
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 6.688000679016113
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 7.801788806915283
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 6.604415416717529
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 5.505950450897217
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 7.349639892578125
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 5.889739990234375
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 6.746487617492676
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 5.564351558685303
    },
    {
      "epoch": 0.44314363143631436,
      "grad_norm": 12.27657413482666,
      "learning_rate": 1e-05,
      "loss": 6.8999,
      "step": 2044
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 6.42225980758667
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 8.24091911315918
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 6.303098201751709
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 5.58589506149292
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 5.45297384262085
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 6.9078264236450195
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 7.33496618270874
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 6.760104179382324
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 6.6966233253479
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 6.547560214996338
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 7.559627532958984
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 7.123706817626953
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 7.704024791717529
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 7.965045928955078
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 6.502099990844727
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 7.981740951538086
    },
    {
      "epoch": 0.4440108401084011,
      "grad_norm": 18.294593811035156,
      "learning_rate": 1e-05,
      "loss": 6.943,
      "step": 2048
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 6.126600742340088
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 6.890950679779053
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 7.135532379150391
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 6.901615619659424
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 6.395834922790527
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 6.147204399108887
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 6.427203178405762
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 7.420367240905762
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.874795436859131
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.90657377243042
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 7.78657865524292
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.335750579833984
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 6.480213165283203
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 7.69763708114624
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 6.003347873687744
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 5.818614959716797
    },
    {
      "epoch": 0.4448780487804878,
      "grad_norm": 12.039874076843262,
      "learning_rate": 1e-05,
      "loss": 6.7093,
      "step": 2052
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 6.990788459777832
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 7.484002590179443
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 6.9094672203063965
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 6.580833435058594
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 6.049571514129639
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 6.8415703773498535
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 6.77030086517334
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 7.633355617523193
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 7.303759574890137
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 6.4001336097717285
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 7.297357559204102
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 6.782413959503174
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 7.152275562286377
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 5.8425116539001465
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 7.1799635887146
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 7.479549407958984
    },
    {
      "epoch": 0.44574525745257454,
      "grad_norm": 14.382076263427734,
      "learning_rate": 1e-05,
      "loss": 6.9186,
      "step": 2056
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 7.1495232582092285
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 6.585055351257324
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 7.474423408508301
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 6.626406192779541
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 6.8742499351501465
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 7.088798999786377
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 6.10758113861084
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 8.115938186645508
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 5.798659801483154
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 6.864704608917236
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 7.304742336273193
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 6.904646396636963
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 7.109765529632568
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 7.150049686431885
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 6.910084247589111
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 7.372923851013184
    },
    {
      "epoch": 0.44661246612466127,
      "grad_norm": 13.415401458740234,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 2060
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 7.453076362609863
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 5.7041096687316895
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 8.01655101776123
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 6.6333441734313965
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 6.876500129699707
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 7.468991756439209
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 5.335824489593506
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 6.937369346618652
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 7.273611545562744
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 7.257974624633789
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 7.632248401641846
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 7.46594762802124
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 7.098423957824707
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 8.755719184875488
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 6.834168910980225
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 4.510146617889404
    },
    {
      "epoch": 0.447479674796748,
      "grad_norm": 11.219548225402832,
      "learning_rate": 1e-05,
      "loss": 6.9534,
      "step": 2064
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 5.44390869140625
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 8.23043441772461
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 7.117440700531006
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 7.228290557861328
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 5.9758195877075195
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 7.757004261016846
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 7.40775203704834
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 6.752233982086182
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 7.360048770904541
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 7.91581392288208
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 5.837075233459473
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 7.735706806182861
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 7.078796863555908
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 6.919654846191406
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 7.563243865966797
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 7.0074944496154785
    },
    {
      "epoch": 0.44834688346883467,
      "grad_norm": 13.254476547241211,
      "learning_rate": 1e-05,
      "loss": 7.0832,
      "step": 2068
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 8.653695106506348
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 5.786529541015625
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 7.471118450164795
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 7.172071933746338
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 8.057755470275879
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 6.406065464019775
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 7.123912811279297
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 7.095614910125732
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 5.549007415771484
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 7.374480247497559
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 6.730444431304932
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 6.981256484985352
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 7.892746448516846
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 7.225477695465088
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 7.290403842926025
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 5.640521049499512
    },
    {
      "epoch": 0.4492140921409214,
      "grad_norm": 12.199268341064453,
      "learning_rate": 1e-05,
      "loss": 7.0282,
      "step": 2072
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 7.077489376068115
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.832889556884766
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 7.431509494781494
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.7416253089904785
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 7.51584529876709
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 7.911576747894287
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 7.103511810302734
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 6.5787224769592285
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 6.8734130859375
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 6.252357006072998
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 6.335142135620117
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 6.768330097198486
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 5.682096004486084
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 6.379043102264404
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 7.2062087059021
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 7.97678804397583
    },
    {
      "epoch": 0.4500813008130081,
      "grad_norm": 12.37453556060791,
      "learning_rate": 1e-05,
      "loss": 6.9167,
      "step": 2076
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 6.780143737792969
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 3.9613616466522217
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 6.35727071762085
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 8.18764591217041
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 6.121100902557373
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 6.903622627258301
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 5.925360202789307
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 6.792529582977295
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 7.136044025421143
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 8.060734748840332
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 7.682672023773193
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 6.348550319671631
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 6.993982315063477
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 6.9177470207214355
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 7.093067646026611
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 5.332559108734131
    },
    {
      "epoch": 0.45094850948509485,
      "grad_norm": 12.093749046325684,
      "learning_rate": 1e-05,
      "loss": 6.6621,
      "step": 2080
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 6.900513172149658
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 6.919442653656006
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 7.989796161651611
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 7.74516487121582
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 7.406830787658691
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 4.125831604003906
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 3.1971957683563232
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 6.35194206237793
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 5.686915397644043
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 6.117166996002197
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 8.163737297058105
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 6.7076263427734375
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 7.04215145111084
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 4.640275955200195
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 7.819491863250732
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 7.39049768447876
    },
    {
      "epoch": 0.4518157181571816,
      "grad_norm": 16.140060424804688,
      "learning_rate": 1e-05,
      "loss": 6.5128,
      "step": 2084
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 6.525530815124512
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 6.453183174133301
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 7.4332756996154785
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 5.505910396575928
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 4.858258247375488
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 6.717040061950684
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 6.360349655151367
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 8.03193187713623
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 8.290793418884277
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 7.140141010284424
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 5.226369857788086
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 8.787144660949707
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 5.25536584854126
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 6.302313804626465
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 7.304789066314697
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 5.791700839996338
    },
    {
      "epoch": 0.4526829268292683,
      "grad_norm": 11.045086860656738,
      "learning_rate": 1e-05,
      "loss": 6.624,
      "step": 2088
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 7.09366512298584
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 7.432729721069336
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 7.7533674240112305
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 5.697821140289307
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 7.5337815284729
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 7.395048141479492
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 6.837111949920654
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 6.531551837921143
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 6.83535099029541
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 8.805550575256348
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 5.583382606506348
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 6.825119972229004
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 7.002035140991211
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 6.7242112159729
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 4.505707740783691
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 7.608689308166504
    },
    {
      "epoch": 0.45355013550135503,
      "grad_norm": 12.398758888244629,
      "learning_rate": 1e-05,
      "loss": 6.8853,
      "step": 2092
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 4.215890884399414
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 6.651864051818848
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 7.40092134475708
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 7.607505798339844
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 5.784031867980957
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 7.510828495025635
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 6.957939147949219
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 7.353462219238281
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 8.529455184936523
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 7.138179302215576
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 7.153011322021484
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 5.1228437423706055
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 6.322464466094971
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 6.728039264678955
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 7.6081037521362305
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 7.517060279846191
    },
    {
      "epoch": 0.45441734417344176,
      "grad_norm": 16.793350219726562,
      "learning_rate": 1e-05,
      "loss": 6.8501,
      "step": 2096
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 7.634174346923828
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 6.722151279449463
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 7.583441257476807
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 8.010684967041016
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 6.33485746383667
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 7.057460308074951
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 6.6945672035217285
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 5.0065178871154785
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.929532051086426
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.144448757171631
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.230805397033691
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.361001491546631
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 4.089776992797852
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 7.0484466552734375
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 7.0575690269470215
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 8.136985778808594
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 17.59453010559082,
      "learning_rate": 1e-05,
      "loss": 6.9402,
      "step": 2100
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 6.42485237121582
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 7.680187225341797
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 6.75713586807251
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 7.503354549407959
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 6.149078369140625
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 7.037574291229248
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 7.0069756507873535
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 9.146876335144043
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.106690406799316
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.190027236938477
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 7.285189628601074
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.764312267303467
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 7.354664325714111
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 5.68789005279541
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 6.774499893188477
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 7.251668930053711
    },
    {
      "epoch": 0.45615176151761516,
      "grad_norm": 12.107847213745117,
      "learning_rate": 1e-05,
      "loss": 6.9451,
      "step": 2104
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 7.02238655090332
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 7.811753273010254
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 6.521302223205566
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 6.910353660583496
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.741435527801514
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 7.548399448394775
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.208808898925781
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.386965751647949
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 7.797554969787598
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 5.705880641937256
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 6.053569793701172
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 6.68353796005249
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 8.003617286682129
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 6.409718990325928
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 7.47691535949707
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 5.538437843322754
    },
    {
      "epoch": 0.4570189701897019,
      "grad_norm": 10.82125473022461,
      "learning_rate": 1e-05,
      "loss": 6.8013,
      "step": 2108
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 7.660813331604004
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 6.896162509918213
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 5.804019927978516
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 7.175207614898682
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.363626480102539
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.311377048492432
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.235686779022217
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.1498870849609375
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 7.609403610229492
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 5.340078830718994
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 6.939882278442383
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 7.208590984344482
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 5.396722793579102
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 6.766181468963623
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 6.9040679931640625
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 7.089394569396973
    },
    {
      "epoch": 0.4578861788617886,
      "grad_norm": 11.23716926574707,
      "learning_rate": 1e-05,
      "loss": 6.8657,
      "step": 2112
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 7.005428314208984
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 5.753154277801514
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 7.52161979675293
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 7.011109828948975
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 6.665330410003662
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 8.447747230529785
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 7.029417514801025
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 7.6613359451293945
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 6.69730281829834
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 6.552768707275391
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 7.816263675689697
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 6.664663314819336
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 6.635520935058594
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 7.068002223968506
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 7.18618631362915
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 7.281238079071045
    },
    {
      "epoch": 0.45875338753387535,
      "grad_norm": 12.690630912780762,
      "learning_rate": 1e-05,
      "loss": 7.0623,
      "step": 2116
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 5.795688629150391
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 7.883099555969238
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 7.39345645904541
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 6.6165595054626465
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 8.769750595092773
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 7.569592475891113
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 6.92317533493042
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 6.877312183380127
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 8.131208419799805
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 6.9922943115234375
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 8.6318998336792
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 6.778153419494629
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 8.114786148071289
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 6.665377616882324
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 7.177454471588135
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 7.333897113800049
    },
    {
      "epoch": 0.4596205962059621,
      "grad_norm": 14.409478187561035,
      "learning_rate": 1e-05,
      "loss": 7.3534,
      "step": 2120
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 7.935389518737793
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 7.159140586853027
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 7.803267002105713
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 6.524662017822266
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 6.478813171386719
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 5.902939319610596
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 5.220372676849365
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 7.657370567321777
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 6.597390651702881
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 7.846309185028076
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 5.275051593780518
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 5.552610397338867
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 6.984900951385498
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 7.678740978240967
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 6.637451171875
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 5.710893630981445
    },
    {
      "epoch": 0.4604878048780488,
      "grad_norm": 10.951179504394531,
      "learning_rate": 1e-05,
      "loss": 6.6853,
      "step": 2124
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 6.652831077575684
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 7.692720890045166
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 6.816586017608643
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 6.625339031219482
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 7.275899887084961
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 7.4045257568359375
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 6.207015514373779
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 7.012758255004883
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 4.607516765594482
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 7.0625481605529785
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 7.491255283355713
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 7.453239917755127
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 7.131300926208496
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 6.8724365234375
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 6.092962741851807
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 4.71597146987915
    },
    {
      "epoch": 0.46135501355013553,
      "grad_norm": 11.64560317993164,
      "learning_rate": 1e-05,
      "loss": 6.6947,
      "step": 2128
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 6.802822589874268
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 6.837444305419922
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 6.95534610748291
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 8.741759300231934
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 6.557315349578857
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 5.195887088775635
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 7.969493865966797
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 6.620577812194824
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 7.753662586212158
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 6.191361904144287
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 5.857883453369141
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 6.002373218536377
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 9.812315940856934
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 5.59945821762085
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 7.367214679718018
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 6.743467330932617
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 17.269620895385742,
      "learning_rate": 1e-05,
      "loss": 6.938,
      "step": 2132
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 5.717250823974609
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 6.263455390930176
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 5.810706615447998
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 6.971035480499268
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 7.5058159828186035
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 6.586692810058594
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 7.666326522827148
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 7.762138366699219
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 6.132370471954346
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 7.3295674324035645
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 6.105710029602051
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 7.155364990234375
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 6.607657432556152
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 7.694698333740234
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 7.254777431488037
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 7.512628078460693
    },
    {
      "epoch": 0.46308943089430893,
      "grad_norm": 16.118839263916016,
      "learning_rate": 1e-05,
      "loss": 6.8798,
      "step": 2136
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 7.051233768463135
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 6.961533069610596
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 8.945550918579102
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 7.849024295806885
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 6.946262836456299
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 4.876161575317383
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 7.206993579864502
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 6.826829433441162
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.145820617675781
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 6.096856117248535
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.30278205871582
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.50028133392334
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 6.7486653327941895
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 6.016116142272949
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 5.662845611572266
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 7.439801216125488
    },
    {
      "epoch": 0.46395663956639566,
      "grad_norm": 9.179425239562988,
      "learning_rate": 1e-05,
      "loss": 6.911,
      "step": 2140
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 7.579479217529297
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 6.981855869293213
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 5.873795986175537
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 7.290746212005615
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 7.259194374084473
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 6.847359657287598
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 4.848418712615967
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 8.318326950073242
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 6.618995189666748
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 5.989109992980957
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 7.633581161499023
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 6.038390159606934
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 6.915976524353027
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 4.523098945617676
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 6.392050743103027
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 6.64594030380249
    },
    {
      "epoch": 0.4648238482384824,
      "grad_norm": 13.810060501098633,
      "learning_rate": 1e-05,
      "loss": 6.6098,
      "step": 2144
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 7.0830841064453125
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 7.588844299316406
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 5.914186954498291
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 7.0549116134643555
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 6.768389701843262
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 6.266383647918701
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 7.68505859375
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 8.567216873168945
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 6.93697452545166
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 7.443845748901367
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 6.471052646636963
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 5.064761161804199
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 7.419659614562988
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 6.919772624969482
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 8.004714012145996
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 7.556612491607666
    },
    {
      "epoch": 0.4656910569105691,
      "grad_norm": 14.362979888916016,
      "learning_rate": 1e-05,
      "loss": 7.0466,
      "step": 2148
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 6.78598165512085
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 8.54777717590332
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 7.016921520233154
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 6.243384838104248
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 5.485531806945801
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 7.068531513214111
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 7.662126064300537
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 7.819609642028809
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 7.102567195892334
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 6.569754600524902
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 7.306872367858887
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 7.177052021026611
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 5.931009292602539
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 7.049139022827148
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 5.975006103515625
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 4.8222975730896
    },
    {
      "epoch": 0.46655826558265584,
      "grad_norm": 16.023605346679688,
      "learning_rate": 1e-05,
      "loss": 6.7852,
      "step": 2152
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 7.469280242919922
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 6.9100751876831055
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 7.066476821899414
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 5.779624938964844
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 6.6532464027404785
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 6.821537971496582
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 6.911084175109863
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 6.104361534118652
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 6.984292030334473
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 7.336058139801025
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 8.216737747192383
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 6.920236587524414
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 7.055210590362549
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 6.404190540313721
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 6.491267204284668
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 6.568965435028076
    },
    {
      "epoch": 0.46742547425474257,
      "grad_norm": 12.789859771728516,
      "learning_rate": 1e-05,
      "loss": 6.8558,
      "step": 2156
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 7.439072132110596
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 6.9501190185546875
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 7.329237461090088
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 7.884912014007568
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.7128190994262695
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 5.333014965057373
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.8613667488098145
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.401939868927002
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 6.965678691864014
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 6.478317737579346
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 7.439549922943115
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 7.426578998565674
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 5.768317222595215
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.671884059906006
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.997105121612549
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.4637250900268555
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 9.681229591369629,
      "learning_rate": 1e-05,
      "loss": 6.8202,
      "step": 2160
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 7.680425643920898
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 8.605831146240234
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 6.396136283874512
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 6.8948516845703125
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 6.198472499847412
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 6.5429887771606445
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 4.531611919403076
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 6.764856815338135
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 7.2966132164001465
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 7.0922088623046875
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 6.296435356140137
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 7.240511894226074
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 7.3313469886779785
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 7.455491065979004
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 4.606653690338135
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 7.5162553787231445
    },
    {
      "epoch": 0.46915989159891597,
      "grad_norm": 12.548358917236328,
      "learning_rate": 1e-05,
      "loss": 6.7782,
      "step": 2164
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 6.357338905334473
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 4.665679454803467
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 8.014599800109863
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 6.6523332595825195
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 4.812036037445068
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 7.198979377746582
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 7.1336445808410645
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 6.689379692077637
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 7.410267353057861
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 7.3593549728393555
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 4.644268989562988
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 5.810447692871094
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 6.0343756675720215
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 7.321684837341309
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 4.123107433319092
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 7.021544456481934
    },
    {
      "epoch": 0.4700271002710027,
      "grad_norm": 13.65004825592041,
      "learning_rate": 1e-05,
      "loss": 6.3281,
      "step": 2168
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 7.457618713378906
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 7.085184574127197
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 6.8376874923706055
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 7.0855488777160645
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 6.6564249992370605
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 7.302724361419678
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 7.580353736877441
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 6.35281229019165
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 7.673001289367676
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 7.251017093658447
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 7.24951171875
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 6.992831230163574
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 6.597469329833984
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 7.428386688232422
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 6.074661731719971
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 6.957427024841309
    },
    {
      "epoch": 0.4708943089430894,
      "grad_norm": 11.708233833312988,
      "learning_rate": 1e-05,
      "loss": 7.0364,
      "step": 2172
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 7.46772575378418
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 6.514650821685791
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 6.776605129241943
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 7.01392936706543
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 6.497796535491943
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 7.701740741729736
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 5.964615345001221
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 6.491754055023193
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 6.117491245269775
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 7.364401340484619
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 6.324892044067383
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 5.291462421417236
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 7.892981052398682
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 5.544372081756592
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 6.481897354125977
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 6.435610294342041
    },
    {
      "epoch": 0.47176151761517615,
      "grad_norm": 12.256074905395508,
      "learning_rate": 1e-05,
      "loss": 6.6176,
      "step": 2176
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 7.31882381439209
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 7.634669303894043
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 7.066282749176025
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 6.01713228225708
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 7.095660209655762
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 8.00612735748291
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 6.583031177520752
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 5.945398330688477
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 6.414790630340576
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 7.213292598724365
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 7.446421146392822
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 7.097838878631592
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 7.098877906799316
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 5.634360313415527
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 7.201315879821777
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 7.277095317840576
    },
    {
      "epoch": 0.4726287262872629,
      "grad_norm": 13.966463088989258,
      "learning_rate": 1e-05,
      "loss": 6.9407,
      "step": 2180
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.131323337554932
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.33314847946167
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.88043737411499
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.728744983673096
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 7.250286102294922
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 7.1622633934021
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 6.50063419342041
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 5.434601306915283
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 4.84173059463501
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 7.574305534362793
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 6.765279769897461
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 6.637344837188721
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 6.983510971069336
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 6.871774196624756
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 7.295618534088135
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 4.7170891761779785
    },
    {
      "epoch": 0.4734959349593496,
      "grad_norm": 11.944182395935059,
      "learning_rate": 1e-05,
      "loss": 6.5068,
      "step": 2184
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 6.663150310516357
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 4.143040180206299
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 6.192800045013428
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 7.890469074249268
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 7.0685296058654785
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 6.587914943695068
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 6.72999382019043
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 7.085423469543457
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.040947437286377
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.057987689971924
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.324542045593262
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.421883583068848
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 7.050042629241943
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 7.8344502449035645
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 6.0638427734375
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 7.241588115692139
    },
    {
      "epoch": 0.47436314363143633,
      "grad_norm": 17.49152946472168,
      "learning_rate": 1e-05,
      "loss": 6.5873,
      "step": 2188
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 8.049941062927246
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 7.0351457595825195
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 6.793892860412598
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 7.639283657073975
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 8.165824890136719
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 7.706928253173828
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 7.441578388214111
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 5.693596839904785
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 7.752205848693848
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 6.774967193603516
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 8.217541694641113
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 7.354114055633545
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 6.363683700561523
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 6.403119087219238
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 6.56944465637207
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 7.06676721572876
    },
    {
      "epoch": 0.47523035230352306,
      "grad_norm": 11.923760414123535,
      "learning_rate": 1e-05,
      "loss": 7.1893,
      "step": 2192
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 7.347431182861328
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 7.618584156036377
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 7.140910625457764
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 5.376585006713867
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 7.199970722198486
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 7.265983581542969
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 6.797635555267334
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 7.15289306640625
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 8.044172286987305
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 6.600573539733887
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 7.483640193939209
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 7.454936981201172
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 7.272310733795166
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 7.325054168701172
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 6.662717819213867
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 8.018555641174316
    },
    {
      "epoch": 0.47609756097560973,
      "grad_norm": 16.370710372924805,
      "learning_rate": 1e-05,
      "loss": 7.1726,
      "step": 2196
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 7.286114692687988
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 6.422577857971191
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 6.800711154937744
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 8.322686195373535
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 7.402581691741943
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 6.63663387298584
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 6.062462329864502
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 7.082315444946289
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.156447887420654
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.541533470153809
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.467654228210449
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.069417476654053
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 7.0334792137146
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 5.687337398529053
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 7.238360404968262
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 7.014987468719482
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 11.829980850219727,
      "learning_rate": 1e-05,
      "loss": 7.0141,
      "step": 2200
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 5.904576778411865
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 7.240691661834717
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 7.554862976074219
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 6.12168550491333
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 5.662585735321045
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 5.2763142585754395
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 9.075172424316406
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 6.88257360458374
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 8.473780632019043
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 7.138327121734619
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 6.425328254699707
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 7.971368312835693
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 6.462368965148926
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 7.417200565338135
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 6.909576892852783
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 7.06397008895874
    },
    {
      "epoch": 0.4778319783197832,
      "grad_norm": 13.103740692138672,
      "learning_rate": 1e-05,
      "loss": 6.9738,
      "step": 2204
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 7.246948719024658
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 7.251767635345459
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 6.1687517166137695
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 7.47553014755249
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 4.721426486968994
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 6.573501110076904
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 7.087100505828857
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 5.990380764007568
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 7.155781269073486
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 7.109396457672119
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 9.03266716003418
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 7.374106407165527
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 7.461126804351807
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 6.931873798370361
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 6.44905424118042
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 7.008890628814697
    },
    {
      "epoch": 0.4786991869918699,
      "grad_norm": 10.666288375854492,
      "learning_rate": 1e-05,
      "loss": 6.9399,
      "step": 2208
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 5.684837341308594
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 7.718613147735596
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 6.580870628356934
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 7.29414701461792
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 6.9095354080200195
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 5.93553352355957
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 7.33868932723999
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 5.3976898193359375
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 7.10303258895874
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 7.55148458480835
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 3.989861249923706
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 6.493869304656982
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 7.550359725952148
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 6.4334492683410645
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 6.84861946105957
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 4.535211086273193
    },
    {
      "epoch": 0.47956639566395665,
      "grad_norm": 10.169690132141113,
      "learning_rate": 1e-05,
      "loss": 6.4604,
      "step": 2212
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 6.685391426086426
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 5.878337383270264
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 7.5323872566223145
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 6.280089378356934
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 7.440856456756592
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 6.7292327880859375
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 6.711804389953613
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 6.6396918296813965
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 7.837075710296631
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 7.212770938873291
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 5.901467800140381
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 6.746333122253418
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 7.377887725830078
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 8.857819557189941
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 5.747748851776123
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 7.50635290145874
    },
    {
      "epoch": 0.4804336043360434,
      "grad_norm": 15.621933937072754,
      "learning_rate": 1e-05,
      "loss": 6.9428,
      "step": 2216
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 6.885498046875
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 5.378012657165527
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 7.643808841705322
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 6.367757320404053
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.532043933868408
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.859088897705078
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.929296970367432
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 5.377514839172363
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 8.024845123291016
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 7.549432754516602
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 7.376462936401367
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 5.315974235534668
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.2656097412109375
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.825676441192627
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.727630615234375
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.196930885314941
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 12.938316345214844,
      "learning_rate": 1e-05,
      "loss": 6.641,
      "step": 2220
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 7.05146598815918
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 6.833287239074707
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 7.538150310516357
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 6.763800144195557
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 7.262765407562256
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 6.062478542327881
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 4.4168381690979
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 7.722890853881836
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 6.954296588897705
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 7.311464309692383
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 6.918319225311279
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 6.804562091827393
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 7.894830226898193
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 7.716207504272461
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 7.723410129547119
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 8.036060333251953
    },
    {
      "epoch": 0.48216802168021683,
      "grad_norm": 14.334880828857422,
      "learning_rate": 1e-05,
      "loss": 7.0632,
      "step": 2224
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 5.270034313201904
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 7.978771686553955
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 6.921360492706299
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 5.651036262512207
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 5.047907829284668
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 6.759255886077881
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 5.295555591583252
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 7.405454635620117
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 6.407474517822266
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 7.972489833831787
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 8.096824645996094
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 7.775643348693848
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 5.858593940734863
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.599394798278809
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.762979984283447
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.757669925689697
    },
    {
      "epoch": 0.4830352303523035,
      "grad_norm": 14.904354095458984,
      "learning_rate": 1e-05,
      "loss": 6.66,
      "step": 2228
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 7.342482089996338
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.706090927124023
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.873581409454346
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.354818344116211
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 7.706531524658203
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 7.603359699249268
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 4.753477573394775
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 6.4389801025390625
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 7.583329677581787
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 7.746941566467285
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 7.445497512817383
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 7.660868167877197
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 6.85186243057251
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 7.37226676940918
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 6.892542362213135
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 4.295351982116699
    },
    {
      "epoch": 0.48390243902439023,
      "grad_norm": 18.023773193359375,
      "learning_rate": 1e-05,
      "loss": 6.8517,
      "step": 2232
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 5.995998382568359
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 6.728055477142334
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 7.2871928215026855
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 7.039763927459717
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 5.795856952667236
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 8.015503883361816
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 6.627531051635742
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 6.343437194824219
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 6.6916375160217285
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 7.389889240264893
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 6.7686872482299805
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 8.555428504943848
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 5.213589191436768
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 7.490394115447998
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 8.133655548095703
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 7.50218391418457
    },
    {
      "epoch": 0.48476964769647696,
      "grad_norm": 11.702518463134766,
      "learning_rate": 1e-05,
      "loss": 6.9737,
      "step": 2236
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 7.826199531555176
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 7.413661003112793
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 7.542150974273682
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 6.7336554527282715
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 7.288631916046143
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 8.344890594482422
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 5.0670166015625
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 6.845586776733398
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.737522602081299
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.937704563140869
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.73624324798584
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.580996036529541
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 4.468227863311768
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 7.563847541809082
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 7.694574356079102
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 7.322041034698486
    },
    {
      "epoch": 0.4856368563685637,
      "grad_norm": 12.458637237548828,
      "learning_rate": 1e-05,
      "loss": 6.9439,
      "step": 2240
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 4.0665602684021
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 6.702508926391602
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 6.358743190765381
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 7.605099201202393
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 6.1888813972473145
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 7.167901039123535
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 6.765915393829346
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 7.132163047790527
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 7.059640407562256
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 7.856034755706787
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 5.626621246337891
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 6.7637553215026855
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 7.756160259246826
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 6.9060211181640625
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 5.4732208251953125
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 6.881131172180176
    },
    {
      "epoch": 0.4865040650406504,
      "grad_norm": 16.005653381347656,
      "learning_rate": 1e-05,
      "loss": 6.6444,
      "step": 2244
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 6.334193706512451
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 7.387507438659668
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 7.267833709716797
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 5.030045509338379
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 6.47224235534668
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 7.547650337219238
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 5.703241348266602
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 8.220207214355469
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 6.78856086730957
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 7.4166483879089355
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 5.99696159362793
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 6.860415458679199
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 7.383190155029297
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 7.236249923706055
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 6.252935886383057
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 7.764832019805908
    },
    {
      "epoch": 0.48737127371273714,
      "grad_norm": 11.522597312927246,
      "learning_rate": 1e-05,
      "loss": 6.8539,
      "step": 2248
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 6.713395595550537
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 7.557714939117432
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 6.528894424438477
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 6.5425262451171875
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 6.994837760925293
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 6.927727699279785
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 7.615691661834717
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 7.086339950561523
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.398038387298584
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 7.427974700927734
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.438121318817139
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.618142604827881
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 6.825907230377197
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 8.041792869567871
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 6.868002414703369
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 6.959043979644775
    },
    {
      "epoch": 0.48823848238482387,
      "grad_norm": 13.072568893432617,
      "learning_rate": 1e-05,
      "loss": 6.9715,
      "step": 2252
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 8.341607093811035
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 6.164425373077393
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 6.383453845977783
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 6.936107635498047
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 6.81780481338501
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 6.2577223777771
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 6.830174446105957
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 7.136120319366455
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 7.6822686195373535
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 6.9617791175842285
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 7.102339744567871
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 5.778767108917236
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 8.096343994140625
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 5.741640567779541
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 7.611260414123535
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 6.71742057800293
    },
    {
      "epoch": 0.4891056910569106,
      "grad_norm": 9.578819274902344,
      "learning_rate": 1e-05,
      "loss": 6.91,
      "step": 2256
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 6.833223819732666
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 7.720800876617432
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 5.585368633270264
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 6.870772838592529
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 4.109259128570557
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 6.513064861297607
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 7.912519931793213
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 4.420603275299072
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 7.8064727783203125
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 7.410985469818115
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 8.449281692504883
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 6.164913177490234
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 7.373996257781982
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 7.364060878753662
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 4.591506004333496
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 7.020622730255127
    },
    {
      "epoch": 0.48997289972899727,
      "grad_norm": 12.087158203125,
      "learning_rate": 1e-05,
      "loss": 6.6342,
      "step": 2260
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 6.797422885894775
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 6.981452941894531
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 8.238630294799805
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 6.5764384269714355
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 6.111536502838135
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 7.4798665046691895
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 6.919707775115967
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 7.243544578552246
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 6.991995811462402
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 5.763437271118164
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 6.22499418258667
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 7.801865577697754
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 7.1725592613220215
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 7.751071453094482
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 6.94523811340332
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 7.20244026184082
    },
    {
      "epoch": 0.490840108401084,
      "grad_norm": 13.320391654968262,
      "learning_rate": 1e-05,
      "loss": 7.0126,
      "step": 2264
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 6.960648536682129
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 6.7654805183410645
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 6.858168601989746
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 8.060912132263184
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 6.256616592407227
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 7.159867286682129
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 7.014376163482666
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 7.4561357498168945
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 5.4180684089660645
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 7.009347438812256
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 7.669259548187256
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 6.87945556640625
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.600287437438965
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.931251049041748
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 7.71181058883667
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 7.584054470062256
    },
    {
      "epoch": 0.4917073170731707,
      "grad_norm": 13.45552921295166,
      "learning_rate": 1e-05,
      "loss": 7.021,
      "step": 2268
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.681581497192383
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.209529399871826
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.8762006759643555
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.717170715332031
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 7.538602352142334
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 7.650996208190918
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 6.9269609451293945
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 8.392973899841309
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 7.091414451599121
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 6.012546539306641
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 7.780340671539307
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 6.880919456481934
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 6.959348201751709
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 7.19189453125
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 7.451879978179932
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 7.596964359283447
    },
    {
      "epoch": 0.49257452574525745,
      "grad_norm": 8.324541091918945,
      "learning_rate": 1e-05,
      "loss": 7.1225,
      "step": 2272
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 7.869983673095703
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 7.858819484710693
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 6.740586280822754
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 6.462953567504883
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 7.153476238250732
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 7.1713714599609375
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 6.3225321769714355
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 6.67434024810791
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 5.698336124420166
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 8.448451042175293
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 6.520299911499023
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 8.012275695800781
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 6.501075744628906
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 7.157382011413574
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 7.585440635681152
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 7.379071235656738
    },
    {
      "epoch": 0.4934417344173442,
      "grad_norm": 9.834733009338379,
      "learning_rate": 1e-05,
      "loss": 7.0973,
      "step": 2276
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 7.185122966766357
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 5.844709873199463
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 5.731494426727295
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 7.173276901245117
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 7.1055378913879395
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 6.775516510009766
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 7.268704891204834
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 8.314346313476562
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 6.647201061248779
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 6.851682186126709
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 6.462589263916016
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 8.63152027130127
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 8.524087905883789
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 5.9415130615234375
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 7.020982265472412
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 6.671607971191406
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 17.9633846282959,
      "learning_rate": 1e-05,
      "loss": 7.0094,
      "step": 2280
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 6.557513236999512
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 8.037858009338379
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 5.415314197540283
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 6.677626609802246
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 6.9802727699279785
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 5.967917442321777
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 7.228368282318115
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 6.712632179260254
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 6.681687831878662
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 7.576896667480469
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 6.5363054275512695
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 7.681334018707275
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 6.99040412902832
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 6.008564472198486
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 6.382514476776123
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 7.319813251495361
    },
    {
      "epoch": 0.49517615176151764,
      "grad_norm": 11.22205924987793,
      "learning_rate": 1e-05,
      "loss": 6.7972,
      "step": 2284
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 7.859574317932129
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 7.0368428230285645
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 7.097421169281006
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 8.01550006866455
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 8.22361946105957
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 4.605180263519287
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 7.530862808227539
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 7.206852912902832
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 7.364536762237549
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 7.8571577072143555
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 7.007402420043945
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 7.480932235717773
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 6.307414531707764
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 7.225489139556885
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 7.244235038757324
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 7.089242458343506
    },
    {
      "epoch": 0.49604336043360436,
      "grad_norm": 11.356942176818848,
      "learning_rate": 1e-05,
      "loss": 7.197,
      "step": 2288
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 6.424074172973633
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 7.525928974151611
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 5.746054649353027
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 6.143566608428955
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 5.472163677215576
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 7.319996356964111
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 6.4939703941345215
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 6.631955623626709
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 6.648739337921143
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 8.014812469482422
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 6.259293079376221
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 6.175239086151123
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 7.33079719543457
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 5.6863250732421875
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 7.038195610046387
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 6.769552230834961
    },
    {
      "epoch": 0.49691056910569104,
      "grad_norm": 13.933785438537598,
      "learning_rate": 1e-05,
      "loss": 6.605,
      "step": 2292
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 7.572305679321289
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 7.940736770629883
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 5.8332295417785645
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 7.135599613189697
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.545456886291504
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.294827938079834
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.635679244995117
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.324005603790283
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 7.672304153442383
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 4.567852973937988
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 4.858391761779785
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 5.121622562408447
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.516193389892578
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.477806568145752
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.270817279815674
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.294106483459473
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 10.396217346191406,
      "learning_rate": 1e-05,
      "loss": 6.6288,
      "step": 2296
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 7.364548683166504
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 6.009206771850586
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 7.242728233337402
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 7.7322235107421875
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 8.317683219909668
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 6.37503719329834
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 5.145934104919434
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 7.327960968017578
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 5.983345031738281
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 7.539910316467285
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 4.672418594360352
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 7.021651268005371
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 5.855503559112549
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 5.828517436981201
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 7.518482208251953
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 5.949386119842529
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 11.584471702575684,
      "learning_rate": 1e-05,
      "loss": 6.6178,
      "step": 2300
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 6.636018753051758
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 6.598762512207031
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 6.018390655517578
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 7.331639289855957
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 5.730061054229736
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 5.6239094734191895
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 7.095487594604492
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 7.228408336639404
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 7.070274353027344
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 8.650586128234863
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 5.163390159606934
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 6.713766574859619
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 7.2727155685424805
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 6.610361099243164
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 7.172774791717529
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 6.88468074798584
    },
    {
      "epoch": 0.4995121951219512,
      "grad_norm": 12.309836387634277,
      "learning_rate": 1e-05,
      "loss": 6.7376,
      "step": 2304
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 5.736874103546143
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 7.105919361114502
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 7.7086100578308105
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 6.738357067108154
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 8.103914260864258
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 7.81634521484375
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 7.872341156005859
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 10.162135124206543
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 4.405064105987549
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 7.327766418457031
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 6.405333995819092
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 7.449780464172363
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 6.551460266113281
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 7.482725620269775
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 7.030477523803711
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 6.644619941711426
    },
    {
      "epoch": 0.5003794037940379,
      "grad_norm": 14.755213737487793,
      "learning_rate": 1e-05,
      "loss": 7.1589,
      "step": 2308
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 6.209171295166016
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 5.917487144470215
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 6.32802677154541
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 6.004030227661133
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 6.932127475738525
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 7.0696797370910645
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 6.373460292816162
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 7.2206010818481445
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 7.406069755554199
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 5.309736251831055
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 6.409610271453857
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 7.342281818389893
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 6.162728786468506
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 7.065801620483398
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 7.553645133972168
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 6.031865119934082
    },
    {
      "epoch": 0.5012466124661247,
      "grad_norm": 12.342442512512207,
      "learning_rate": 1e-05,
      "loss": 6.5835,
      "step": 2312
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 7.274160861968994
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 6.118039608001709
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 5.892732620239258
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 6.152327060699463
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 4.295284271240234
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 6.52397346496582
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 6.618517875671387
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 6.2897629737854
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 7.5509934425354
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 6.519373416900635
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 7.448990345001221
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 7.94338321685791
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 7.371675968170166
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 7.044511318206787
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 5.922222137451172
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 7.862499237060547
    },
    {
      "epoch": 0.5021138211382113,
      "grad_norm": 13.186573028564453,
      "learning_rate": 1e-05,
      "loss": 6.6768,
      "step": 2316
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 8.303128242492676
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 6.078944683074951
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 7.508798599243164
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 5.872704029083252
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 7.198906898498535
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 6.845314025878906
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 6.294945240020752
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 7.746402263641357
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 5.44199275970459
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 6.852327346801758
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 5.991989612579346
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 4.041462421417236
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 6.015544414520264
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 7.180425643920898
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 6.719348907470703
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 5.019753456115723
    },
    {
      "epoch": 0.5029810298102981,
      "grad_norm": 11.645991325378418,
      "learning_rate": 1e-05,
      "loss": 6.4445,
      "step": 2320
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 6.989842414855957
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 7.449023723602295
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 7.087903022766113
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 7.66904354095459
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 7.0042877197265625
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 8.091078758239746
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 7.341638088226318
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 6.934621334075928
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 7.40469217300415
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 5.989960193634033
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 6.595091819763184
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 7.643240451812744
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.107717990875244
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.7207112312316895
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.180055141448975
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.844841957092285
    },
    {
      "epoch": 0.5038482384823848,
      "grad_norm": 13.113790512084961,
      "learning_rate": 1e-05,
      "loss": 7.0034,
      "step": 2324
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 6.658657550811768
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 6.563462257385254
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 7.133469104766846
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 7.0003790855407715
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.682619094848633
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.918939113616943
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.756552696228027
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.64220666885376
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 6.885993003845215
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 6.975711822509766
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 6.7112531661987305
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 6.762504577636719
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 6.962131977081299
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 6.447053909301758
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 5.754755973815918
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 6.809087753295898
    },
    {
      "epoch": 0.5047154471544716,
      "grad_norm": 12.438841819763184,
      "learning_rate": 1e-05,
      "loss": 6.729,
      "step": 2328
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 4.502970218658447
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 6.9925007820129395
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 6.252362251281738
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 6.758498191833496
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 5.491933822631836
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 7.6891608238220215
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 7.15629243850708
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 5.940679550170898
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 7.23301362991333
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 5.711327075958252
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 6.2185797691345215
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 7.756136417388916
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 6.022143840789795
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 7.3988142013549805
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 6.181739330291748
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 5.745031833648682
    },
    {
      "epoch": 0.5055826558265583,
      "grad_norm": 11.239104270935059,
      "learning_rate": 1e-05,
      "loss": 6.4407,
      "step": 2332
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 8.041947364807129
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 6.115314483642578
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 6.048578262329102
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 7.111814498901367
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 6.303647518157959
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 7.454707622528076
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 7.465203762054443
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 7.2442545890808105
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 8.192587852478027
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 6.9823384284973145
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 5.521665096282959
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 7.293025016784668
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 8.177425384521484
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 8.0286865234375
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 6.603274345397949
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 7.752811908721924
    },
    {
      "epoch": 0.506449864498645,
      "grad_norm": 11.984048843383789,
      "learning_rate": 1e-05,
      "loss": 7.1461,
      "step": 2336
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 5.949794292449951
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 6.351705074310303
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 7.842196464538574
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 6.825072765350342
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 6.782690525054932
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 4.97705078125
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 6.33119535446167
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 5.416686534881592
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 7.133121490478516
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 5.4538893699646
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 4.356648921966553
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 6.786111831665039
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 7.647588729858398
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 7.013522624969482
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 8.061790466308594
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 6.274486064910889
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 11.646127700805664,
      "learning_rate": 1e-05,
      "loss": 6.4502,
      "step": 2340
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 5.790719509124756
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 7.138439655303955
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 6.577857971191406
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 6.38137149810791
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 9.0420503616333
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 7.394242763519287
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 6.493489742279053
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 7.330668926239014
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 7.211864948272705
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 6.817040920257568
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 7.534049034118652
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 6.8827128410339355
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 6.556780815124512
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 5.952182292938232
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 6.076414108276367
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 6.4402971267700195
    },
    {
      "epoch": 0.5081842818428184,
      "grad_norm": 12.148274421691895,
      "learning_rate": 1e-05,
      "loss": 6.8513,
      "step": 2344
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 7.094594955444336
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 6.850766658782959
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 6.142760276794434
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 6.514831066131592
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 5.579534530639648
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 4.458329200744629
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 7.266457557678223
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 6.528939247131348
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 6.518988132476807
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 6.924685955047607
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 5.802048683166504
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 6.46990442276001
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 7.150992393493652
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 8.8556547164917
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 6.978060722351074
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 7.077686786651611
    },
    {
      "epoch": 0.5090514905149052,
      "grad_norm": 13.464286804199219,
      "learning_rate": 1e-05,
      "loss": 6.6384,
      "step": 2348
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 6.5149617195129395
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 6.489715576171875
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 8.076383590698242
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 6.776278495788574
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 7.510525703430176
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 5.755075931549072
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 5.173762321472168
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 7.060737609863281
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 6.188090801239014
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 7.02993106842041
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 4.625436305999756
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 7.2639360427856445
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 7.478057384490967
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 7.081859588623047
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 6.448467254638672
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 7.41684103012085
    },
    {
      "epoch": 0.5099186991869918,
      "grad_norm": 13.31988525390625,
      "learning_rate": 1e-05,
      "loss": 6.6806,
      "step": 2352
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 6.6956658363342285
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 6.925435543060303
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 7.360335350036621
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 5.612654209136963
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 7.333844184875488
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 7.410505771636963
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 7.432720184326172
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 7.637816905975342
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 6.586947441101074
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 6.85688591003418
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 6.947535514831543
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 7.619475841522217
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 6.4983320236206055
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 4.487545490264893
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 5.7322211265563965
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 5.727097511291504
    },
    {
      "epoch": 0.5107859078590786,
      "grad_norm": 15.30784797668457,
      "learning_rate": 1e-05,
      "loss": 6.6791,
      "step": 2356
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 6.98959493637085
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 7.193638801574707
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 6.317139625549316
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 7.934152603149414
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 6.161142349243164
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 6.629738807678223
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 5.814805030822754
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 7.085445880889893
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 8.504520416259766
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 7.85002326965332
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 7.867783069610596
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 6.265850067138672
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 7.116322994232178
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 7.24543571472168
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 5.497590065002441
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 7.57440710067749
    },
    {
      "epoch": 0.5116531165311653,
      "grad_norm": 11.835763931274414,
      "learning_rate": 1e-05,
      "loss": 7.003,
      "step": 2360
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 6.6256103515625
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 7.580205917358398
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 7.140655040740967
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 7.239922046661377
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 7.167105674743652
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 7.45441198348999
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 7.629201889038086
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 6.837608814239502
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 6.455315113067627
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 4.267739772796631
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 7.101375102996826
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 7.341674327850342
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 8.25566291809082
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 7.760720252990723
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 6.048499584197998
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 6.920924186706543
    },
    {
      "epoch": 0.5125203252032521,
      "grad_norm": 15.019758224487305,
      "learning_rate": 1e-05,
      "loss": 6.9892,
      "step": 2364
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 6.753501892089844
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 6.522096157073975
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 6.784909248352051
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 7.307184219360352
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 4.44899320602417
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 6.840958595275879
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 6.679327011108398
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 7.43365478515625
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 6.534791469573975
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 7.993114471435547
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 6.258545875549316
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 5.530107498168945
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 7.622817039489746
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 7.392999172210693
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 4.741857051849365
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 6.629544258117676
    },
    {
      "epoch": 0.5133875338753388,
      "grad_norm": 11.993670463562012,
      "learning_rate": 1e-05,
      "loss": 6.5922,
      "step": 2368
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 4.80838680267334
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 7.778444290161133
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 7.235029220581055
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 6.976475715637207
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 5.552089214324951
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 6.907423973083496
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 8.346779823303223
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 6.428380966186523
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 7.322305679321289
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 6.961938381195068
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 3.9685556888580322
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 8.102096557617188
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 5.9448747634887695
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 6.448541641235352
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 7.895506381988525
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 5.518752574920654
    },
    {
      "epoch": 0.5142547425474254,
      "grad_norm": 20.12604331970215,
      "learning_rate": 1e-05,
      "loss": 6.6372,
      "step": 2372
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.8853607177734375
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.701299667358398
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 7.521179676055908
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.849665641784668
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 7.511011123657227
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 7.422115325927734
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 6.672871112823486
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 5.95280647277832
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 6.60568904876709
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 5.278992652893066
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 7.232639789581299
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 7.187976837158203
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 6.677722930908203
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 4.22428035736084
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 7.87301778793335
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 6.605607032775879
    },
    {
      "epoch": 0.5151219512195122,
      "grad_norm": 11.516352653503418,
      "learning_rate": 1e-05,
      "loss": 6.7001,
      "step": 2376
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 6.477113723754883
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 7.281742572784424
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 5.412835121154785
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 6.30094051361084
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 5.691413402557373
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 7.7220540046691895
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 9.733586311340332
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 9.06879997253418
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 5.4463701248168945
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 6.327273368835449
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 6.648692607879639
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 8.829681396484375
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 7.835794925689697
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 5.753963947296143
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 6.93437385559082
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 7.469688415527344
    },
    {
      "epoch": 0.5159891598915989,
      "grad_norm": 13.383065223693848,
      "learning_rate": 1e-05,
      "loss": 7.0584,
      "step": 2380
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 6.575503349304199
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 6.948602199554443
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 6.125335216522217
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 7.830715179443359
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 7.198050498962402
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 6.5576491355896
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 7.291121959686279
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 5.420532703399658
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 7.636412620544434
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 7.340132713317871
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 6.447566986083984
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 4.9461989402771
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 6.221593856811523
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 6.971205711364746
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 7.086714744567871
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 6.437554836273193
    },
    {
      "epoch": 0.5168563685636857,
      "grad_norm": 12.089286804199219,
      "learning_rate": 1e-05,
      "loss": 6.6897,
      "step": 2384
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 7.95427942276001
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 4.275272846221924
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 6.521849632263184
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 5.513670921325684
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 7.842084884643555
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 7.168145656585693
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 3.8814547061920166
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 7.158134937286377
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 7.153252601623535
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 6.170762538909912
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 6.876389026641846
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 7.181501865386963
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 6.193200588226318
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 7.737102031707764
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 7.356363296508789
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 7.04132604598999
    },
    {
      "epoch": 0.5177235772357723,
      "grad_norm": 12.139411926269531,
      "learning_rate": 1e-05,
      "loss": 6.6265,
      "step": 2388
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 6.711945533752441
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 6.444393634796143
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 8.403071403503418
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 7.36286735534668
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 5.772186279296875
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 7.17384672164917
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 7.5760016441345215
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 6.250471591949463
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.866016864776611
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 7.221433639526367
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.173117160797119
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.512253284454346
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 6.976535797119141
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 5.165347576141357
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 8.356695175170898
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 6.002647876739502
    },
    {
      "epoch": 0.5185907859078591,
      "grad_norm": 12.098577499389648,
      "learning_rate": 1e-05,
      "loss": 6.8106,
      "step": 2392
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 7.561505317687988
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 7.384321689605713
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 7.750540256500244
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 7.780457496643066
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 6.893123149871826
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 7.030087947845459
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 7.118278503417969
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 5.791453838348389
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.670372009277344
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.986008644104004
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.418156623840332
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 8.025653839111328
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 7.506386756896973
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 7.1441731452941895
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 6.049649715423584
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 7.130312919616699
    },
    {
      "epoch": 0.5194579945799458,
      "grad_norm": 17.58724594116211,
      "learning_rate": 1e-05,
      "loss": 7.0775,
      "step": 2396
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 6.213770389556885
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 6.066048622131348
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 6.528586387634277
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 6.779770851135254
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 6.324228763580322
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 7.018723011016846
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 6.7822184562683105
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 7.164776802062988
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 7.035050868988037
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 7.09804630279541
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 6.7376389503479
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 7.130862712860107
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 7.684057712554932
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 4.283817768096924
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 6.725591659545898
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 6.822591781616211
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 10.444092750549316,
      "learning_rate": 1e-05,
      "loss": 6.6497,
      "step": 2400
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 6.951664447784424
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 4.892939567565918
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 7.183631420135498
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 4.540160179138184
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 5.718820571899414
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 8.273042678833008
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 7.829313278198242
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 7.246791362762451
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 7.17739200592041
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 6.8098015785217285
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 8.049318313598633
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 7.851906776428223
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 6.875898361206055
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 7.8238115310668945
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 7.934503555297852
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 5.190850734710693
    },
    {
      "epoch": 0.5211924119241192,
      "grad_norm": 11.2418851852417,
      "learning_rate": 1e-05,
      "loss": 6.8969,
      "step": 2404
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 7.88555383682251
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 5.705921649932861
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 6.466851234436035
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 5.8341827392578125
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 7.260800361633301
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 5.433126926422119
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 6.662585735321045
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 7.476176738739014
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 6.385128974914551
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 7.667940616607666
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 5.7769269943237305
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 3.7940609455108643
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 6.546256065368652
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 7.488485813140869
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 6.953624248504639
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 4.497984409332275
    },
    {
      "epoch": 0.5220596205962059,
      "grad_norm": 11.243903160095215,
      "learning_rate": 1e-05,
      "loss": 6.3647,
      "step": 2408
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 7.528820037841797
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 7.455644130706787
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 6.944106578826904
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 6.35828161239624
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 7.285860538482666
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 8.742036819458008
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 7.509214878082275
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 6.644900798797607
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 5.456064701080322
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 6.932283878326416
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 7.875686168670654
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 8.145127296447754
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 6.484469413757324
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 5.687037944793701
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 7.293431758880615
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 7.8387675285339355
    },
    {
      "epoch": 0.5229268292682927,
      "grad_norm": 10.424600601196289,
      "learning_rate": 1e-05,
      "loss": 7.1364,
      "step": 2412
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.7796454429626465
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.201766490936279
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.7748212814331055
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.488888740539551
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 5.287881851196289
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 6.61198091506958
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 7.08916711807251
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 4.197822093963623
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 6.601130962371826
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 5.309475898742676
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 6.728652477264404
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 5.850986003875732
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 7.640863418579102
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 5.4641313552856445
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 7.667573928833008
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 7.178481101989746
    },
    {
      "epoch": 0.5237940379403794,
      "grad_norm": 14.396644592285156,
      "learning_rate": 1e-05,
      "loss": 6.3671,
      "step": 2416
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 6.669693946838379
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 4.287955284118652
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 7.204092979431152
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 7.087838649749756
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 7.260562419891357
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 4.755768299102783
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 7.853824615478516
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 6.971259117126465
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 7.795543670654297
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 7.389706134796143
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 7.606832504272461
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 6.420858383178711
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 8.738157272338867
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.638448238372803
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.1202521324157715
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.019858360290527
    },
    {
      "epoch": 0.5246612466124662,
      "grad_norm": 12.939255714416504,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 2420
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 6.99078893661499
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 8.372818946838379
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 6.726801872253418
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 6.3365325927734375
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 6.869709014892578
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 8.096755027770996
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 6.211286544799805
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 7.51231050491333
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 5.456270694732666
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 6.81317663192749
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 6.032557487487793
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 7.186517238616943
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 7.8737101554870605
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 5.552450180053711
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 7.468788146972656
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 4.995179653167725
    },
    {
      "epoch": 0.5255284552845528,
      "grad_norm": 11.986014366149902,
      "learning_rate": 1e-05,
      "loss": 6.781,
      "step": 2424
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 5.437894344329834
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 6.692652225494385
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 7.742513656616211
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 7.210651874542236
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 6.788521766662598
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 6.536921501159668
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 6.809709072113037
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 5.528759479522705
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 6.897302150726318
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 7.060944080352783
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 7.023475170135498
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 5.982514381408691
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 7.534923553466797
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 7.184730052947998
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 6.41294002532959
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 5.8518195152282715
    },
    {
      "epoch": 0.5263956639566396,
      "grad_norm": 13.639863967895508,
      "learning_rate": 1e-05,
      "loss": 6.6685,
      "step": 2428
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.399998188018799
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 7.917290210723877
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.2534379959106445
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.408863067626953
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 7.420772552490234
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 6.796796798706055
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 7.0313920974731445
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 5.322987079620361
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 7.962982177734375
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 6.272111892700195
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 7.089391708374023
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 6.269033908843994
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 5.607425212860107
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 5.540670394897461
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 6.861155986785889
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 7.9278411865234375
    },
    {
      "epoch": 0.5272628726287263,
      "grad_norm": 19.628311157226562,
      "learning_rate": 1e-05,
      "loss": 6.6926,
      "step": 2432
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 7.083944320678711
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 6.917276859283447
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 7.315186977386475
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 6.396486759185791
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 8.988991737365723
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 6.51439905166626
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 7.900964736938477
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 8.6137113571167
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 6.549357891082764
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 7.034558296203613
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 5.9020280838012695
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 8.207769393920898
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.9433979988098145
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 7.4801344871521
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.61876916885376
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 5.677324295043945
    },
    {
      "epoch": 0.528130081300813,
      "grad_norm": 12.848013877868652,
      "learning_rate": 1e-05,
      "loss": 7.134,
      "step": 2436
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 7.938337802886963
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 7.23502779006958
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 6.83475923538208
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 6.194612979888916
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 7.1477274894714355
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 7.077349662780762
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 6.1474409103393555
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 6.727540016174316
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 8.053845405578613
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 8.128211975097656
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 6.69074010848999
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 6.912012100219727
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 6.699732780456543
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 7.841773509979248
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 7.815237998962402
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 4.866086959838867
    },
    {
      "epoch": 0.5289972899728997,
      "grad_norm": 18.18678092956543,
      "learning_rate": 1e-05,
      "loss": 7.0194,
      "step": 2440
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 7.0086283683776855
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 7.058691501617432
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 6.932230472564697
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 5.107583045959473
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 7.06016206741333
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 7.520569801330566
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 7.067278861999512
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 6.858964443206787
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 6.904538154602051
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 4.3514933586120605
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 7.1959547996521
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 7.232011795043945
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 6.554439067840576
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 6.076656341552734
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 6.53115701675415
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 6.187021255493164
    },
    {
      "epoch": 0.5298644986449864,
      "grad_norm": 11.368574142456055,
      "learning_rate": 1e-05,
      "loss": 6.603,
      "step": 2444
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 4.014242172241211
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 7.678441047668457
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 6.878391742706299
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 6.911705017089844
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 5.833793640136719
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 4.5017476081848145
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 6.521193027496338
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 7.442843437194824
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 6.7934136390686035
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 7.54975700378418
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 6.697264194488525
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 7.426894664764404
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 7.871574401855469
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 6.181634426116943
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 6.833987712860107
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 7.357132911682129
    },
    {
      "epoch": 0.5307317073170732,
      "grad_norm": 15.287532806396484,
      "learning_rate": 1e-05,
      "loss": 6.6559,
      "step": 2448
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 7.144443511962891
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 7.878006935119629
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 7.116678237915039
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 7.1494340896606445
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 7.866942405700684
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 8.781288146972656
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 7.930176258087158
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 6.78520393371582
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 4.3459858894348145
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 4.749444007873535
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 7.730030059814453
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 6.69426965713501
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 7.306427478790283
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 7.012813091278076
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 6.077824592590332
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 6.976545333862305
    },
    {
      "epoch": 0.5315989159891599,
      "grad_norm": 13.392216682434082,
      "learning_rate": 1e-05,
      "loss": 6.9716,
      "step": 2452
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 5.699696063995361
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 4.4929304122924805
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 8.28738021850586
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 6.577730655670166
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 7.785135746002197
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 6.3340535163879395
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 7.101861953735352
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 5.8910136222839355
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 7.021543025970459
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 6.73483419418335
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 7.468082427978516
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 7.313547611236572
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 6.852609634399414
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 6.417643070220947
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 6.618753910064697
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 7.080657005310059
    },
    {
      "epoch": 0.5324661246612467,
      "grad_norm": 15.795018196105957,
      "learning_rate": 1e-05,
      "loss": 6.7298,
      "step": 2456
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 7.260860443115234
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 7.194522857666016
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 6.9906463623046875
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 7.733343601226807
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 5.4315290451049805
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 7.121959686279297
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 7.40520715713501
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 6.850764751434326
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 6.946733474731445
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 7.537847995758057
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 6.8804707527160645
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 9.718214988708496
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 7.129606246948242
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 6.560470104217529
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 6.214646816253662
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 7.474252700805664
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 13.915496826171875,
      "learning_rate": 1e-05,
      "loss": 7.1532,
      "step": 2460
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 7.508598327636719
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 3.556102991104126
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 6.932389259338379
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 7.88920259475708
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 7.134342670440674
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 7.306041717529297
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 9.032757759094238
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 7.418430328369141
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.969077110290527
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.327110290527344
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.176706790924072
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.095393657684326
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 6.839080333709717
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 5.951910495758057
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 6.6692280769348145
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 7.499122142791748
    },
    {
      "epoch": 0.5342005420054201,
      "grad_norm": 16.793663024902344,
      "learning_rate": 1e-05,
      "loss": 6.8316,
      "step": 2464
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 6.773930072784424
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 7.0721259117126465
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 5.998119831085205
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 5.761707782745361
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 6.909160137176514
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 8.433882713317871
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 8.054664611816406
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 8.516392707824707
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 6.945382118225098
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 6.447846412658691
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 7.48291015625
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 6.483306407928467
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 7.36765193939209
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 6.240920066833496
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 7.245281219482422
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 7.725675106048584
    },
    {
      "epoch": 0.5350677506775068,
      "grad_norm": 13.343648910522461,
      "learning_rate": 1e-05,
      "loss": 7.0912,
      "step": 2468
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 7.240360260009766
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 6.191751003265381
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 7.813450813293457
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 7.879220485687256
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 6.9301958084106445
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 6.897475719451904
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 6.610062599182129
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 7.442410469055176
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 7.225107669830322
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 5.885499954223633
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 6.511250972747803
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 6.818479537963867
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 6.878809452056885
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 5.153294563293457
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 6.540369510650635
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 6.569057941436768
    },
    {
      "epoch": 0.5359349593495935,
      "grad_norm": 15.76380729675293,
      "learning_rate": 1e-05,
      "loss": 6.7867,
      "step": 2472
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 4.365865707397461
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 6.777560234069824
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 6.5958991050720215
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 8.169933319091797
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 6.877084255218506
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 8.168804168701172
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 7.368995666503906
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 7.451392650604248
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 6.136964321136475
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 7.774837493896484
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 6.761420726776123
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 7.42940616607666
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 7.9674577713012695
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 6.846923351287842
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 7.2269487380981445
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 8.218396186828613
    },
    {
      "epoch": 0.5368021680216802,
      "grad_norm": 12.591771125793457,
      "learning_rate": 1e-05,
      "loss": 7.1336,
      "step": 2476
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 4.837515354156494
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 6.920172214508057
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 7.962255477905273
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 6.741028308868408
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.718209743499756
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.531782627105713
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.751915454864502
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 7.393528938293457
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 6.3084716796875
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 7.473792552947998
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 7.022979259490967
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 6.900627136230469
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 6.99552583694458
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 8.17287540435791
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 5.863039493560791
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 6.368597507476807
    },
    {
      "epoch": 0.5376693766937669,
      "grad_norm": 15.370247840881348,
      "learning_rate": 1e-05,
      "loss": 6.8101,
      "step": 2480
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 8.048583984375
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 8.108699798583984
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 7.43805456161499
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 8.612414360046387
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 6.81412935256958
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 7.271091938018799
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 8.165030479431152
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 6.441598415374756
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 5.58201265335083
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 6.605794906616211
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 6.7822723388671875
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 6.811229228973389
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 7.103426933288574
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 5.622258186340332
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 7.731564521789551
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 6.134995937347412
    },
    {
      "epoch": 0.5385365853658537,
      "grad_norm": 16.917434692382812,
      "learning_rate": 1e-05,
      "loss": 7.0796,
      "step": 2484
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 5.461259365081787
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 6.5870137214660645
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 8.1226167678833
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 7.102790832519531
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 6.665356636047363
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 7.282085418701172
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 5.935858726501465
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 6.604010105133057
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 7.38787841796875
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 6.612335681915283
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 6.790799140930176
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 7.255502223968506
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 7.571375370025635
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 6.879677772521973
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 7.628330707550049
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 7.425503253936768
    },
    {
      "epoch": 0.5394037940379404,
      "grad_norm": 11.545686721801758,
      "learning_rate": 1e-05,
      "loss": 6.957,
      "step": 2488
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 6.963079452514648
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 5.942166805267334
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 6.080007076263428
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 7.757519721984863
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 7.164015293121338
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 7.517567157745361
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 6.9145026206970215
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 7.21447229385376
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 6.507024765014648
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 7.621707916259766
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 6.892360687255859
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 5.810979843139648
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 5.6147027015686035
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 9.130226135253906
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 7.674529075622559
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 6.708571910858154
    },
    {
      "epoch": 0.5402710027100271,
      "grad_norm": 22.524965286254883,
      "learning_rate": 1e-05,
      "loss": 6.9696,
      "step": 2492
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 7.12578821182251
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 7.546735763549805
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 4.389494895935059
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 6.406828880310059
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 6.9806413650512695
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 5.069834232330322
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 7.368515968322754
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 6.530990123748779
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 7.235210418701172
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 6.173862457275391
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 6.409515857696533
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 7.52862024307251
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 6.694764614105225
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 6.445458889007568
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 6.271158695220947
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 6.904334545135498
    },
    {
      "epoch": 0.5411382113821138,
      "grad_norm": 10.293113708496094,
      "learning_rate": 1e-05,
      "loss": 6.5676,
      "step": 2496
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 7.513017654418945
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 7.025008678436279
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 6.523837089538574
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 6.88551139831543
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 6.400934219360352
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 4.999632358551025
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 7.9750075340271
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 5.397546768188477
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 7.2524847984313965
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 3.9405198097229004
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 6.331817626953125
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 7.466488361358643
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 4.836521625518799
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 6.466639995574951
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 6.134042263031006
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 6.4494171142578125
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 13.544988632202148,
      "learning_rate": 1e-05,
      "loss": 6.3499,
      "step": 2500
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 8.532745361328125
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 5.304664611816406
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 7.651978492736816
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 6.971622467041016
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 7.948321342468262
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 6.6701860427856445
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 7.961188793182373
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 6.116756439208984
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.934375762939453
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 7.306707859039307
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 7.369235992431641
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.485702037811279
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 8.172992706298828
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 4.019229888916016
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 4.598757743835449
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 6.9810991287231445
    },
    {
      "epoch": 0.5428726287262873,
      "grad_norm": 16.669858932495117,
      "learning_rate": 1e-05,
      "loss": 6.8141,
      "step": 2504
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 5.938580513000488
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 7.1315226554870605
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 7.351691722869873
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 6.51345157623291
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.468394756317139
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.375547409057617
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.418582916259766
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.984147071838379
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 6.540961742401123
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 6.101571559906006
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 6.81283712387085
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 5.883363246917725
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 6.793344497680664
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 6.558277606964111
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 8.763106346130371
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 4.891299247741699
    },
    {
      "epoch": 0.543739837398374,
      "grad_norm": 15.287964820861816,
      "learning_rate": 1e-05,
      "loss": 6.5954,
      "step": 2508
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 4.143923282623291
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 6.039316654205322
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 6.674912452697754
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 7.29310941696167
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 7.326047897338867
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 6.85995626449585
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 7.219657897949219
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 7.5714430809021
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 8.548563957214355
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 7.645691394805908
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 6.134606838226318
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 6.654989242553711
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 7.556931495666504
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 4.279922008514404
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 6.875153064727783
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 6.886478900909424
    },
    {
      "epoch": 0.5446070460704607,
      "grad_norm": 13.483332633972168,
      "learning_rate": 1e-05,
      "loss": 6.7319,
      "step": 2512
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 6.497110366821289
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 8.015923500061035
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 6.7371826171875
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 6.231503486633301
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 8.816370010375977
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 7.90797758102417
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 8.306740760803223
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 6.994856834411621
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 7.551991939544678
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 7.083251953125
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 7.808923244476318
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 6.679977893829346
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 6.47458028793335
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 7.3273091316223145
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 6.657009601593018
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 5.423541069030762
    },
    {
      "epoch": 0.5454742547425474,
      "grad_norm": 17.070049285888672,
      "learning_rate": 1e-05,
      "loss": 7.1571,
      "step": 2516
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.404959201812744
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.276968002319336
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 3.9802772998809814
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.916646480560303
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 5.721906661987305
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 5.519492149353027
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 5.471542835235596
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 6.060344696044922
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 5.792539119720459
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 6.954973220825195
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 9.900010108947754
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 7.058194637298584
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 6.699538230895996
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 6.161405086517334
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 3.9867501258850098
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 7.248271942138672
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 13.457584381103516,
      "learning_rate": 1e-05,
      "loss": 6.2596,
      "step": 2520
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.396470546722412
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.995380878448486
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.645310878753662
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.079126358032227
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 7.16264009475708
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 6.32706356048584
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 7.200082778930664
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 5.779906749725342
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 6.056241035461426
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 7.665191650390625
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 8.11622142791748
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 8.121788024902344
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.3920488357543945
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 6.474411487579346
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.826259136199951
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.566441535949707
    },
    {
      "epoch": 0.5472086720867209,
      "grad_norm": 12.499098777770996,
      "learning_rate": 1e-05,
      "loss": 6.9878,
      "step": 2524
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 6.331620693206787
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 7.02507209777832
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 5.988858222961426
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 4.657204627990723
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 7.851365089416504
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 6.325466632843018
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 7.142574787139893
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 8.043795585632324
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 6.457437515258789
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 6.898837566375732
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 7.020105838775635
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 7.512421131134033
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 7.558401107788086
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 7.939931869506836
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 7.150365352630615
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 4.6529154777526855
    },
    {
      "epoch": 0.5480758807588076,
      "grad_norm": 16.826656341552734,
      "learning_rate": 1e-05,
      "loss": 6.7848,
      "step": 2528
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 7.875909805297852
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 6.929440021514893
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 6.712395191192627
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 7.473876476287842
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 7.199470520019531
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 7.003085136413574
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 7.147890090942383
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 8.8252534866333
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 7.0903778076171875
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 7.188718795776367
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 6.241522312164307
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 7.711062908172607
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 7.796128273010254
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 6.941021919250488
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 6.867088794708252
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 6.5469746589660645
    },
    {
      "epoch": 0.5489430894308943,
      "grad_norm": 11.483977317810059,
      "learning_rate": 1e-05,
      "loss": 7.2219,
      "step": 2532
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 6.742570877075195
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 6.162842273712158
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 7.003839492797852
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 4.522860527038574
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 4.983786106109619
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 5.716475486755371
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 8.386821746826172
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 5.561156272888184
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 6.752914905548096
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 7.256999969482422
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 7.184110164642334
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 5.706597805023193
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 6.998507499694824
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 6.36704683303833
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 6.885847568511963
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 5.635056495666504
    },
    {
      "epoch": 0.549810298102981,
      "grad_norm": 18.19390869140625,
      "learning_rate": 1e-05,
      "loss": 6.3667,
      "step": 2536
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 6.10958194732666
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 6.9234938621521
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 7.551598072052002
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 6.713502407073975
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 6.999874591827393
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 6.661977291107178
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 6.839358806610107
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 7.325363636016846
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 7.146730899810791
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 7.496459484100342
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 6.358811378479004
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 7.015264511108398
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 6.108572959899902
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 5.076144695281982
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 6.744546413421631
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 7.136259078979492
    },
    {
      "epoch": 0.5506775067750678,
      "grad_norm": 13.379094123840332,
      "learning_rate": 1e-05,
      "loss": 6.763,
      "step": 2540
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 5.847532272338867
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 5.206791400909424
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 7.007941246032715
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 8.774832725524902
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 5.38153600692749
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 5.174110412597656
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 6.922085762023926
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 6.167379379272461
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 5.243165016174316
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 7.010087490081787
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 6.1587138175964355
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 9.300477027893066
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 6.995825290679932
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 6.5333991050720215
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 7.141554832458496
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 7.016788005828857
    },
    {
      "epoch": 0.5515447154471544,
      "grad_norm": 11.088616371154785,
      "learning_rate": 1e-05,
      "loss": 6.6176,
      "step": 2544
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 7.568017959594727
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 6.505282402038574
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 6.701223373413086
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 8.288617134094238
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 6.855654239654541
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 7.73919153213501
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 6.881076335906982
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 5.797654151916504
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 7.451242446899414
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 5.459603786468506
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 7.244643688201904
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 7.219178676605225
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 8.196516036987305
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 5.6782989501953125
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 6.784782409667969
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 7.196720600128174
    },
    {
      "epoch": 0.5524119241192412,
      "grad_norm": 18.462352752685547,
      "learning_rate": 1e-05,
      "loss": 6.973,
      "step": 2548
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 6.574878215789795
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 4.022956371307373
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 7.10847282409668
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 8.067240715026855
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 7.251014709472656
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 5.780139446258545
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 7.955130100250244
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 6.34650182723999
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 8.890569686889648
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 4.875661373138428
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 6.279875755310059
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 6.815792560577393
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 6.677149772644043
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 5.564062118530273
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 6.936290740966797
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 6.701573371887207
    },
    {
      "epoch": 0.5532791327913279,
      "grad_norm": 19.212474822998047,
      "learning_rate": 1e-05,
      "loss": 6.6155,
      "step": 2552
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 6.102055072784424
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 5.299739360809326
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 7.565845489501953
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 6.17680025100708
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 3.807201385498047
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 4.6665802001953125
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 6.242334842681885
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 6.342941761016846
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 5.799614429473877
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 7.201122283935547
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 7.138839244842529
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 6.64125394821167
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 6.950610160827637
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 5.447022438049316
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 6.409637451171875
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 6.671414852142334
    },
    {
      "epoch": 0.5541463414634147,
      "grad_norm": 13.032567024230957,
      "learning_rate": 1e-05,
      "loss": 6.1539,
      "step": 2556
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 6.754211902618408
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 7.268423080444336
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 7.565616130828857
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 7.193093776702881
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 5.91738748550415
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 8.257341384887695
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 6.558370590209961
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 7.292587757110596
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 7.2487688064575195
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 4.594001293182373
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 7.690035820007324
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 6.54148006439209
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 7.009345054626465
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 6.527998447418213
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 7.144129753112793
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 7.225623607635498
    },
    {
      "epoch": 0.5550135501355014,
      "grad_norm": 11.729612350463867,
      "learning_rate": 1e-05,
      "loss": 6.9243,
      "step": 2560
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 6.482957363128662
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 7.462703227996826
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 5.283533573150635
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 7.467997074127197
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 6.972052097320557
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 5.68209171295166
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 6.5577263832092285
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 7.6152215003967285
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 7.07029914855957
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 6.9341959953308105
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 7.867372989654541
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 7.020002841949463
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 6.9777398109436035
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 6.735380172729492
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 6.153354644775391
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 5.5607733726501465
    },
    {
      "epoch": 0.555880758807588,
      "grad_norm": 16.005449295043945,
      "learning_rate": 1e-05,
      "loss": 6.7402,
      "step": 2564
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 6.796158313751221
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 6.886162281036377
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 6.774493217468262
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 7.470089912414551
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 6.3800578117370605
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 7.2108659744262695
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 7.202149391174316
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 7.456575870513916
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 5.6760101318359375
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 6.345170021057129
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 7.2251434326171875
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 8.864603042602539
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 6.871863842010498
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 7.845638275146484
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 7.146626949310303
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 7.132126331329346
    },
    {
      "epoch": 0.5567479674796748,
      "grad_norm": 10.992621421813965,
      "learning_rate": 1e-05,
      "loss": 7.0802,
      "step": 2568
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 7.232364654541016
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 6.619579315185547
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 6.657811641693115
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 8.147526741027832
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 7.063638687133789
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 5.638059139251709
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 7.35650110244751
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 7.2106804847717285
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 6.858739376068115
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 6.950416564941406
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 6.498934268951416
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 5.806047439575195
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 6.763399600982666
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 6.862873077392578
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 4.221857070922852
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 5.564768314361572
    },
    {
      "epoch": 0.5576151761517615,
      "grad_norm": 11.122727394104004,
      "learning_rate": 1e-05,
      "loss": 6.5908,
      "step": 2572
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 6.117737293243408
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 7.476150989532471
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 9.622369766235352
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 6.691857814788818
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.237037181854248
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.132155418395996
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.424325466156006
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.442827224731445
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 8.675704956054688
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 8.3821382522583
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 6.403572082519531
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 6.049290180206299
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 4.286039352416992
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 7.402305603027344
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 5.944771766662598
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 7.576328754425049
    },
    {
      "epoch": 0.5584823848238483,
      "grad_norm": 15.944034576416016,
      "learning_rate": 1e-05,
      "loss": 7.1165,
      "step": 2576
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 7.324495315551758
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 5.7974324226379395
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 5.952941417694092
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 7.67708158493042
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 6.881067752838135
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 6.02055025100708
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 6.115456581115723
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 7.352178573608398
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 7.934492588043213
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 7.859492778778076
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 8.129590034484863
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 6.590789318084717
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 5.5691986083984375
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 6.3885602951049805
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 7.317741394042969
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 6.372150897979736
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 11.74239444732666,
      "learning_rate": 1e-05,
      "loss": 6.8302,
      "step": 2580
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 6.992717266082764
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 6.920529842376709
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 6.946736812591553
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 8.200223922729492
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 7.716615200042725
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 8.339844703674316
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 7.383028984069824
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 4.678009510040283
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 7.45304536819458
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 6.139623641967773
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 7.089859962463379
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 6.192234992980957
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 3.8995227813720703
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 8.42182445526123
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 7.748758316040039
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 6.225068092346191
    },
    {
      "epoch": 0.5602168021680217,
      "grad_norm": 13.185800552368164,
      "learning_rate": 1e-05,
      "loss": 6.8967,
      "step": 2584
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 6.000364780426025
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 8.490283012390137
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 5.743098735809326
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 7.40009069442749
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 7.107499122619629
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 6.3950910568237305
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 9.612953186035156
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 6.005245208740234
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 6.746977806091309
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 6.750187397003174
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 6.761855125427246
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 6.822081089019775
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 7.332063674926758
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.612057685852051
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.7675371170043945
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.914970874786377
    },
    {
      "epoch": 0.5610840108401084,
      "grad_norm": 13.114336013793945,
      "learning_rate": 1e-05,
      "loss": 6.9664,
      "step": 2588
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 6.586763858795166
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 8.03111457824707
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 6.384706020355225
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 6.455321311950684
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 7.303367614746094
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 6.93585205078125
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 5.180548667907715
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 5.265237808227539
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 9.203740119934082
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 6.7568206787109375
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 6.7679443359375
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 6.748363018035889
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 8.01398754119873
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 6.327448844909668
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 4.854601860046387
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 6.891815662384033
    },
    {
      "epoch": 0.5619512195121952,
      "grad_norm": 12.988688468933105,
      "learning_rate": 1e-05,
      "loss": 6.7317,
      "step": 2592
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 6.843759536743164
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 6.6850738525390625
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 5.348296165466309
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 6.313221454620361
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 7.628566741943359
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 6.067473411560059
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 6.539026737213135
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 7.665948390960693
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 6.983269691467285
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 6.2196455001831055
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 7.435543060302734
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 5.580425262451172
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 7.696589946746826
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 6.318207740783691
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 6.908423900604248
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 7.422609806060791
    },
    {
      "epoch": 0.5628184281842818,
      "grad_norm": 12.92993450164795,
      "learning_rate": 1e-05,
      "loss": 6.7285,
      "step": 2596
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 6.837175369262695
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 6.643586158752441
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 5.7759504318237305
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 8.140027046203613
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 6.681821346282959
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 6.996463775634766
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 9.101299285888672
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 5.813668251037598
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 6.725986480712891
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 6.330691337585449
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 5.9327168464660645
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 6.836489677429199
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 6.496217727661133
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 6.500024318695068
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 7.868740558624268
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 5.832535743713379
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 14.591507911682129,
      "learning_rate": 1e-05,
      "loss": 6.7821,
      "step": 2600
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 5.7923665046691895
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 6.074372291564941
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 7.568171501159668
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 7.450918197631836
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 7.86412239074707
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 4.582707405090332
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 5.82958984375
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 8.266617774963379
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 6.883486270904541
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 7.543364524841309
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 6.3052167892456055
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 6.915915489196777
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 6.565924167633057
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 6.689501762390137
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 7.900576114654541
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 4.854907035827637
    },
    {
      "epoch": 0.5645528455284553,
      "grad_norm": 13.214313507080078,
      "learning_rate": 1e-05,
      "loss": 6.693,
      "step": 2604
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 7.0468010902404785
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 7.525877952575684
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 6.921959400177002
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 6.621101379394531
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 6.097307205200195
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 7.792330741882324
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 7.421483516693115
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 4.321192741394043
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 5.255024433135986
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 6.936152935028076
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 7.582555294036865
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 7.085230827331543
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 5.690314292907715
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 8.32679271697998
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 6.329614162445068
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 6.980528831481934
    },
    {
      "epoch": 0.565420054200542,
      "grad_norm": 14.210590362548828,
      "learning_rate": 1e-05,
      "loss": 6.7459,
      "step": 2608
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 6.4505839347839355
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 7.70046329498291
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 6.065917491912842
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 6.943802833557129
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 7.237803936004639
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 6.389780044555664
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 6.620105743408203
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 6.889685153961182
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 7.207498550415039
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 4.488947868347168
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 7.314403057098389
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 6.228845119476318
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.349399089813232
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.214034080505371
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.620457649230957
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.857936859130859
    },
    {
      "epoch": 0.5662872628726288,
      "grad_norm": 11.038415908813477,
      "learning_rate": 1e-05,
      "loss": 6.5987,
      "step": 2612
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 7.306093692779541
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 6.072344779968262
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 5.473689079284668
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 7.694631099700928
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 5.456060409545898
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 6.582525253295898
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 7.963072776794434
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 7.36660623550415
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 6.7329840660095215
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 7.356500148773193
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 6.737441539764404
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 6.507088661193848
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 7.5660080909729
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 8.459596633911133
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 7.25199556350708
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 6.1839165687561035
    },
    {
      "epoch": 0.5671544715447154,
      "grad_norm": 12.683375358581543,
      "learning_rate": 1e-05,
      "loss": 6.9194,
      "step": 2616
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 7.051875591278076
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 6.626967906951904
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 5.914597988128662
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 7.770364761352539
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 7.507328987121582
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 6.447431564331055
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 6.964929103851318
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 7.562230110168457
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 6.587918758392334
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 7.308321475982666
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 6.464033126831055
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 7.494600772857666
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 7.883602619171143
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 7.354536533355713
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 5.389237403869629
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 6.211935043334961
    },
    {
      "epoch": 0.5680216802168022,
      "grad_norm": 20.191242218017578,
      "learning_rate": 1e-05,
      "loss": 6.9087,
      "step": 2620
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 7.771080017089844
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 6.877408027648926
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 7.1241455078125
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 6.439640045166016
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 7.406736373901367
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 7.947206974029541
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 6.942745685577393
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 4.866589546203613
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 5.263969421386719
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 5.720200538635254
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 6.963551044464111
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 7.233647346496582
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 7.669041156768799
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 7.005852222442627
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 6.805436611175537
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 5.825883388519287
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 13.585906982421875,
      "learning_rate": 1e-05,
      "loss": 6.7414,
      "step": 2624
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 8.138480186462402
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 7.385007858276367
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 6.853738307952881
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 6.105648517608643
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 6.205366611480713
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 3.92502498626709
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 4.95751428604126
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 6.075480937957764
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 7.370704174041748
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 5.209940433502197
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 5.969809055328369
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 6.818267822265625
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 6.416699409484863
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 5.752986431121826
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 6.460064888000488
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 8.06265926361084
    },
    {
      "epoch": 0.5697560975609756,
      "grad_norm": 12.081850051879883,
      "learning_rate": 1e-05,
      "loss": 6.3567,
      "step": 2628
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 7.674101829528809
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 5.614731788635254
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 6.911001205444336
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 7.116444110870361
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 6.98879337310791
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 6.067829132080078
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 6.524423122406006
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 5.3691606521606445
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 7.102553367614746
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 4.067502498626709
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 7.083705425262451
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 6.361634254455566
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 7.6252121925354
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 10.17773723602295
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 6.447465896606445
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 7.325313568115234
    },
    {
      "epoch": 0.5706233062330623,
      "grad_norm": 17.486242294311523,
      "learning_rate": 1e-05,
      "loss": 6.7786,
      "step": 2632
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 7.562530994415283
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 7.575342655181885
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 7.438262939453125
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 5.790158748626709
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 8.230573654174805
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 8.658806800842285
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 8.126081466674805
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 7.861932277679443
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 7.882976531982422
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 4.7592082023620605
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 6.519501209259033
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 7.424707889556885
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 7.530113697052002
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 7.139223575592041
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 6.777020454406738
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 7.501779556274414
    },
    {
      "epoch": 0.571490514905149,
      "grad_norm": 12.606573104858398,
      "learning_rate": 1e-05,
      "loss": 7.2986,
      "step": 2636
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 6.2973809242248535
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 5.590480804443359
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 7.928467273712158
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 7.195540428161621
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 6.430563926696777
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 6.995749473571777
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 7.54747200012207
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 7.737678527832031
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 7.271257400512695
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 7.193896770477295
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 5.413910865783691
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 7.123183250427246
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 7.619342803955078
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 7.36863374710083
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 5.821406364440918
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 5.822525501251221
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 13.136943817138672,
      "learning_rate": 1e-05,
      "loss": 6.8348,
      "step": 2640
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 7.181126117706299
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 8.110008239746094
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 6.541685104370117
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 5.2397894859313965
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 8.456426620483398
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 7.094184875488281
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 6.030412197113037
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 5.91464900970459
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 7.2403082847595215
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 5.391489028930664
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 5.70858907699585
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 4.016270637512207
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 7.775526523590088
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 6.172055721282959
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 6.104434490203857
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 6.345385551452637
    },
    {
      "epoch": 0.5732249322493225,
      "grad_norm": 14.23491096496582,
      "learning_rate": 1e-05,
      "loss": 6.4576,
      "step": 2644
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 6.833125114440918
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 6.4827704429626465
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 6.871211528778076
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 6.706439971923828
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 7.39152717590332
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 5.112117290496826
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.645931720733643
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.702192783355713
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 5.197881698608398
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 6.412862300872803
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 7.43690824508667
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 7.185885906219482
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 5.760196685791016
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 7.03022575378418
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 6.68037223815918
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 5.271360397338867
    },
    {
      "epoch": 0.5740921409214093,
      "grad_norm": 15.796727180480957,
      "learning_rate": 1e-05,
      "loss": 6.4826,
      "step": 2648
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 7.70299768447876
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 7.391146183013916
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 5.997473239898682
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 7.115784168243408
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 5.708676338195801
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 6.622997283935547
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 6.305770397186279
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 5.742011070251465
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 5.523279190063477
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 6.418160915374756
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 4.7596025466918945
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 6.10585880279541
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 6.642151355743408
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 7.037773132324219
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 6.564314365386963
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 6.689505100250244
    },
    {
      "epoch": 0.5749593495934959,
      "grad_norm": 13.844066619873047,
      "learning_rate": 1e-05,
      "loss": 6.3955,
      "step": 2652
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 7.338241100311279
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 8.139178276062012
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 5.273636817932129
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 8.509052276611328
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.81222677230835
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.592153072357178
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 7.2185163497924805
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.899820327758789
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 8.348506927490234
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 6.936673164367676
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 8.495401382446289
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 6.435667514801025
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 6.306102275848389
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 5.377802848815918
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 6.406447887420654
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 7.9525227546691895
    },
    {
      "epoch": 0.5758265582655827,
      "grad_norm": 13.102521896362305,
      "learning_rate": 1e-05,
      "loss": 7.0651,
      "step": 2656
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 7.1458258628845215
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 6.313562870025635
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 6.913889408111572
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 8.22061824798584
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 7.257213115692139
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 4.316123962402344
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 5.683572769165039
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 7.502379894256592
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 6.344959735870361
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 7.262664794921875
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 6.4007439613342285
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 5.2942938804626465
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 7.434247016906738
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 7.292350769042969
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 5.934926986694336
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 6.547408580780029
    },
    {
      "epoch": 0.5766937669376694,
      "grad_norm": 16.779958724975586,
      "learning_rate": 1e-05,
      "loss": 6.6165,
      "step": 2660
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 7.39116096496582
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 7.950102806091309
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 7.5201897621154785
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 8.054340362548828
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 6.88700532913208
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 7.617595672607422
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 5.551273345947266
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 7.437283515930176
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 6.871984958648682
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 5.094224452972412
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 5.70767068862915
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 7.8578410148620605
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 8.146110534667969
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 6.2638092041015625
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 6.069100379943848
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 6.780433177947998
    },
    {
      "epoch": 0.577560975609756,
      "grad_norm": 11.936871528625488,
      "learning_rate": 1e-05,
      "loss": 6.95,
      "step": 2664
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 5.405538082122803
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 8.47711181640625
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 7.259026050567627
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 4.361222743988037
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.3249640464782715
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.435267925262451
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.54908561706543
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.075408458709717
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 6.066045761108398
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 6.636785507202148
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 7.189327716827393
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 7.054385185241699
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 7.583354949951172
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 3.9168386459350586
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 7.1666693687438965
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 4.245269298553467
    },
    {
      "epoch": 0.5784281842818428,
      "grad_norm": 18.55366325378418,
      "learning_rate": 1e-05,
      "loss": 6.5466,
      "step": 2668
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 7.643275737762451
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 7.663346767425537
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 6.829620838165283
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 4.618216514587402
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 7.959160327911377
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 6.586524486541748
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 8.351834297180176
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 6.799597263336182
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 6.923388481140137
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 7.1128644943237305
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 7.2991862297058105
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 9.225201606750488
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 6.9419074058532715
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 6.047268390655518
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 6.218955039978027
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 6.961989879608154
    },
    {
      "epoch": 0.5792953929539295,
      "grad_norm": 11.665580749511719,
      "learning_rate": 1e-05,
      "loss": 7.0739,
      "step": 2672
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 6.6624579429626465
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 7.409172058105469
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 7.14764404296875
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 6.910321235656738
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 6.30883264541626
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 7.130339622497559
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 5.859875202178955
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 5.7451066970825195
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 7.768468856811523
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 3.929513692855835
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 7.281825542449951
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 7.70815896987915
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 7.6193437576293945
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 6.119208335876465
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 5.956048965454102
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 5.781623363494873
    },
    {
      "epoch": 0.5801626016260163,
      "grad_norm": 14.779648780822754,
      "learning_rate": 1e-05,
      "loss": 6.5836,
      "step": 2676
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 6.422306060791016
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 5.798417568206787
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 7.250713348388672
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 6.830315589904785
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 5.670312404632568
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 6.8742995262146
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 6.476125240325928
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 5.953288555145264
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 6.294435977935791
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 6.132711410522461
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 7.070828914642334
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 6.252846717834473
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 4.827894687652588
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 7.189681053161621
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 7.558030605316162
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 6.2602643966674805
    },
    {
      "epoch": 0.581029810298103,
      "grad_norm": 11.093317985534668,
      "learning_rate": 1e-05,
      "loss": 6.4289,
      "step": 2680
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 7.747509002685547
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 7.394124507904053
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 6.925696849822998
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 7.929521560668945
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 7.002941131591797
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 7.673361778259277
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 5.418943405151367
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 6.643513202667236
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 6.187310218811035
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 7.186299800872803
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 7.4943342208862305
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 7.3695759773254395
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 7.27278470993042
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 6.735597133636475
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 7.5964860916137695
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 7.571744441986084
    },
    {
      "epoch": 0.5818970189701897,
      "grad_norm": 11.309617042541504,
      "learning_rate": 1e-05,
      "loss": 7.1344,
      "step": 2684
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 6.611952781677246
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 7.610063552856445
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 5.938325881958008
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 6.424335956573486
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 8.81508731842041
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 7.245547771453857
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 6.6006975173950195
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 8.131169319152832
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 7.2333478927612305
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 6.679961681365967
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 6.6342315673828125
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 5.8353495597839355
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 6.218982696533203
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 5.190592288970947
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 7.2469587326049805
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 6.579869747161865
    },
    {
      "epoch": 0.5827642276422764,
      "grad_norm": 12.912064552307129,
      "learning_rate": 1e-05,
      "loss": 6.8123,
      "step": 2688
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 5.849403381347656
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 7.168865203857422
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 6.837403774261475
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 6.870368957519531
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 7.10115385055542
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 6.279381275177002
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 7.35746431350708
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 6.740997314453125
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 7.152923583984375
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 9.08797550201416
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 7.768004894256592
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 5.843694686889648
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 6.520078182220459
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 7.8751020431518555
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 6.892945289611816
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 7.711760997772217
    },
    {
      "epoch": 0.5836314363143631,
      "grad_norm": 17.183406829833984,
      "learning_rate": 1e-05,
      "loss": 7.0661,
      "step": 2692
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 8.43532943725586
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 7.0424933433532715
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 7.536916732788086
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 6.466437339782715
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 7.298920631408691
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 7.217277526855469
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 8.330819129943848
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 7.006754398345947
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.9814276695251465
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.960936069488525
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 5.597355842590332
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.903204441070557
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 6.862250804901123
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 6.5616455078125
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 4.78732967376709
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 6.843844890594482
    },
    {
      "epoch": 0.5844986449864499,
      "grad_norm": 11.477710723876953,
      "learning_rate": 1e-05,
      "loss": 6.9271,
      "step": 2696
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 7.28794002532959
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 7.001369476318359
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 6.7260026931762695
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 7.239980220794678
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 4.397267818450928
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 5.976212024688721
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 7.167693138122559
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 7.142507553100586
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 7.1849141120910645
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 7.520232677459717
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 5.312763690948486
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 7.1698317527771
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 7.349431991577148
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 5.8632917404174805
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 6.878393173217773
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 6.133314609527588
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 12.70582103729248,
      "learning_rate": 1e-05,
      "loss": 6.6469,
      "step": 2700
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 7.069969654083252
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 5.70466947555542
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 7.074642181396484
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 6.657009124755859
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 9.374706268310547
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 5.871863842010498
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 8.745017051696777
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 7.134850978851318
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 7.062089443206787
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 7.489536762237549
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 6.491825103759766
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 5.153962135314941
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 6.168608665466309
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 9.177972793579102
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 6.926827430725098
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 8.835172653198242
    },
    {
      "epoch": 0.5862330623306233,
      "grad_norm": 24.635944366455078,
      "learning_rate": 1e-05,
      "loss": 7.1837,
      "step": 2704
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 7.068771839141846
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 7.983659267425537
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 7.254602909088135
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 6.576722145080566
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 7.045180320739746
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 5.368602275848389
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 6.7281646728515625
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 6.177109718322754
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 7.97261381149292
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 6.086987495422363
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 7.390210151672363
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 6.009658336639404
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 5.668238162994385
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 5.330382823944092
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 6.729133605957031
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 7.475019454956055
    },
    {
      "epoch": 0.58710027100271,
      "grad_norm": 14.477837562561035,
      "learning_rate": 1e-05,
      "loss": 6.6791,
      "step": 2708
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 6.190571308135986
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 6.681344032287598
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 7.767879962921143
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 5.564445972442627
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 6.838268756866455
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 5.58997106552124
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 4.832516193389893
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 4.274801254272461
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 6.535280227661133
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 7.1078314781188965
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 6.506119728088379
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 6.133756160736084
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 7.081151485443115
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 8.682881355285645
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 6.564382553100586
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 7.013913154602051
    },
    {
      "epoch": 0.5879674796747968,
      "grad_norm": 14.124814987182617,
      "learning_rate": 1e-05,
      "loss": 6.4603,
      "step": 2712
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 6.433375358581543
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 5.978738784790039
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 6.134820461273193
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 7.601375102996826
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 6.909858703613281
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 7.920261859893799
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 6.208246231079102
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 6.814804553985596
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 6.438681602478027
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 5.505489349365234
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 7.037020683288574
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 7.000452518463135
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 8.12570571899414
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 6.920674800872803
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 4.645916938781738
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 6.845712661743164
    },
    {
      "epoch": 0.5888346883468835,
      "grad_norm": 13.391791343688965,
      "learning_rate": 1e-05,
      "loss": 6.6576,
      "step": 2716
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 5.963011741638184
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 6.257412910461426
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 7.585594654083252
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 7.126036167144775
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 8.08092975616455
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 6.608659744262695
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 6.042222023010254
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 6.194077968597412
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 7.811446189880371
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 7.733785152435303
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 7.19315767288208
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 7.269253253936768
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 6.445195198059082
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 7.003841876983643
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 7.569980621337891
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 7.724974155426025
    },
    {
      "epoch": 0.5897018970189702,
      "grad_norm": 22.732545852661133,
      "learning_rate": 1e-05,
      "loss": 7.0381,
      "step": 2720
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 6.538879871368408
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 6.935969829559326
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 6.142284870147705
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 5.47101354598999
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 6.558901309967041
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 6.386412143707275
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 6.887387275695801
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 7.24896764755249
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 6.982645034790039
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 7.357235908508301
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 7.590891361236572
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 6.522129058837891
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 7.092350482940674
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 7.599579811096191
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 6.9892778396606445
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 6.81553840637207
    },
    {
      "epoch": 0.5905691056910569,
      "grad_norm": 15.368236541748047,
      "learning_rate": 1e-05,
      "loss": 6.82,
      "step": 2724
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 7.323848247528076
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 7.003019332885742
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 6.144002914428711
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 7.074228286743164
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 7.311824321746826
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 6.02895975112915
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 6.585095405578613
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 7.465806007385254
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 5.885665416717529
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 7.389727592468262
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 6.810591697692871
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 6.922740936279297
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.195470333099365
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.569821834564209
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.441388130187988
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.424943923950195
    },
    {
      "epoch": 0.5914363143631436,
      "grad_norm": 12.15636157989502,
      "learning_rate": 1e-05,
      "loss": 6.9736,
      "step": 2728
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 8.040456771850586
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 7.768303871154785
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 6.607892036437988
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 4.810418128967285
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 6.152891159057617
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 11.651639938354492
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 7.303497314453125
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 4.258163928985596
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 6.518109321594238
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 5.094200611114502
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 6.067142963409424
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 8.660810470581055
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 8.004963874816895
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 7.042863845825195
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 6.6305952072143555
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 7.214416027069092
    },
    {
      "epoch": 0.5923035230352304,
      "grad_norm": 13.672507286071777,
      "learning_rate": 1e-05,
      "loss": 6.9891,
      "step": 2732
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 6.79099702835083
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 7.569601058959961
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 6.522891044616699
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 7.2845048904418945
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 6.684748649597168
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 5.845386981964111
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 9.859740257263184
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 6.575052261352539
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 8.673927307128906
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 4.544957160949707
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 7.208695411682129
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 5.951126575469971
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 6.708109378814697
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 5.053747177124023
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 5.3775739669799805
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 6.759092807769775
    },
    {
      "epoch": 0.593170731707317,
      "grad_norm": 13.681923866271973,
      "learning_rate": 1e-05,
      "loss": 6.7131,
      "step": 2736
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 7.763156414031982
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 8.729251861572266
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 7.0245819091796875
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 6.259541034698486
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 7.578550338745117
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 7.66604471206665
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 6.332540988922119
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 4.321906089782715
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 7.0687127113342285
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 6.631009578704834
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 7.554293632507324
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 6.849733352661133
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 7.282987117767334
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 6.470136642456055
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 6.507823944091797
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 9.194665908813477
    },
    {
      "epoch": 0.5940379403794038,
      "grad_norm": 17.545597076416016,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 2740
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 7.059177398681641
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 6.9850969314575195
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 6.127612113952637
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 8.75497055053711
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 5.368904113769531
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 7.430932998657227
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 7.855835437774658
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 6.305690765380859
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 6.07427978515625
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 6.819847583770752
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 7.413909912109375
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 6.670342922210693
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 7.288906574249268
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 4.116655349731445
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 6.485167503356934
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 7.311624526977539
    },
    {
      "epoch": 0.5949051490514905,
      "grad_norm": 11.671095848083496,
      "learning_rate": 1e-05,
      "loss": 6.7543,
      "step": 2744
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 6.821426868438721
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 7.358695030212402
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 7.536681652069092
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 7.03140926361084
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 5.578345775604248
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 6.9985175132751465
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 7.99391508102417
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 6.435873985290527
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 6.085859775543213
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 7.883391380310059
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 6.265691757202148
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 6.806731224060059
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 7.923442363739014
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 7.551684379577637
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 7.7216267585754395
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 6.146387100219727
    },
    {
      "epoch": 0.5957723577235773,
      "grad_norm": 18.778709411621094,
      "learning_rate": 1e-05,
      "loss": 7.0087,
      "step": 2748
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 6.73408842086792
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 7.455312728881836
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 7.31199836730957
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 6.6763176918029785
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 6.081447124481201
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 6.526583671569824
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 4.250044822692871
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 7.163040637969971
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 7.051985740661621
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 6.978405952453613
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 6.451267719268799
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 6.154803276062012
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.996577739715576
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.099331855773926
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.369486331939697
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 4.9738993644714355
    },
    {
      "epoch": 0.596639566395664,
      "grad_norm": 12.932366371154785,
      "learning_rate": 1e-05,
      "loss": 6.4547,
      "step": 2752
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 7.736764907836914
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 6.651859760284424
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 5.1172590255737305
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 7.0563883781433105
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 6.736300468444824
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 7.246599197387695
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 6.0994062423706055
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 6.266921520233154
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 7.1460418701171875
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 5.677978992462158
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 8.441421508789062
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 4.507286071777344
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 7.025030612945557
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 8.92790699005127
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 5.764771461486816
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 6.57283878326416
    },
    {
      "epoch": 0.5975067750677506,
      "grad_norm": 16.507944107055664,
      "learning_rate": 1e-05,
      "loss": 6.6859,
      "step": 2756
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 7.93856954574585
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 6.059890270233154
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 5.467400550842285
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 7.012340068817139
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 6.707741737365723
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 7.78359842300415
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 7.794182300567627
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 6.251201152801514
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 6.891017436981201
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 7.000006198883057
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 7.952150821685791
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 7.630238056182861
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 5.738650798797607
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 6.344920635223389
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 5.525362968444824
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 7.289305686950684
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 16.11041259765625,
      "learning_rate": 1e-05,
      "loss": 6.8367,
      "step": 2760
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.906590461730957
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.994279861450195
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.208801746368408
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.705595016479492
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 7.17513370513916
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 7.075998306274414
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 5.151403427124023
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 6.225897789001465
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 6.990653038024902
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 6.8932905197143555
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 6.960098743438721
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 5.942968368530273
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 6.683229446411133
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 6.977936744689941
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 8.396683692932129
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 6.210997581481934
    },
    {
      "epoch": 0.5992411924119241,
      "grad_norm": 12.521507263183594,
      "learning_rate": 1e-05,
      "loss": 6.7187,
      "step": 2764
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 6.53240442276001
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 7.82283353805542
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 6.573128700256348
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 6.593563556671143
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 7.00413703918457
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 5.872281551361084
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 6.102499485015869
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 8.035576820373535
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 6.477007865905762
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 6.239253997802734
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 7.233392715454102
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 6.789780139923096
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 7.1632208824157715
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 6.858715534210205
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 7.60198974609375
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 8.18160343170166
    },
    {
      "epoch": 0.6001084010840109,
      "grad_norm": 17.077299118041992,
      "learning_rate": 1e-05,
      "loss": 6.9426,
      "step": 2768
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 6.3953938484191895
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 7.506507396697998
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 6.4398908615112305
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 3.869847297668457
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 7.382387161254883
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 6.972245216369629
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 6.286404609680176
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 7.248781681060791
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 8.100844383239746
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 6.719971179962158
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 7.9565534591674805
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 6.081911087036133
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 7.861771106719971
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 6.658750534057617
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 6.9675397872924805
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 5.431984901428223
    },
    {
      "epoch": 0.6009756097560975,
      "grad_norm": 14.679285049438477,
      "learning_rate": 1e-05,
      "loss": 6.7425,
      "step": 2772
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 5.653171062469482
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 6.78046178817749
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 7.011384010314941
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 6.9837565422058105
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 7.482391834259033
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 5.344732284545898
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 7.9148359298706055
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 8.277307510375977
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 7.188758373260498
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 7.130026817321777
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 5.664750099182129
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 7.013178825378418
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 7.642830848693848
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.773437976837158
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.560581684112549
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.239471912384033
    },
    {
      "epoch": 0.6018428184281843,
      "grad_norm": 13.40522575378418,
      "learning_rate": 1e-05,
      "loss": 6.8538,
      "step": 2776
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 8.31590747833252
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 6.513503551483154
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 5.470594882965088
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 6.945920944213867
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 5.232031345367432
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 8.157064437866211
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 8.375960350036621
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 6.155942916870117
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 6.340304851531982
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 5.659747123718262
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 7.881533145904541
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 6.439428806304932
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 7.715180397033691
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 6.964210510253906
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 6.653205394744873
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 6.787538051605225
    },
    {
      "epoch": 0.602710027100271,
      "grad_norm": 11.473526954650879,
      "learning_rate": 1e-05,
      "loss": 6.8505,
      "step": 2780
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 7.197994709014893
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 6.803005695343018
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 6.187440395355225
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 5.623316764831543
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 7.2706146240234375
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 6.907918930053711
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 5.624423980712891
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 5.6225810050964355
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 7.756911754608154
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 7.133339881896973
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 5.306058883666992
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 5.766624927520752
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 6.198990821838379
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 6.942760944366455
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 6.862424850463867
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 7.158855438232422
    },
    {
      "epoch": 0.6035772357723578,
      "grad_norm": 17.91686248779297,
      "learning_rate": 1e-05,
      "loss": 6.5227,
      "step": 2784
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 7.526193618774414
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 7.250904560089111
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 4.0329909324646
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 7.571736812591553
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 7.437852382659912
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 7.766620635986328
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 4.992190361022949
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 7.517255783081055
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 7.040566444396973
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 6.549391746520996
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 6.695101737976074
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 4.668261528015137
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.909438610076904
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.527280330657959
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 6.554577350616455
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.471611499786377
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 14.561660766601562,
      "learning_rate": 1e-05,
      "loss": 6.782,
      "step": 2788
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 7.5018744468688965
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 6.09201192855835
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 8.811042785644531
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 6.962122917175293
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 7.651113510131836
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 6.718848705291748
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 6.921694278717041
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 5.504245758056641
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.7963361740112305
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.556214332580566
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.394689083099365
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.8514251708984375
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 6.7745137214660645
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 6.81658935546875
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 6.398123741149902
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 5.638293266296387
    },
    {
      "epoch": 0.6053116531165311,
      "grad_norm": 17.217248916625977,
      "learning_rate": 1e-05,
      "loss": 6.7743,
      "step": 2792
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 6.715752601623535
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 7.162240028381348
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 7.2863593101501465
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 7.7039031982421875
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 4.507936477661133
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 6.056427001953125
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 6.764255523681641
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 6.211697101593018
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 7.134396076202393
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 7.19810676574707
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 6.151974678039551
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 6.744631767272949
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.4462456703186035
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.4885759353637695
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.361165523529053
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 7.119998931884766
    },
    {
      "epoch": 0.6061788617886179,
      "grad_norm": 10.007362365722656,
      "learning_rate": 1e-05,
      "loss": 6.6284,
      "step": 2796
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 7.369802951812744
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 4.222117900848389
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 6.8780364990234375
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 8.118918418884277
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 5.5456929206848145
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 6.17143440246582
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 8.42697811126709
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 7.360955715179443
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 7.16552209854126
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 7.12426233291626
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 6.950270652770996
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 6.6697845458984375
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 7.716526508331299
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 6.707237243652344
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 6.033125877380371
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 6.099613666534424
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 12.4301176071167,
      "learning_rate": 1e-05,
      "loss": 6.785,
      "step": 2800
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 9.291275978088379
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 6.967255592346191
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 7.662477970123291
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 7.571620941162109
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 6.207897663116455
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 6.506217956542969
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 6.762682914733887
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 5.7434983253479
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 9.08408260345459
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 6.290308475494385
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 6.520408630371094
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 8.228062629699707
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 7.982557773590088
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 8.154646873474121
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 6.393990993499756
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 7.075372219085693
    },
    {
      "epoch": 0.6079132791327914,
      "grad_norm": 14.281652450561523,
      "learning_rate": 1e-05,
      "loss": 7.2776,
      "step": 2804
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 5.859013557434082
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 7.5920515060424805
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 7.791973114013672
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 8.185726165771484
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 6.418067932128906
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 5.7318115234375
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 7.436496257781982
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 6.856588840484619
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 6.262536525726318
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 7.9893059730529785
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 7.943924427032471
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 6.739874839782715
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 7.696950912475586
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 4.700356483459473
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 4.839610576629639
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 7.857666015625
    },
    {
      "epoch": 0.608780487804878,
      "grad_norm": 12.460153579711914,
      "learning_rate": 1e-05,
      "loss": 6.8689,
      "step": 2808
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 7.49476957321167
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 7.679872035980225
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 7.5224080085754395
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 6.857673645019531
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 5.844496250152588
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 7.128844261169434
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 6.525552272796631
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 7.159438610076904
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 5.7902750968933105
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 7.696369647979736
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 6.853558540344238
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 6.3648152351379395
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 6.373289585113525
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 4.219743728637695
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 4.777446746826172
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 7.109079837799072
    },
    {
      "epoch": 0.6096476964769648,
      "grad_norm": 15.705571174621582,
      "learning_rate": 1e-05,
      "loss": 6.5874,
      "step": 2812
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 7.076354503631592
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 6.341519355773926
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 4.583257675170898
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 5.37729549407959
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 8.192009925842285
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 6.523295879364014
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 7.931580066680908
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 7.190563678741455
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 5.524460315704346
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 6.630990982055664
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 7.0241804122924805
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 6.00548791885376
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 6.9089155197143555
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 6.81018590927124
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 9.016149520874023
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 5.683535575866699
    },
    {
      "epoch": 0.6105149051490515,
      "grad_norm": 15.941898345947266,
      "learning_rate": 1e-05,
      "loss": 6.6762,
      "step": 2816
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 3.856121301651001
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.4802045822143555
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.374114036560059
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.41750431060791
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 6.422831058502197
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 5.318769931793213
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 7.04073429107666
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 8.210907936096191
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 6.326467514038086
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 6.402570724487305
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 7.317829608917236
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 7.151120185852051
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 7.3444013595581055
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 6.748900890350342
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 6.364695072174072
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 6.565458297729492
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 15.542455673217773,
      "learning_rate": 1e-05,
      "loss": 6.5214,
      "step": 2820
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 4.896751880645752
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 5.660501956939697
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 6.338494777679443
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 7.509515285491943
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 7.240972995758057
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 6.007654190063477
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 5.497714519500732
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 6.505445957183838
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 6.351663589477539
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 5.51961612701416
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 7.371725082397461
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 6.049870014190674
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 6.3468780517578125
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 7.865437984466553
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 7.427776336669922
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 6.501120567321777
    },
    {
      "epoch": 0.612249322493225,
      "grad_norm": 12.455381393432617,
      "learning_rate": 1e-05,
      "loss": 6.4432,
      "step": 2824
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 5.50938606262207
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 6.305056571960449
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 7.158911228179932
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 7.442326068878174
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 6.149451732635498
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 5.945383548736572
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 6.694097518920898
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 7.6328511238098145
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 5.579524517059326
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 6.800246238708496
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 7.210615634918213
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 7.305051326751709
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 6.964170455932617
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 6.244561195373535
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 6.941896915435791
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 6.100233554840088
    },
    {
      "epoch": 0.6131165311653116,
      "grad_norm": 16.07228660583496,
      "learning_rate": 1e-05,
      "loss": 6.624,
      "step": 2828
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 7.296481609344482
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 6.042710304260254
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 7.2303571701049805
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 7.174863338470459
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 7.007490158081055
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 7.014399528503418
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 7.392733573913574
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 4.694185733795166
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 7.060219764709473
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 6.434322357177734
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 6.948186874389648
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 7.741384029388428
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 6.55406379699707
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 5.8425421714782715
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 5.708854675292969
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 5.165640354156494
    },
    {
      "epoch": 0.6139837398373984,
      "grad_norm": 15.579316139221191,
      "learning_rate": 1e-05,
      "loss": 6.5818,
      "step": 2832
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 7.670608043670654
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 6.483217239379883
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 7.2088470458984375
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 6.121575355529785
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 5.7657904624938965
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 6.7665252685546875
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 5.752753257751465
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 6.649280548095703
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 6.992629051208496
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 5.953804969787598
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 8.384716033935547
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 5.421087741851807
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 7.796810626983643
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 5.846492767333984
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 7.405245304107666
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 6.509500503540039
    },
    {
      "epoch": 0.6148509485094851,
      "grad_norm": 17.671592712402344,
      "learning_rate": 1e-05,
      "loss": 6.6706,
      "step": 2836
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 7.074159622192383
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 7.16596794128418
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 6.93652868270874
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 6.546648025512695
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 6.795493125915527
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 6.952841758728027
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 7.327287673950195
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 7.707530498504639
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 6.769991874694824
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 6.23517370223999
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 4.595832347869873
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 6.998809814453125
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 8.210712432861328
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.489048004150391
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.755606174468994
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.142043590545654
    },
    {
      "epoch": 0.6157181571815719,
      "grad_norm": 14.553421974182129,
      "learning_rate": 1e-05,
      "loss": 6.794,
      "step": 2840
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 6.961140155792236
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 7.110058784484863
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 7.658966541290283
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 6.850924968719482
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 6.155214309692383
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 6.88286018371582
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 6.679111957550049
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 6.46077823638916
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 7.510700225830078
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 7.018609523773193
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 6.404324054718018
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 7.964832782745361
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 7.906429290771484
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.59902811050415
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.4084625244140625
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.576578140258789
    },
    {
      "epoch": 0.6165853658536585,
      "grad_norm": 14.586644172668457,
      "learning_rate": 1e-05,
      "loss": 6.9468,
      "step": 2844
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 7.093798637390137
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 6.955313205718994
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 7.1992902755737305
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 7.773565292358398
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 7.692234992980957
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 5.679047107696533
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 6.648293495178223
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 5.803753852844238
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 6.374977111816406
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 7.453478813171387
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 6.542229652404785
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 7.274629592895508
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 6.474287033081055
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 4.959311008453369
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 7.177793979644775
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 5.517973899841309
    },
    {
      "epoch": 0.6174525745257453,
      "grad_norm": 14.23936939239502,
      "learning_rate": 1e-05,
      "loss": 6.6637,
      "step": 2848
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 6.327163219451904
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 7.77616024017334
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 6.658660411834717
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 7.622954368591309
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 4.664573669433594
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 7.5467119216918945
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 7.481223106384277
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 6.750880241394043
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 5.681211471557617
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 7.740293025970459
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 6.144954204559326
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 4.932125091552734
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 6.565516948699951
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 6.159246444702148
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 5.368935585021973
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 4.237442970275879
    },
    {
      "epoch": 0.618319783197832,
      "grad_norm": 10.739510536193848,
      "learning_rate": 1e-05,
      "loss": 6.3536,
      "step": 2852
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 6.915613651275635
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 7.69415283203125
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 7.4269700050354
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 7.879580974578857
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 7.42840576171875
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 6.763885021209717
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 7.833502292633057
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 4.202825546264648
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 6.918463230133057
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 6.667092323303223
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 5.675240993499756
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 3.616546869277954
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 6.777650833129883
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 7.491796493530273
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 6.182111740112305
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 7.5564351081848145
    },
    {
      "epoch": 0.6191869918699187,
      "grad_norm": 11.142793655395508,
      "learning_rate": 1e-05,
      "loss": 6.6894,
      "step": 2856
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 5.661327838897705
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 7.378166198730469
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 6.860542297363281
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 7.6185455322265625
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 6.627579212188721
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 7.079357147216797
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 7.216246128082275
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 6.889342784881592
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 6.987448215484619
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 8.177338600158691
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 6.546790599822998
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 8.949885368347168
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.156546592712402
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 7.138677597045898
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.505321502685547
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.346861362457275
    },
    {
      "epoch": 0.6200542005420054,
      "grad_norm": 19.32866096496582,
      "learning_rate": 1e-05,
      "loss": 7.0087,
      "step": 2860
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 7.557873249053955
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 6.632143020629883
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 8.247220039367676
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 5.79668664932251
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 6.617449760437012
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 7.414296627044678
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 8.906270980834961
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 6.529980182647705
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 4.3115668296813965
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 4.797238349914551
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 4.505855560302734
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 7.1711039543151855
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 6.383058071136475
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 7.673670291900635
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 7.498111248016357
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 6.769301891326904
    },
    {
      "epoch": 0.6209214092140921,
      "grad_norm": 12.768209457397461,
      "learning_rate": 1e-05,
      "loss": 6.6757,
      "step": 2864
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 7.353857040405273
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 5.816827297210693
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 6.926506042480469
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 6.515384674072266
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 8.304855346679688
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 4.9052839279174805
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 6.475844860076904
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 7.40253210067749
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 8.085371971130371
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 4.585113525390625
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 7.351163864135742
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 9.597562789916992
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 6.328104019165039
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 6.209489822387695
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 7.960391044616699
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 7.516222953796387
    },
    {
      "epoch": 0.6217886178861789,
      "grad_norm": 11.805091857910156,
      "learning_rate": 1e-05,
      "loss": 6.9584,
      "step": 2868
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 7.739044189453125
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 6.766573429107666
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 6.814615726470947
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 7.302679538726807
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 7.000471115112305
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 7.160862445831299
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 6.385294437408447
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 6.8255205154418945
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 6.561889171600342
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 7.044961452484131
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 7.978976726531982
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 6.907705307006836
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 7.173198223114014
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 6.878781795501709
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 6.523412227630615
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 6.720782279968262
    },
    {
      "epoch": 0.6226558265582656,
      "grad_norm": 14.740925788879395,
      "learning_rate": 1e-05,
      "loss": 6.9865,
      "step": 2872
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 4.140867710113525
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 7.737199306488037
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 7.407397270202637
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 6.66535758972168
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 7.677283763885498
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 5.98647928237915
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 6.4539337158203125
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 6.110246658325195
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 4.3283514976501465
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 7.047573089599609
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 7.659388065338135
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 6.9382805824279785
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 6.358439922332764
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 5.6752519607543945
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 6.49271821975708
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 6.391416072845459
    },
    {
      "epoch": 0.6235230352303524,
      "grad_norm": 14.318866729736328,
      "learning_rate": 1e-05,
      "loss": 6.4419,
      "step": 2876
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 6.124303340911865
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 5.716794967651367
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 5.444113254547119
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 3.6183338165283203
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 7.060810089111328
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 6.300135612487793
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 6.338077545166016
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 6.855233192443848
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 7.367743968963623
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 6.741415500640869
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 4.5215044021606445
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 6.139451026916504
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 7.06640625
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 6.789772033691406
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 5.682029724121094
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 6.779738903045654
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 16.076688766479492,
      "learning_rate": 1e-05,
      "loss": 6.1591,
      "step": 2880
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 7.424869060516357
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 7.694343566894531
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 7.569427967071533
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 7.0421600341796875
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 8.179163932800293
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 7.679787635803223
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 7.914504528045654
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 6.206838607788086
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 6.9009108543396
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 5.235537528991699
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 6.688065052032471
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 7.10872220993042
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 6.616541862487793
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 6.684658527374268
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 6.061112403869629
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 7.3426513671875
    },
    {
      "epoch": 0.6252574525745257,
      "grad_norm": 12.984963417053223,
      "learning_rate": 1e-05,
      "loss": 7.0218,
      "step": 2884
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 7.316714763641357
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 7.669378280639648
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 6.083823204040527
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 5.5942559242248535
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 5.994674205780029
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 7.65727424621582
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 7.491889476776123
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 7.544705867767334
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 6.231135845184326
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 9.19821548461914
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 5.837100505828857
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 7.025590896606445
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 6.921371936798096
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 7.386505603790283
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 7.048504829406738
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 8.870550155639648
    },
    {
      "epoch": 0.6261246612466125,
      "grad_norm": 16.058135986328125,
      "learning_rate": 1e-05,
      "loss": 7.117,
      "step": 2888
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 6.961270809173584
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 6.323180675506592
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 5.7001471519470215
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 6.618306636810303
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 7.515141010284424
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.949538707733154
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.833883285522461
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.776295185089111
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 7.107987403869629
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 5.858280658721924
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 4.123016834259033
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 6.336791515350342
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 7.801311492919922
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 7.320178985595703
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 5.932521820068359
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 5.628948211669922
    },
    {
      "epoch": 0.6269918699186992,
      "grad_norm": 19.8896484375,
      "learning_rate": 1e-05,
      "loss": 6.4867,
      "step": 2892
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 6.0996270179748535
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 5.883898735046387
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 6.9693474769592285
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 5.325878143310547
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 7.126956939697266
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 5.727004528045654
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 6.210131645202637
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 6.602412223815918
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 6.301314353942871
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 7.421432018280029
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 6.85776424407959
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 6.8183064460754395
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 7.6466193199157715
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 7.659642696380615
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 7.11370325088501
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 7.800291538238525
    },
    {
      "epoch": 0.6278590785907859,
      "grad_norm": 11.852243423461914,
      "learning_rate": 1e-05,
      "loss": 6.7228,
      "step": 2896
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 6.520269393920898
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 6.61933708190918
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 4.809135913848877
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 8.72439193725586
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 5.992181777954102
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 4.42287540435791
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 5.553189754486084
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 7.69468879699707
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 6.688141822814941
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 5.819249629974365
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 6.70602560043335
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 5.99154806137085
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 6.91455078125
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 6.4732770919799805
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 7.365283012390137
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 6.948885440826416
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 11.160177230834961,
      "learning_rate": 1e-05,
      "loss": 6.4527,
      "step": 2900
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 6.102463722229004
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 7.544510841369629
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 6.652069568634033
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 6.137784481048584
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 6.930039882659912
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 7.150925159454346
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 7.1927947998046875
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 6.284048557281494
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 7.198973655700684
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 6.176645755767822
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 6.62582540512085
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 8.112987518310547
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 6.092918872833252
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 6.860413074493408
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 6.949301719665527
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 7.457238674163818
    },
    {
      "epoch": 0.6295934959349594,
      "grad_norm": 13.082505226135254,
      "learning_rate": 1e-05,
      "loss": 6.8418,
      "step": 2904
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 6.76523494720459
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 7.958374977111816
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 6.782784938812256
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 5.346006393432617
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 7.034312725067139
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 5.966742992401123
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 6.352673530578613
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 6.645475387573242
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 7.782662391662598
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 7.2684326171875
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 5.004591941833496
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 6.436066150665283
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 6.629383563995361
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 6.971982002258301
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 6.082483291625977
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 7.448995113372803
    },
    {
      "epoch": 0.6304607046070461,
      "grad_norm": 12.627994537353516,
      "learning_rate": 1e-05,
      "loss": 6.6548,
      "step": 2908
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 6.23740291595459
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 6.311976909637451
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 5.212954044342041
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 7.083738803863525
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 6.850632190704346
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 5.5211005210876465
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 6.016540050506592
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 5.962192535400391
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 6.78920316696167
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 6.240565299987793
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 6.077442169189453
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 7.835999011993408
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 6.229988098144531
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 6.4245924949646
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 7.262609481811523
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 6.681662082672119
    },
    {
      "epoch": 0.6313279132791328,
      "grad_norm": 10.593846321105957,
      "learning_rate": 1e-05,
      "loss": 6.4212,
      "step": 2912
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.286750793457031
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.348808765411377
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.236781120300293
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 6.219198703765869
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 6.346336841583252
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 6.501184463500977
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 6.719212532043457
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 4.620849609375
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 7.051093578338623
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 7.2415618896484375
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 6.394054412841797
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 7.4106035232543945
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 7.234339714050293
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 6.745616912841797
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 7.26104211807251
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 7.908285617828369
    },
    {
      "epoch": 0.6321951219512195,
      "grad_norm": 12.17974853515625,
      "learning_rate": 1e-05,
      "loss": 6.8454,
      "step": 2916
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 7.259070873260498
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 6.623355865478516
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 7.5175042152404785
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 7.838759422302246
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 7.419409275054932
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 6.150729179382324
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 7.375971794128418
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 7.1316704750061035
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 3.7950079441070557
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 4.946311950683594
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 7.2549896240234375
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 5.836474418640137
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 7.085110664367676
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 6.363537788391113
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 5.7608442306518555
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 6.54072380065918
    },
    {
      "epoch": 0.6330623306233062,
      "grad_norm": 11.184121131896973,
      "learning_rate": 1e-05,
      "loss": 6.5562,
      "step": 2920
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 5.9486894607543945
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 6.794875621795654
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 7.047513484954834
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 7.230314254760742
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 5.791388034820557
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 7.172773838043213
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 6.784804821014404
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 5.949589252471924
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.878331661224365
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.259545803070068
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 7.361484050750732
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.529605388641357
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 7.944180011749268
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 6.986174583435059
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 7.503406524658203
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 9.895962715148926
    },
    {
      "epoch": 0.633929539295393,
      "grad_norm": 23.025367736816406,
      "learning_rate": 1e-05,
      "loss": 7.0049,
      "step": 2924
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 7.327630996704102
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 7.28208589553833
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 7.08030891418457
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 6.2869720458984375
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 5.69514274597168
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 6.978360176086426
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 7.4008588790893555
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 7.117042064666748
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 7.136325359344482
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 6.964322090148926
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 6.474874973297119
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 5.792557239532471
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 6.32825231552124
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 8.071592330932617
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 5.5080885887146
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 7.147639751434326
    },
    {
      "epoch": 0.6347967479674796,
      "grad_norm": 12.841803550720215,
      "learning_rate": 1e-05,
      "loss": 6.787,
      "step": 2928
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 5.690371990203857
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 6.296979904174805
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 7.245286464691162
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 6.19757604598999
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 7.295477867126465
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 8.559340476989746
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 6.577478408813477
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 5.497926712036133
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 7.4216179847717285
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 8.175374984741211
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 7.018924713134766
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 6.5655412673950195
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 6.955780982971191
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 6.328681468963623
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 6.634375095367432
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 6.551774501800537
    },
    {
      "epoch": 0.6356639566395664,
      "grad_norm": 15.757410049438477,
      "learning_rate": 1e-05,
      "loss": 6.8133,
      "step": 2932
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.6495747566223145
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.389499664306641
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.4310736656188965
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.8707051277160645
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 5.980794429779053
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 4.390344619750977
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 6.735741138458252
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 7.239660739898682
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 5.485915184020996
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 7.31658411026001
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 5.722437381744385
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 6.374282360076904
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 6.971195697784424
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 7.702112197875977
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 6.2108869552612305
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 7.586514949798584
    },
    {
      "epoch": 0.6365311653116531,
      "grad_norm": 16.04740333557129,
      "learning_rate": 1e-05,
      "loss": 6.5036,
      "step": 2936
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 6.364093780517578
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 4.8036932945251465
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 6.3790812492370605
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 7.196275234222412
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 6.755807876586914
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 6.850652694702148
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 6.2046003341674805
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 7.068620204925537
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 6.290462493896484
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 7.813479900360107
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 4.3123579025268555
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 6.854496955871582
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.062154769897461
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.089585304260254
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.752026081085205
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.296201705932617
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 12.864632606506348,
      "learning_rate": 1e-05,
      "loss": 6.6308,
      "step": 2940
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 6.915544033050537
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 5.5110368728637695
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 7.412018775939941
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 4.008595943450928
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 6.388251304626465
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 5.827747344970703
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 8.36933708190918
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 7.041099548339844
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 6.50473690032959
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 7.218450546264648
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 8.460700035095215
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 6.826929569244385
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 7.965693950653076
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 7.666812896728516
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 4.573280334472656
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 7.22517204284668
    },
    {
      "epoch": 0.6382655826558266,
      "grad_norm": 12.968968391418457,
      "learning_rate": 1e-05,
      "loss": 6.7447,
      "step": 2944
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 4.256819725036621
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 7.220003604888916
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 5.84385871887207
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 6.83231782913208
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 6.577240467071533
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 5.731452941894531
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 7.173474311828613
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 7.387933254241943
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 6.000561714172363
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 7.0321784019470215
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 7.213622570037842
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 7.875412464141846
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 6.685591220855713
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 5.849691390991211
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 6.566549777984619
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 6.619061470031738
    },
    {
      "epoch": 0.6391327913279132,
      "grad_norm": 13.759678840637207,
      "learning_rate": 1e-05,
      "loss": 6.5541,
      "step": 2948
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 5.14940071105957
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 7.230422496795654
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 6.712510108947754
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 6.277798175811768
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 7.171990871429443
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 7.040064811706543
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 6.870144367218018
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 6.1712565422058105
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 7.077470302581787
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 6.716280937194824
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 6.258222579956055
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 7.4835686683654785
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 7.396604537963867
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.766620635986328
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.17078161239624
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.679590225219727
    },
    {
      "epoch": 0.64,
      "grad_norm": 12.051959037780762,
      "learning_rate": 1e-05,
      "loss": 6.6983,
      "step": 2952
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 5.861114025115967
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 6.9058027267456055
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 7.626570224761963
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 7.841182231903076
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 7.301858901977539
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 6.2342305183410645
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 7.069175720214844
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 6.238261699676514
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 7.432559013366699
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 6.333946704864502
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 7.26953649520874
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 8.270184516906738
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 5.0260910987854
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 7.027473449707031
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 6.596469402313232
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 5.960641384124756
    },
    {
      "epoch": 0.6408672086720867,
      "grad_norm": 11.891212463378906,
      "learning_rate": 1e-05,
      "loss": 6.8122,
      "step": 2956
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 7.077924728393555
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 6.464395999908447
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 6.107536315917969
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 7.675489902496338
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 6.987309455871582
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 7.236858367919922
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 3.809643030166626
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 6.721347332000732
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 8.402836799621582
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 6.275081634521484
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 6.557848930358887
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 4.3856682777404785
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 7.530221939086914
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 7.8997907638549805
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 7.808840751647949
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 6.5876007080078125
    },
    {
      "epoch": 0.6417344173441735,
      "grad_norm": 16.49610137939453,
      "learning_rate": 1e-05,
      "loss": 6.7205,
      "step": 2960
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 7.368305206298828
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 6.832971096038818
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 6.922855377197266
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 7.127011775970459
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.903748989105225
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.644607067108154
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.857578754425049
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.312663555145264
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 7.089494228363037
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 6.9689178466796875
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 5.396499156951904
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 5.601929187774658
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 6.493768692016602
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 6.645949840545654
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 4.989226818084717
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 7.195793151855469
    },
    {
      "epoch": 0.6426016260162601,
      "grad_norm": 15.71510124206543,
      "learning_rate": 1e-05,
      "loss": 6.5845,
      "step": 2964
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 6.633315086364746
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 7.106334686279297
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 6.356083393096924
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 8.009420394897461
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 6.889753341674805
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 6.314848899841309
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 5.619261741638184
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 5.777863502502441
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 7.066571235656738
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 7.288528919219971
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 7.20443868637085
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 6.132144927978516
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 6.580475330352783
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 7.1837992668151855
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 6.305206298828125
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 6.563481330871582
    },
    {
      "epoch": 0.6434688346883469,
      "grad_norm": 14.91136360168457,
      "learning_rate": 1e-05,
      "loss": 6.6895,
      "step": 2968
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 6.559068202972412
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 7.035038948059082
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 7.3584489822387695
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 6.879783630371094
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 6.901321887969971
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 4.568371295928955
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 6.691761016845703
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 6.147943019866943
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 6.729831218719482
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 7.292705535888672
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 7.004543304443359
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 6.075597763061523
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 6.35466194152832
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 7.055107116699219
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 5.843308448791504
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 5.611433506011963
    },
    {
      "epoch": 0.6443360433604336,
      "grad_norm": 12.609126091003418,
      "learning_rate": 1e-05,
      "loss": 6.5068,
      "step": 2972
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 7.174829483032227
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 6.68557071685791
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 6.411746978759766
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 7.775049686431885
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 6.3362956047058105
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 8.470206260681152
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 4.169771194458008
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 5.664223670959473
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.673942565917969
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 7.559276103973389
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.266852378845215
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.975184440612793
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 6.754261493682861
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 8.201759338378906
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 5.917564868927002
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 7.118641376495361
    },
    {
      "epoch": 0.6452032520325204,
      "grad_norm": 14.690506935119629,
      "learning_rate": 1e-05,
      "loss": 6.7597,
      "step": 2976
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 7.057379245758057
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 6.849740982055664
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 5.859715938568115
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 7.3385329246521
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 6.897967338562012
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 7.303454399108887
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 6.744917869567871
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 6.674863338470459
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 4.685157775878906
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 7.754047870635986
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.771893501281738
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.79261589050293
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 5.761813163757324
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 6.350078105926514
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 6.84158182144165
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 5.4168171882629395
    },
    {
      "epoch": 0.646070460704607,
      "grad_norm": 15.847610473632812,
      "learning_rate": 1e-05,
      "loss": 6.5688,
      "step": 2980
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 5.990406036376953
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.8375043869018555
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.426719665527344
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 7.1093363761901855
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 5.694310188293457
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 6.247706890106201
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 6.18417501449585
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 7.420140266418457
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 7.8492045402526855
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 6.299777030944824
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 7.312253475189209
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 7.406601428985596
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 5.19572114944458
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 7.974205017089844
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 6.38613748550415
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 4.986990451812744
    },
    {
      "epoch": 0.6469376693766937,
      "grad_norm": 15.7678804397583,
      "learning_rate": 1e-05,
      "loss": 6.5826,
      "step": 2984
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.112422943115234
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.725095748901367
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.610837459564209
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.423206806182861
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 5.962355136871338
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 5.711612701416016
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 6.874331474304199
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 7.299546718597412
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 7.160798072814941
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 8.435261726379395
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 6.033905029296875
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 7.364127159118652
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.111031532287598
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.294989109039307
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.0827789306640625
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.254892826080322
    },
    {
      "epoch": 0.6478048780487805,
      "grad_norm": 12.077766418457031,
      "learning_rate": 1e-05,
      "loss": 6.8411,
      "step": 2988
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 6.622997283935547
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 8.229836463928223
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 6.680899620056152
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 6.8239665031433105
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 6.422321319580078
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 7.291476726531982
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 6.672753810882568
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 8.073515892028809
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 6.337156295776367
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 6.051248073577881
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 4.489126205444336
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 7.026289939880371
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 5.430425643920898
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 6.962909698486328
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 6.96090841293335
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 7.422876358032227
    },
    {
      "epoch": 0.6486720867208672,
      "grad_norm": 11.221120834350586,
      "learning_rate": 1e-05,
      "loss": 6.7187,
      "step": 2992
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 6.4373860359191895
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 4.144524574279785
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 6.001603603363037
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 5.997293472290039
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 6.123618125915527
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 6.841954708099365
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 6.60831356048584
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 7.887632369995117
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 5.12526273727417
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 7.064792633056641
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 7.441007137298584
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 6.500251293182373
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 6.826262474060059
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 7.361700534820557
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 5.8416595458984375
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 7.953126907348633
    },
    {
      "epoch": 0.649539295392954,
      "grad_norm": 14.1998929977417,
      "learning_rate": 1e-05,
      "loss": 6.5098,
      "step": 2996
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 6.905327320098877
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 7.0829176902771
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 6.5724592208862305
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 6.807876110076904
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 5.7998199462890625
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 6.1451945304870605
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 6.344065189361572
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 6.096579551696777
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 6.553387641906738
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 6.973588943481445
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 7.130362510681152
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 7.9946722984313965
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 6.681268215179443
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 6.465756416320801
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 7.265545845031738
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 6.552027702331543
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 11.181787490844727,
      "learning_rate": 1e-05,
      "loss": 6.7107,
      "step": 3000
    },
    {
      "epoch": 0.6504065040650406,
      "eval_runtime": 480.0086,
      "eval_samples_per_second": 4.271,
      "eval_steps_per_second": 4.271,
      "step": 3000
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 6.6430535316467285
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 8.356087684631348
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 5.4631123542785645
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 6.937084674835205
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 6.849002361297607
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 6.385186672210693
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 8.974177360534668
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 7.358759880065918
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 6.395013809204102
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 6.074800968170166
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 5.899938106536865
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 6.654277324676514
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 5.84924840927124
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 6.997673988342285
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 5.850350379943848
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 5.91982889175415
    },
    {
      "epoch": 0.6512737127371274,
      "grad_norm": 20.300992965698242,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 3004
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 6.150787830352783
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 6.77723503112793
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 7.643569469451904
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 3.782912492752075
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 6.768078804016113
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 5.353968143463135
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 6.454559803009033
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 7.372260093688965
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 5.092303276062012
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 5.2260918617248535
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 5.0473408699035645
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 6.010507583618164
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 6.940317153930664
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 6.93406343460083
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 6.95508337020874
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 6.879912376403809
    },
    {
      "epoch": 0.6521409214092141,
      "grad_norm": 11.811129570007324,
      "learning_rate": 1e-05,
      "loss": 6.2118,
      "step": 3008
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 8.003988265991211
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 5.822079181671143
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 5.448883533477783
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 5.568692207336426
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 11.075931549072266
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 4.962958812713623
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 7.735376834869385
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 8.473557472229004
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 7.377138137817383
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 7.076942443847656
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 6.678914546966553
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 7.319223880767822
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 4.822524070739746
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 6.353198051452637
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 7.91782808303833
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 7.208099842071533
    },
    {
      "epoch": 0.6530081300813008,
      "grad_norm": 13.969758987426758,
      "learning_rate": 1e-05,
      "loss": 6.9903,
      "step": 3012
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 5.528438568115234
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 7.843838691711426
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 5.963204860687256
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 6.45867919921875
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 6.607290744781494
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 6.645744800567627
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 8.034977912902832
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 7.004552841186523
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 7.084984302520752
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 6.576303005218506
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 7.854079723358154
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 5.262631416320801
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 7.330925464630127
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 7.122564315795898
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 4.4892778396606445
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 6.56545877456665
    },
    {
      "epoch": 0.6538753387533875,
      "grad_norm": 11.737849235534668,
      "learning_rate": 1e-05,
      "loss": 6.6483,
      "step": 3016
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 7.032382488250732
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 7.032955169677734
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 6.380462169647217
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 7.912294864654541
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.151156425476074
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 5.953837871551514
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.334918022155762
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.44343900680542
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 7.1997528076171875
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 6.048315048217773
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 6.964562892913818
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 8.014921188354492
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 7.610780715942383
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 7.25827169418335
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 4.466124534606934
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 5.403763771057129
    },
    {
      "epoch": 0.6547425474254742,
      "grad_norm": 14.986077308654785,
      "learning_rate": 1e-05,
      "loss": 6.638,
      "step": 3020
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 6.880217552185059
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 5.424717426300049
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 6.143463611602783
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 5.586856365203857
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 7.2579731941223145
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 5.9719343185424805
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 6.694579124450684
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 6.232352256774902
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.279646396636963
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.420371055603027
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.618443489074707
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.50661563873291
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 8.0020751953125
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 7.0121235847473145
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 6.687322616577148
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 5.5922346115112305
    },
    {
      "epoch": 0.655609756097561,
      "grad_norm": 11.934768676757812,
      "learning_rate": 1e-05,
      "loss": 6.7069,
      "step": 3024
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 6.861358165740967
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 7.709053039550781
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 5.675206661224365
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 4.830430030822754
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 8.19210433959961
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 7.9561872482299805
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 7.456727027893066
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 6.0283708572387695
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.1364359855651855
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.706719398498535
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.756761074066162
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 7.412383079528809
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.542088508605957
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.682152271270752
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.183129787445068
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.7861647605896
    },
    {
      "epoch": 0.6564769647696477,
      "grad_norm": 16.85672950744629,
      "learning_rate": 1e-05,
      "loss": 6.7447,
      "step": 3028
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 4.845658302307129
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 6.633310794830322
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 7.007221698760986
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 6.013808250427246
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.976566791534424
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.826359272003174
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.257246017456055
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 7.355965614318848
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 6.622927665710449
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 6.882028579711914
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 6.893784046173096
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 6.776672840118408
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 7.683507442474365
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 8.604832649230957
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 6.471139907836914
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 6.786866664886475
    },
    {
      "epoch": 0.6573441734417345,
      "grad_norm": 18.371715545654297,
      "learning_rate": 1e-05,
      "loss": 6.7899,
      "step": 3032
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 4.407371520996094
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 7.720034599304199
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 6.413964748382568
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 6.215782165527344
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 6.623758316040039
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 6.581293106079102
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 7.118018627166748
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 5.636114597320557
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 5.325270175933838
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 7.911734580993652
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 6.725233554840088
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 8.023470878601074
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 4.299328804016113
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 8.074248313903809
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 5.514705181121826
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 4.270267009735107
    },
    {
      "epoch": 0.6582113821138211,
      "grad_norm": 15.13094425201416,
      "learning_rate": 1e-05,
      "loss": 6.3038,
      "step": 3036
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 7.102472305297852
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 6.270120620727539
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 7.169041633605957
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 5.1138596534729
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.096777439117432
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.613690376281738
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.896790981292725
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 7.939177989959717
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 7.926271915435791
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 6.409136772155762
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 6.706665515899658
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 6.080914497375488
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 7.794886112213135
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 6.011620998382568
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 6.847752571105957
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 7.251901626586914
    },
    {
      "epoch": 0.6590785907859079,
      "grad_norm": 17.496572494506836,
      "learning_rate": 1e-05,
      "loss": 6.7644,
      "step": 3040
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 6.754157543182373
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 7.353460311889648
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 6.3503923416137695
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 6.370490550994873
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 6.287861347198486
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 6.967191696166992
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 6.299654483795166
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 8.202604293823242
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 8.020248413085938
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 6.294399261474609
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 5.931447505950928
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 5.351676940917969
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 6.589940071105957
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 8.2031888961792
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 6.982544422149658
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 7.280282020568848
    },
    {
      "epoch": 0.6599457994579946,
      "grad_norm": 15.338545799255371,
      "learning_rate": 1e-05,
      "loss": 6.8275,
      "step": 3044
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 6.453329563140869
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 7.426760196685791
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 8.071081161499023
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 3.84506893157959
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 6.772703170776367
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 6.9366254806518555
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 7.137063503265381
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 5.668680191040039
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 7.416452884674072
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 3.886460065841675
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 6.008760929107666
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 7.407702922821045
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 6.804866313934326
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 5.06141471862793
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 6.843888759613037
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 6.727861404418945
    },
    {
      "epoch": 0.6608130081300813,
      "grad_norm": 15.819573402404785,
      "learning_rate": 1e-05,
      "loss": 6.4043,
      "step": 3048
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 5.999175071716309
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 6.885838508605957
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 8.045076370239258
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 6.863835334777832
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 7.351132392883301
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 5.77943229675293
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 7.559179306030273
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 6.885720252990723
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 6.521335601806641
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 5.030561923980713
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 6.560245513916016
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 8.261482238769531
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 6.543927192687988
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 5.806148529052734
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 7.666011333465576
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 7.771152973175049
    },
    {
      "epoch": 0.661680216802168,
      "grad_norm": 12.484796524047852,
      "learning_rate": 1e-05,
      "loss": 6.8456,
      "step": 3052
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 7.60159969329834
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 7.389766216278076
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 6.551119327545166
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 5.1379475593566895
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.636692523956299
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 4.284613132476807
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.1021575927734375
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.199016094207764
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 6.66177225112915
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 6.484530448913574
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 7.312780857086182
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 7.037057399749756
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 6.295810699462891
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 7.330053806304932
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 7.688817501068115
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 7.95220422744751
    },
    {
      "epoch": 0.6625474254742547,
      "grad_norm": 14.064882278442383,
      "learning_rate": 1e-05,
      "loss": 6.6666,
      "step": 3056
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 7.3411545753479
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 8.26278305053711
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 7.027453422546387
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 7.68352746963501
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 8.291297912597656
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 5.621434688568115
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 6.380456447601318
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 7.099815368652344
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 7.663685321807861
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 7.642379283905029
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 7.487539291381836
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 8.360776901245117
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 6.877273082733154
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 7.262320518493652
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 6.098107814788818
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 8.04455852508545
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 18.35636329650879,
      "learning_rate": 1e-05,
      "loss": 7.3215,
      "step": 3060
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 6.618167877197266
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 6.282453536987305
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 7.104316711425781
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 6.8343048095703125
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 6.644942283630371
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 7.183122158050537
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 6.972250938415527
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 6.604275703430176
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 6.719560623168945
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 8.921048164367676
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 6.432621955871582
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 7.3294453620910645
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 7.32162618637085
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 4.4286932945251465
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 6.864017486572266
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 4.08181619644165
    },
    {
      "epoch": 0.6642818428184282,
      "grad_norm": 16.118080139160156,
      "learning_rate": 1e-05,
      "loss": 6.6464,
      "step": 3064
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 5.823160648345947
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 6.082753658294678
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 7.004042148590088
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 6.545057773590088
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 7.285462856292725
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 6.633755207061768
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 6.825705528259277
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 7.930554389953613
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 5.405650615692139
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 7.161682605743408
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 6.853884220123291
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 7.433938026428223
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 5.86821174621582
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 8.196069717407227
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 7.054405689239502
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 6.346357345581055
    },
    {
      "epoch": 0.665149051490515,
      "grad_norm": 14.565918922424316,
      "learning_rate": 1e-05,
      "loss": 6.7782,
      "step": 3068
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 6.419741630554199
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 6.539949893951416
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 7.013645172119141
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 6.669002056121826
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 7.176020622253418
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 7.387882709503174
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 6.734313488006592
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 6.755761623382568
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 5.58289909362793
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 7.716768741607666
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 7.133611679077148
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 7.329147815704346
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 5.900304794311523
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 6.8630170822143555
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 7.762312889099121
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 6.007187843322754
    },
    {
      "epoch": 0.6660162601626016,
      "grad_norm": 16.263452529907227,
      "learning_rate": 1e-05,
      "loss": 6.812,
      "step": 3072
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 8.436808586120605
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 6.89450216293335
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 4.635805606842041
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 6.565013408660889
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 7.020777702331543
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 6.408511161804199
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 6.037426948547363
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 5.888520240783691
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 4.375392913818359
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 5.015242099761963
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 11.500782012939453
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 6.971255302429199
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 5.112746715545654
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 4.917848587036133
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 8.348817825317383
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 6.670815467834473
    },
    {
      "epoch": 0.6668834688346883,
      "grad_norm": 26.976526260375977,
      "learning_rate": 1e-05,
      "loss": 6.55,
      "step": 3076
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 7.141389846801758
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 5.348437786102295
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 7.184892654418945
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 6.928079605102539
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 7.003798007965088
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 7.560640811920166
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 7.792349338531494
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 6.300506591796875
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 5.528512477874756
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 6.886420726776123
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 7.738693714141846
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 7.297525882720947
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.7627058029174805
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.981835842132568
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.131626605987549
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.683210849761963
    },
    {
      "epoch": 0.6677506775067751,
      "grad_norm": 14.50956916809082,
      "learning_rate": 1e-05,
      "loss": 6.8294,
      "step": 3080
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 9.419401168823242
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 6.883665084838867
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 4.798916816711426
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 7.4314775466918945
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 6.934772491455078
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 7.104612827301025
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 6.718283653259277
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 6.946263313293457
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 5.871624946594238
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 6.73533296585083
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 7.668963432312012
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 6.6155686378479
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 7.073528289794922
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 6.493428707122803
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 6.829248905181885
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 7.0225419998168945
    },
    {
      "epoch": 0.6686178861788618,
      "grad_norm": 13.43653392791748,
      "learning_rate": 1e-05,
      "loss": 6.9092,
      "step": 3084
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 8.695253372192383
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 4.395115375518799
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 7.305434226989746
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 5.7079057693481445
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 5.036707878112793
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 7.169957637786865
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 6.567324638366699
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 6.938299179077148
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 5.8399834632873535
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 6.927510738372803
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 7.138111591339111
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 7.360803604125977
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 7.325863361358643
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 7.100378513336182
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 7.175107479095459
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 7.379731178283691
    },
    {
      "epoch": 0.6694850948509485,
      "grad_norm": 15.324769973754883,
      "learning_rate": 1e-05,
      "loss": 6.754,
      "step": 3088
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 7.606177806854248
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 6.347537994384766
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 6.55180025100708
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 4.141624927520752
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 6.075517654418945
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 6.818945407867432
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 7.292520999908447
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 7.3561835289001465
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 4.127093315124512
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 7.205801486968994
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 6.726280689239502
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 7.167605876922607
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 6.72224235534668
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 7.221676349639893
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 4.915550231933594
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 7.425318717956543
    },
    {
      "epoch": 0.6703523035230352,
      "grad_norm": 17.66419792175293,
      "learning_rate": 1e-05,
      "loss": 6.4814,
      "step": 3092
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 7.340456008911133
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 4.7539753913879395
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 6.792947769165039
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 9.4846830368042
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 7.778075695037842
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 7.016268730163574
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 6.925815105438232
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 5.249983310699463
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 7.457511901855469
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 5.6746296882629395
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 6.445501327514648
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 5.56270170211792
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 6.893149375915527
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 6.791014194488525
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 5.398098468780518
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 7.258001804351807
    },
    {
      "epoch": 0.671219512195122,
      "grad_norm": 11.329109191894531,
      "learning_rate": 1e-05,
      "loss": 6.6764,
      "step": 3096
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 7.166272163391113
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 4.417164325714111
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 7.140293598175049
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 6.839194297790527
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 6.747791767120361
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 7.305822372436523
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 7.1923723220825195
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 6.461906909942627
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 6.455948352813721
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 4.5823798179626465
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 6.251270771026611
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 7.887225151062012
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 6.142505168914795
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 5.817783355712891
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 6.564638614654541
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 6.879035472869873
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 14.235913276672363,
      "learning_rate": 1e-05,
      "loss": 6.4907,
      "step": 3100
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 5.893850803375244
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 7.475178241729736
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 7.357606887817383
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 6.660159587860107
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 5.821924209594727
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 7.376152515411377
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 6.894192218780518
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 6.518221378326416
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 5.906835556030273
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 7.263460636138916
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 4.451439380645752
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 6.592782497406006
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 7.4408793449401855
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 6.709859848022461
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 7.318637371063232
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 6.341332912445068
    },
    {
      "epoch": 0.6729539295392954,
      "grad_norm": 16.25467300415039,
      "learning_rate": 1e-05,
      "loss": 6.6264,
      "step": 3104
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 6.1267499923706055
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 5.222875118255615
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 6.8162455558776855
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 6.778524398803711
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 6.594171047210693
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 7.304202556610107
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 6.155803203582764
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 5.985201835632324
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 6.602996349334717
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 7.22621488571167
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 6.963758945465088
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 6.328255653381348
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 6.396734714508057
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 6.243399620056152
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 6.699132442474365
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 6.230587482452393
    },
    {
      "epoch": 0.6738211382113821,
      "grad_norm": 16.631933212280273,
      "learning_rate": 1e-05,
      "loss": 6.4797,
      "step": 3108
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.648554801940918
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 7.312134265899658
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.529329776763916
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.3100152015686035
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 7.3097333908081055
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 7.122884750366211
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 6.239742755889893
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 5.696510314941406
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 7.14739990234375
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 7.880595684051514
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 7.24822998046875
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 6.541991233825684
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 7.805408000946045
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 7.1241302490234375
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 5.9815216064453125
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 4.238613605499268
    },
    {
      "epoch": 0.6746883468834688,
      "grad_norm": 15.778225898742676,
      "learning_rate": 1e-05,
      "loss": 6.696,
      "step": 3112
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 7.279792785644531
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 7.994779109954834
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 5.899753570556641
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 7.405228137969971
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 7.414249897003174
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 6.617863655090332
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 7.151651382446289
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 7.450989723205566
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 7.685500144958496
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 7.116785526275635
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 7.413395881652832
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 6.50251579284668
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.688157558441162
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.70862340927124
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.832610130310059
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 9.606429100036621
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 13.745814323425293,
      "learning_rate": 1e-05,
      "loss": 7.2355,
      "step": 3116
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 6.575294494628906
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 6.625267505645752
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 5.1541948318481445
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 7.087172985076904
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 5.988989353179932
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 6.677440643310547
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 6.800328731536865
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 6.012556552886963
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 6.554917335510254
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 6.637660980224609
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 6.240764617919922
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 7.053420066833496
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 8.139394760131836
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 6.717067241668701
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 4.652884483337402
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 6.867452144622803
    },
    {
      "epoch": 0.6764227642276422,
      "grad_norm": 10.653048515319824,
      "learning_rate": 1e-05,
      "loss": 6.4866,
      "step": 3120
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.911978244781494
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.911139011383057
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.552277565002441
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.486490726470947
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.963820934295654
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 7.123997211456299
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.385986328125
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 7.017512798309326
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 6.286756992340088
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 7.2729902267456055
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 5.240616321563721
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 7.454142093658447
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 6.169125556945801
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 7.102479934692383
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 3.755666971206665
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 5.500953674316406
    },
    {
      "epoch": 0.677289972899729,
      "grad_norm": 13.431207656860352,
      "learning_rate": 1e-05,
      "loss": 6.446,
      "step": 3124
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 3.9953114986419678
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 7.023213863372803
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 6.5439252853393555
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 7.363394260406494
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 6.518923282623291
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 6.4076151847839355
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 7.901010036468506
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 6.7525153160095215
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 6.352352619171143
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 6.346587181091309
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 6.4103617668151855
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 6.534364223480225
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 7.178454399108887
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 8.080158233642578
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 6.734088897705078
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 6.286498546600342
    },
    {
      "epoch": 0.6781571815718157,
      "grad_norm": 10.778429985046387,
      "learning_rate": 1e-05,
      "loss": 6.6518,
      "step": 3128
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 6.591091632843018
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 6.2214674949646
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 6.763278484344482
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 8.980976104736328
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 6.521393299102783
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 6.700088977813721
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 7.063473224639893
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 6.996626377105713
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 6.20829963684082
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 8.105621337890625
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 4.765518665313721
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 6.770419597625732
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 6.493396759033203
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 4.957300662994385
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 8.045936584472656
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 5.210772514343262
    },
    {
      "epoch": 0.6790243902439025,
      "grad_norm": 11.219489097595215,
      "learning_rate": 1e-05,
      "loss": 6.6497,
      "step": 3132
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 7.0276713371276855
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 7.582641124725342
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 5.7219133377075195
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 10.710503578186035
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 7.715288162231445
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 6.991793155670166
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 6.757054805755615
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 7.197739601135254
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 8.238565444946289
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 4.550709247589111
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 6.566037654876709
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 6.817623615264893
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 5.635059833526611
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 6.177839279174805
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 7.441128253936768
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 6.856297016143799
    },
    {
      "epoch": 0.6798915989159892,
      "grad_norm": 11.46139907836914,
      "learning_rate": 1e-05,
      "loss": 6.9992,
      "step": 3136
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.8286662101745605
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.965875625610352
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.685858249664307
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.174970626831055
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 5.854742050170898
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 6.047853469848633
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 7.384068012237549
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 5.103357315063477
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 6.68730354309082
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 7.226191520690918
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 7.335867404937744
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 8.318870544433594
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 5.432699203491211
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 6.393816947937012
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 3.4443938732147217
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 7.342390537261963
    },
    {
      "epoch": 0.6807588075880758,
      "grad_norm": 13.530052185058594,
      "learning_rate": 1e-05,
      "loss": 6.4517,
      "step": 3140
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 7.496636867523193
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 4.832215785980225
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 6.3810625076293945
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 6.910205364227295
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 7.753830909729004
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 6.281393051147461
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 5.457685470581055
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 6.266436576843262
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 8.156505584716797
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 6.88854455947876
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 7.616025924682617
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 5.612383842468262
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 6.597033500671387
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 6.697036266326904
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 6.639718055725098
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 7.124227046966553
    },
    {
      "epoch": 0.6816260162601626,
      "grad_norm": 14.659127235412598,
      "learning_rate": 1e-05,
      "loss": 6.6694,
      "step": 3144
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 6.702719211578369
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 6.268210411071777
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 7.2088775634765625
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 7.63961124420166
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 5.891770839691162
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 7.133334636688232
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 6.620997428894043
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 4.768259525299072
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 7.687658309936523
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 6.65647554397583
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 7.2757720947265625
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 7.114518165588379
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 6.924847602844238
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 7.016953468322754
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 7.13620138168335
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 7.628429889678955
    },
    {
      "epoch": 0.6824932249322493,
      "grad_norm": 14.253479957580566,
      "learning_rate": 1e-05,
      "loss": 6.8547,
      "step": 3148
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 6.392457962036133
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 6.165771007537842
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 7.255860805511475
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 5.534262657165527
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 6.639930725097656
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 7.600618362426758
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 7.209080696105957
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 7.147799015045166
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 7.478972911834717
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 6.765793800354004
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 5.924666404724121
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 6.594627380371094
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 7.717128276824951
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 6.543539524078369
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 6.980435371398926
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 7.12115478515625
    },
    {
      "epoch": 0.6833604336043361,
      "grad_norm": 12.41922378540039,
      "learning_rate": 1e-05,
      "loss": 6.817,
      "step": 3152
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.37639045715332
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.5706400871276855
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 7.973569869995117
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 7.74952507019043
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 7.979527473449707
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 4.793483257293701
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 8.6519193649292
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 7.049219608306885
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 6.042855739593506
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 6.941741466522217
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 8.561640739440918
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 6.713115692138672
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 6.549021244049072
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 6.245985984802246
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 7.324416160583496
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 7.412308692932129
    },
    {
      "epoch": 0.6842276422764227,
      "grad_norm": 16.949325561523438,
      "learning_rate": 1e-05,
      "loss": 7.0585,
      "step": 3156
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 4.937875747680664
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 5.750771522521973
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 6.814063549041748
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 6.455303192138672
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 7.4748077392578125
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 7.786291599273682
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 7.649120330810547
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 6.643436908721924
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 7.568569660186768
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 6.337569713592529
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 7.756599426269531
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 6.722350597381592
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 9.782960891723633
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 8.183961868286133
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 6.290607452392578
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 5.306130409240723
    },
    {
      "epoch": 0.6850948509485095,
      "grad_norm": 23.257083892822266,
      "learning_rate": 1e-05,
      "loss": 6.9663,
      "step": 3160
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 7.2303466796875
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 7.416967391967773
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 7.161050796508789
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 7.936948776245117
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 4.552273273468018
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 7.737447261810303
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 7.225824356079102
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 6.744736671447754
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 6.228042125701904
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 7.384419918060303
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 7.349033832550049
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 7.534470558166504
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 6.334514141082764
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 8.137890815734863
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 7.553277492523193
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 6.089383125305176
    },
    {
      "epoch": 0.6859620596205962,
      "grad_norm": 14.366002082824707,
      "learning_rate": 1e-05,
      "loss": 7.0385,
      "step": 3164
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 7.549815654754639
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 6.884127140045166
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 6.362930774688721
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 5.565733909606934
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 7.67808723449707
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 6.778112888336182
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 5.493626117706299
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 3.950209379196167
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 7.558316707611084
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 6.99381160736084
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 5.256901741027832
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 7.150997638702393
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 6.8224992752075195
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 7.250980854034424
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 8.16071605682373
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 4.769413471221924
    },
    {
      "epoch": 0.686829268292683,
      "grad_norm": 12.57912826538086,
      "learning_rate": 1e-05,
      "loss": 6.5141,
      "step": 3168
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 7.22796630859375
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 6.2438530921936035
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 8.017062187194824
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 6.107720851898193
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 8.242650032043457
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 5.987921237945557
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 3.759178638458252
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 6.847244739532471
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 6.819196701049805
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 7.61808443069458
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 6.757737636566162
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 7.8989458084106445
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 7.372180938720703
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 6.541980266571045
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 6.982135772705078
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 6.298113822937012
    },
    {
      "epoch": 0.6876964769647697,
      "grad_norm": 13.589303970336914,
      "learning_rate": 1e-05,
      "loss": 6.7951,
      "step": 3172
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 6.529097557067871
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 8.694768905639648
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 7.222049236297607
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 7.460326194763184
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 6.225841045379639
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 7.1358323097229
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 7.112811088562012
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 6.706820487976074
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 7.858440399169922
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 7.905235767364502
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 6.762881278991699
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 4.846595764160156
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 6.485614776611328
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 6.323178291320801
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 7.251567840576172
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 6.240309238433838
    },
    {
      "epoch": 0.6885636856368563,
      "grad_norm": 12.950742721557617,
      "learning_rate": 1e-05,
      "loss": 6.9226,
      "step": 3176
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 6.679394721984863
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 7.154181957244873
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 6.3447160720825195
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 6.526447772979736
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 7.466861724853516
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 5.193294048309326
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 6.248506546020508
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 5.641322135925293
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 6.092576503753662
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 7.282070636749268
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 7.059774875640869
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 6.4330668449401855
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 6.816746234893799
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 8.264513969421387
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 4.6166815757751465
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 7.544172286987305
    },
    {
      "epoch": 0.6894308943089431,
      "grad_norm": 12.714524269104004,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 3180
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 6.313596725463867
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 6.701550483703613
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 7.487135410308838
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 7.144519329071045
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.09623908996582
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 6.431479454040527
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.621871471405029
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.310916900634766
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 4.355876445770264
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.605350971221924
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.731228828430176
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.22174072265625
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 4.566770076751709
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 7.231600284576416
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 5.386518478393555
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 7.448775768280029
    },
    {
      "epoch": 0.6902981029810298,
      "grad_norm": 19.857017517089844,
      "learning_rate": 1e-05,
      "loss": 6.5409,
      "step": 3184
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 4.3262152671813965
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 6.495809555053711
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 6.4391069412231445
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 4.796468734741211
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 7.835301876068115
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 7.2797346115112305
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 7.426695823669434
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 3.8241183757781982
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.982243061065674
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.979791641235352
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 7.355151176452637
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.067091941833496
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 8.432404518127441
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 6.4759674072265625
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 8.11359977722168
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 6.74733829498291
    },
    {
      "epoch": 0.6911653116531166,
      "grad_norm": 16.862436294555664,
      "learning_rate": 1e-05,
      "loss": 6.5986,
      "step": 3188
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 6.650252342224121
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 7.377345561981201
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 6.198541164398193
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 5.897351264953613
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 5.918773651123047
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 7.617143630981445
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 5.693271636962891
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 6.726565837860107
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 8.141383171081543
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 6.953595161437988
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 5.739829063415527
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 7.485979080200195
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 7.4196038246154785
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 6.991357803344727
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 7.035168170928955
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 7.206604957580566
    },
    {
      "epoch": 0.6920325203252032,
      "grad_norm": 15.529784202575684,
      "learning_rate": 1e-05,
      "loss": 6.8158,
      "step": 3192
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 7.192352294921875
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 7.835326671600342
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 7.335683822631836
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 9.434624671936035
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 7.736574172973633
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 6.325892448425293
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 5.857804775238037
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 6.899656772613525
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 6.400824069976807
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 8.425315856933594
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 6.90153169631958
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 7.995847225189209
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 5.242502689361572
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 5.626404762268066
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 6.542351722717285
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 7.78339147567749
    },
    {
      "epoch": 0.69289972899729,
      "grad_norm": 13.56089973449707,
      "learning_rate": 1e-05,
      "loss": 7.096,
      "step": 3196
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 4.3450236320495605
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 7.2266716957092285
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 5.611503601074219
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 7.810464859008789
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 7.577244281768799
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 5.707327365875244
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 7.668972492218018
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 7.395395755767822
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 7.80902624130249
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 7.098207473754883
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 6.230511665344238
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 7.965237617492676
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 7.974755764007568
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 6.0679802894592285
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 5.701981544494629
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 6.8926286697387695
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 14.802237510681152,
      "learning_rate": 1e-05,
      "loss": 6.8177,
      "step": 3200
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 6.214493751525879
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 7.205840587615967
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 6.199040412902832
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 7.122884750366211
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 5.95927619934082
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 7.37979793548584
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 5.883342266082764
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 7.373129367828369
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 8.271303176879883
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 7.056187152862549
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 7.31144905090332
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 6.892487525939941
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 6.774799823760986
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 7.97303581237793
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 7.161345481872559
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 7.0922088623046875
    },
    {
      "epoch": 0.6946341463414634,
      "grad_norm": 14.733749389648438,
      "learning_rate": 1e-05,
      "loss": 6.9919,
      "step": 3204
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 7.619675159454346
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 3.8383612632751465
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 8.056689262390137
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 6.455013751983643
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 7.265152931213379
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 5.481089115142822
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 6.698128700256348
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 6.889891147613525
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 7.152707099914551
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 7.808652400970459
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 4.122654914855957
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 5.820375919342041
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.266839027404785
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.855172157287598
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.757794380187988
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.671878814697266
    },
    {
      "epoch": 0.6955013550135501,
      "grad_norm": 10.663454055786133,
      "learning_rate": 1e-05,
      "loss": 6.485,
      "step": 3208
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 6.263323783874512
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 5.881139755249023
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 6.751173973083496
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 7.944857597351074
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 6.903833866119385
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 5.023727893829346
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 6.131345272064209
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 6.98924446105957
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 4.747225284576416
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 7.159152507781982
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 7.865621089935303
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 6.846392631530762
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 7.577534198760986
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 7.242578029632568
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 6.867345333099365
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 6.76970100402832
    },
    {
      "epoch": 0.6963685636856368,
      "grad_norm": 11.51254940032959,
      "learning_rate": 1e-05,
      "loss": 6.6853,
      "step": 3212
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 5.9326958656311035
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 5.38547420501709
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 6.828978061676025
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 6.677741050720215
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 4.9863433837890625
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 7.030806541442871
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 6.958139896392822
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 5.979995250701904
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 7.38889741897583
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 5.294836521148682
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 7.42649507522583
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 6.4756245613098145
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 6.748545169830322
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 8.32238483428955
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 7.704308986663818
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 6.577224254608154
    },
    {
      "epoch": 0.6972357723577236,
      "grad_norm": 23.436513900756836,
      "learning_rate": 1e-05,
      "loss": 6.6074,
      "step": 3216
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 6.789823055267334
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 7.009745121002197
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 6.142955303192139
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 4.880630016326904
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 4.113590240478516
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 6.8004655838012695
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 4.988260269165039
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 5.475432872772217
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 7.518576145172119
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 5.730064868927002
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 6.8889479637146
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 4.806482791900635
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 7.521934986114502
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 5.901029109954834
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 7.9518232345581055
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 7.516493797302246
    },
    {
      "epoch": 0.6981029810298103,
      "grad_norm": 15.7789306640625,
      "learning_rate": 1e-05,
      "loss": 6.2523,
      "step": 3220
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 6.454049110412598
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 6.9864912033081055
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 6.546681880950928
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 7.021920680999756
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 7.419469356536865
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 7.195582389831543
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 7.700723648071289
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 6.8578200340271
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 6.946720600128174
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 6.866175651550293
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 7.384482383728027
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 6.507913589477539
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 6.788791179656982
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 5.884879112243652
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 7.416712284088135
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 6.872776031494141
    },
    {
      "epoch": 0.6989701897018971,
      "grad_norm": 16.410411834716797,
      "learning_rate": 1e-05,
      "loss": 6.9282,
      "step": 3224
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 5.810122013092041
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 6.376010417938232
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 7.172010898590088
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 6.880353927612305
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 7.696399688720703
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 7.845385551452637
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 6.99953556060791
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 6.987853050231934
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 7.2675371170043945
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 7.114041805267334
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 6.491259574890137
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 6.875357627868652
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 7.318687915802002
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 5.528781890869141
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 7.574615001678467
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 6.399538516998291
    },
    {
      "epoch": 0.6998373983739837,
      "grad_norm": 14.397878646850586,
      "learning_rate": 1e-05,
      "loss": 6.8961,
      "step": 3228
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 6.661983013153076
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 5.823894500732422
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 6.782650470733643
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 6.316276550292969
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 6.651571273803711
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 7.480037689208984
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 5.562663555145264
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 7.240004539489746
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 7.83546781539917
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 5.592990875244141
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 7.0318989753723145
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 6.699380874633789
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 7.883141994476318
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 6.055520534515381
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 6.892671585083008
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 5.972583770751953
    },
    {
      "epoch": 0.7007046070460705,
      "grad_norm": 15.444762229919434,
      "learning_rate": 1e-05,
      "loss": 6.6552,
      "step": 3232
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 7.200063228607178
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 6.890411853790283
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 5.664958477020264
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 6.16837739944458
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 7.012070655822754
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 7.2737836837768555
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 6.144131660461426
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 6.820148944854736
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 6.538313865661621
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 5.810006141662598
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 7.386938095092773
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 6.88830041885376
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 5.666451454162598
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 6.722325801849365
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 6.745422840118408
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 6.393380641937256
    },
    {
      "epoch": 0.7015718157181572,
      "grad_norm": 14.996318817138672,
      "learning_rate": 1e-05,
      "loss": 6.5828,
      "step": 3236
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 6.721674919128418
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 3.9432027339935303
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 6.427549839019775
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 6.287612438201904
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 6.272711277008057
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 5.827520847320557
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 7.302346229553223
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 4.708900451660156
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 7.864716053009033
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 7.387694835662842
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 7.573032855987549
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 6.348071575164795
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 7.092475891113281
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 7.081152439117432
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 6.766253471374512
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 7.846553325653076
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 16.756587982177734,
      "learning_rate": 1e-05,
      "loss": 6.5907,
      "step": 3240
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 7.293521404266357
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 6.606555938720703
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 8.045477867126465
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 7.628954887390137
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 7.518701553344727
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 7.886354923248291
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 6.414554119110107
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 7.604092597961426
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 6.692888259887695
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 7.2164225578308105
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 7.049890518188477
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 6.191346168518066
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 7.37471342086792
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 6.114028453826904
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 6.09141206741333
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 8.39107894897461
    },
    {
      "epoch": 0.7033062330623306,
      "grad_norm": 18.653335571289062,
      "learning_rate": 1e-05,
      "loss": 7.1325,
      "step": 3244
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 6.338134288787842
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 6.853118896484375
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 7.82784366607666
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 7.307778358459473
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 7.171164035797119
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 7.180932521820068
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 7.0463056564331055
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 7.031993389129639
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 7.894584655761719
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 6.557762145996094
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 6.2412495613098145
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 5.844135284423828
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 6.122988700866699
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 4.562942028045654
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 6.950515270233154
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 7.619053840637207
    },
    {
      "epoch": 0.7041734417344173,
      "grad_norm": 12.9691801071167,
      "learning_rate": 1e-05,
      "loss": 6.7844,
      "step": 3248
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 7.212632179260254
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 6.214106559753418
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 7.702269554138184
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 6.899062156677246
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 6.1041789054870605
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 7.63043737411499
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 7.083769798278809
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 8.115682601928711
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 4.810502052307129
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 6.922943592071533
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 8.46020221710205
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 5.434011936187744
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 8.339542388916016
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 3.510683059692383
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 8.447375297546387
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 6.190124988555908
    },
    {
      "epoch": 0.7050406504065041,
      "grad_norm": 14.781970977783203,
      "learning_rate": 1e-05,
      "loss": 6.8173,
      "step": 3252
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 6.288792610168457
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 7.4442973136901855
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 5.434338569641113
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 7.132274150848389
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 5.608635425567627
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 6.677182674407959
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 6.506507396697998
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 7.223563194274902
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 7.078792572021484
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 7.157787322998047
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 7.495519161224365
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 6.582047939300537
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 5.721162796020508
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 6.654331684112549
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 7.383026599884033
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 8.420450210571289
    },
    {
      "epoch": 0.7059078590785908,
      "grad_norm": 15.103195190429688,
      "learning_rate": 1e-05,
      "loss": 6.8005,
      "step": 3256
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 4.820809364318848
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 6.800032138824463
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 6.541267395019531
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 5.926365852355957
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 7.943645000457764
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 7.079014301300049
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 6.858544826507568
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 6.647586822509766
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 7.17956018447876
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 7.383320331573486
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 7.154733180999756
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 6.683201789855957
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 6.800457000732422
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 8.098670959472656
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 4.615981101989746
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 6.9328227043151855
    },
    {
      "epoch": 0.7067750677506776,
      "grad_norm": 13.649320602416992,
      "learning_rate": 1e-05,
      "loss": 6.7166,
      "step": 3260
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 7.257878303527832
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 6.754737377166748
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 8.161848068237305
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 6.701808452606201
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 5.797165393829346
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 6.6501688957214355
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 7.3938469886779785
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 7.421245098114014
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 6.488790035247803
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 7.183773517608643
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 5.891415596008301
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 7.18413782119751
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 4.3310770988464355
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 6.416019916534424
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 7.773026466369629
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 6.837095260620117
    },
    {
      "epoch": 0.7076422764227642,
      "grad_norm": 13.416081428527832,
      "learning_rate": 1e-05,
      "loss": 6.7653,
      "step": 3264
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 7.860310077667236
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 7.088172912597656
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 6.912459373474121
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 6.710826873779297
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.602665424346924
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.316051959991455
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.57461404800415
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.790434837341309
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.957414150238037
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.642131805419922
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.43576192855835
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.64175271987915
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 7.085611343383789
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 6.975827693939209
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 5.877704620361328
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 6.469181060791016
    },
    {
      "epoch": 0.7085094850948509,
      "grad_norm": 14.358378410339355,
      "learning_rate": 1e-05,
      "loss": 6.7463,
      "step": 3268
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 3.8790078163146973
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 5.940969467163086
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 7.074763298034668
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 6.4583611488342285
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 4.3082685470581055
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 7.024482250213623
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 6.74722146987915
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 7.395768165588379
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 6.325840473175049
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 7.699467658996582
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 6.900516986846924
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 6.67717981338501
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 5.58432149887085
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 5.2443132400512695
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 5.013676166534424
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 6.525544166564941
    },
    {
      "epoch": 0.7093766937669377,
      "grad_norm": 13.7626314163208,
      "learning_rate": 1e-05,
      "loss": 6.175,
      "step": 3272
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 6.8736572265625
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 7.563358306884766
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 6.120623588562012
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 7.703317642211914
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 6.889551639556885
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 6.10750150680542
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 7.7833251953125
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 8.640586853027344
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 6.517302513122559
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 6.46006965637207
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 5.941053867340088
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 6.526993274688721
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 6.45803689956665
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 6.827693462371826
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 6.656059741973877
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 6.837751865386963
    },
    {
      "epoch": 0.7102439024390244,
      "grad_norm": 10.690014839172363,
      "learning_rate": 1e-05,
      "loss": 6.8692,
      "step": 3276
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 6.610444068908691
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 7.083300590515137
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 7.3036370277404785
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 6.839487075805664
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 5.3689069747924805
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 7.098005294799805
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 8.360620498657227
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 6.98538875579834
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 6.8987274169921875
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 6.764351844787598
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 7.256262302398682
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 6.865817546844482
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 7.895149230957031
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 6.058810234069824
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 5.2608962059021
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 6.777246475219727
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 10.94123363494873,
      "learning_rate": 1e-05,
      "loss": 6.8392,
      "step": 3280
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 6.285976409912109
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 7.345956802368164
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 7.638879776000977
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 7.589066982269287
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 6.782820701599121
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 7.433107852935791
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 7.99470329284668
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 6.52367639541626
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 7.65927267074585
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 8.266326904296875
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 6.187539577484131
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 6.757997035980225
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 7.632033824920654
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 7.54628849029541
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 7.692299842834473
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 7.798202991485596
    },
    {
      "epoch": 0.7119783197831978,
      "grad_norm": 14.630694389343262,
      "learning_rate": 1e-05,
      "loss": 7.3209,
      "step": 3284
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 7.554971218109131
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 7.525566577911377
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 5.458393096923828
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 6.914206027984619
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 4.301344394683838
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 6.602536678314209
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 6.160237789154053
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 7.184023380279541
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 6.828799724578857
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 6.370105743408203
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 7.085318565368652
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 5.871897220611572
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 6.939517021179199
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 7.112764835357666
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 7.4045515060424805
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 6.142845153808594
    },
    {
      "epoch": 0.7128455284552846,
      "grad_norm": 18.13207244873047,
      "learning_rate": 1e-05,
      "loss": 6.5911,
      "step": 3288
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 4.790769577026367
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 7.081507205963135
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 7.092226505279541
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 6.999680519104004
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 7.067401885986328
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 6.6519036293029785
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 5.975767612457275
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 7.062065601348877
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.029797554016113
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.329441070556641
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.897284030914307
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 7.101715564727783
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 7.113242149353027
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 5.980326175689697
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 8.126899719238281
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 6.710862636566162
    },
    {
      "epoch": 0.7137127371273713,
      "grad_norm": 11.69426441192627,
      "learning_rate": 1e-05,
      "loss": 6.6882,
      "step": 3292
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 7.66313362121582
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 6.36634635925293
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 6.660760402679443
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 7.2049431800842285
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 7.29351282119751
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 7.600393295288086
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 8.008204460144043
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 7.158373832702637
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 6.720094680786133
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 7.071234226226807
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 7.150592803955078
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 7.846958637237549
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 7.133363723754883
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 7.543365001678467
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 6.233331203460693
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 7.193020820617676
    },
    {
      "epoch": 0.714579945799458,
      "grad_norm": 12.096477508544922,
      "learning_rate": 1e-05,
      "loss": 7.178,
      "step": 3296
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 6.723742485046387
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 7.508169174194336
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 7.638166904449463
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 8.01620101928711
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 6.329728126525879
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 6.930055141448975
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 4.193840980529785
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 5.312222003936768
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 7.344417572021484
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 6.813701152801514
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 7.314288139343262
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 9.157548904418945
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 5.964426517486572
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 7.212848663330078
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 7.859318256378174
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 6.226551532745361
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 16.634138107299805,
      "learning_rate": 1e-05,
      "loss": 6.9091,
      "step": 3300
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 6.091594219207764
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 7.299543380737305
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 6.14338493347168
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 7.17392110824585
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 6.584503173828125
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 6.436384201049805
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 6.850484848022461
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 4.917886734008789
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 6.94952392578125
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 6.990874767303467
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 6.8231635093688965
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 5.9800124168396
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 6.976663589477539
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 6.234786510467529
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 7.107478618621826
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 7.4872660636901855
    },
    {
      "epoch": 0.7163143631436314,
      "grad_norm": 17.183536529541016,
      "learning_rate": 1e-05,
      "loss": 6.628,
      "step": 3304
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 6.16840934753418
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 4.608920574188232
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 6.104922771453857
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 5.7775092124938965
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 7.651617050170898
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 6.468716621398926
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 7.128983497619629
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 7.498589038848877
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 5.957456111907959
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 7.38035249710083
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 4.949130058288574
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 6.967103481292725
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 7.817656517028809
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 7.821306228637695
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 6.63667631149292
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 4.890369892120361
    },
    {
      "epoch": 0.7171815718157182,
      "grad_norm": 15.784586906433105,
      "learning_rate": 1e-05,
      "loss": 6.4892,
      "step": 3308
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 6.111171245574951
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 6.3348388671875
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 7.240790843963623
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 6.807145595550537
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 4.122321128845215
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 6.781018257141113
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 6.951457500457764
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 6.129864692687988
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 5.977646350860596
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 7.585620880126953
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 7.342462062835693
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 5.326567649841309
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 6.495797157287598
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 4.938092231750488
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 7.679841041564941
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 6.119091033935547
    },
    {
      "epoch": 0.7180487804878048,
      "grad_norm": 15.857287406921387,
      "learning_rate": 1e-05,
      "loss": 6.3715,
      "step": 3312
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 8.172699928283691
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 7.647010803222656
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 7.502365589141846
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 7.638204097747803
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 7.0125885009765625
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 6.766730308532715
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 4.704744338989258
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 6.329516410827637
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 6.965349197387695
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 7.534013748168945
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 5.956212520599365
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 6.189192295074463
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 5.209110736846924
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 6.170027732849121
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 5.830406188964844
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 6.445044040679932
    },
    {
      "epoch": 0.7189159891598916,
      "grad_norm": 13.3307523727417,
      "learning_rate": 1e-05,
      "loss": 6.6296,
      "step": 3316
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 7.549030780792236
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 6.751072883605957
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 6.521438121795654
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 8.292707443237305
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 5.853397846221924
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 7.9985761642456055
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 6.142205715179443
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 6.3216633796691895
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 6.540398120880127
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 6.523340225219727
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 8.017483711242676
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 7.096132755279541
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 6.665127754211426
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 6.29838228225708
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 6.709503650665283
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 7.1383748054504395
    },
    {
      "epoch": 0.7197831978319783,
      "grad_norm": 13.837041854858398,
      "learning_rate": 1e-05,
      "loss": 6.9012,
      "step": 3320
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 5.3187079429626465
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 5.447690010070801
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 6.603118419647217
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 6.069879055023193
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 7.681085586547852
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 8.0088529586792
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 4.507986545562744
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 7.562960147857666
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 7.68611478805542
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 7.3891282081604
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 6.680023193359375
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 6.291396141052246
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 7.890863418579102
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 7.767083168029785
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 6.0570268630981445
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 6.570468902587891
    },
    {
      "epoch": 0.7206504065040651,
      "grad_norm": 15.458418846130371,
      "learning_rate": 1e-05,
      "loss": 6.7208,
      "step": 3324
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 5.573493003845215
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 7.106578826904297
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 7.217565536499023
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 6.767719268798828
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.127771854400635
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.602511405944824
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 7.359700679779053
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.119728088378906
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 6.187170505523682
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 5.482888698577881
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 6.709680080413818
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 6.582359313964844
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 7.3036088943481445
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 6.570519924163818
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 6.044473648071289
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 5.760034561157227
    },
    {
      "epoch": 0.7215176151761518,
      "grad_norm": 16.952552795410156,
      "learning_rate": 1e-05,
      "loss": 6.4697,
      "step": 3328
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 7.148401737213135
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 7.3658857345581055
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 6.994781970977783
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 7.686416149139404
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 6.997875690460205
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 5.065306663513184
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 6.536200046539307
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 7.440705299377441
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 4.95815896987915
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 6.9981369972229
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 6.648807525634766
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 6.081752777099609
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 7.260491847991943
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 7.008841514587402
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 7.675246715545654
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 7.273725509643555
    },
    {
      "epoch": 0.7223848238482384,
      "grad_norm": 12.51523494720459,
      "learning_rate": 1e-05,
      "loss": 6.8213,
      "step": 3332
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 6.963753700256348
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 7.004157543182373
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 6.810487270355225
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 8.083723068237305
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 4.7542033195495605
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 7.405125617980957
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 7.858829975128174
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 6.920883655548096
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 7.703713893890381
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 6.546367168426514
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 6.520933151245117
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 4.046664714813232
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 5.284708499908447
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 5.378173351287842
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 6.147475719451904
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 6.681868076324463
    },
    {
      "epoch": 0.7232520325203252,
      "grad_norm": 15.79376220703125,
      "learning_rate": 1e-05,
      "loss": 6.5069,
      "step": 3336
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 6.093089580535889
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 7.604656219482422
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 6.89963960647583
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 6.121476650238037
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 5.66852331161499
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 7.89412260055542
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 6.335219383239746
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 6.739997863769531
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 6.401448726654053
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 7.369812965393066
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 5.90264368057251
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 5.753705978393555
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 5.876840591430664
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 7.565094470977783
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 7.742188453674316
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 6.962587356567383
    },
    {
      "epoch": 0.7241192411924119,
      "grad_norm": 15.406346321105957,
      "learning_rate": 1e-05,
      "loss": 6.6832,
      "step": 3340
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 7.260135173797607
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 7.968270301818848
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 7.1819987297058105
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 6.779653549194336
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 7.982766151428223
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 7.18123197555542
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 8.084580421447754
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 6.780575275421143
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.110054969787598
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 7.957241535186768
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 5.6499457359313965
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.719837665557861
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 7.161783695220947
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 6.518682479858398
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 7.084080696105957
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 8.3167142868042
    },
    {
      "epoch": 0.7249864498644987,
      "grad_norm": 19.160987854003906,
      "learning_rate": 1e-05,
      "loss": 7.1711,
      "step": 3344
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 7.2245259284973145
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 5.506545066833496
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 6.661006927490234
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 7.092048645019531
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 6.358613967895508
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 6.889759063720703
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 7.582494258880615
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 6.233333587646484
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 6.995699405670166
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 6.619259834289551
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 5.376436233520508
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 6.9589457511901855
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 7.036911487579346
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 6.3350982666015625
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 5.9885406494140625
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 7.555487155914307
    },
    {
      "epoch": 0.7258536585365853,
      "grad_norm": 11.802306175231934,
      "learning_rate": 1e-05,
      "loss": 6.6509,
      "step": 3348
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 6.929625511169434
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 9.002889633178711
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 7.337357997894287
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 7.865854263305664
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 4.75073766708374
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 7.05987548828125
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 6.0598859786987305
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 7.3927836418151855
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 8.006712913513184
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 7.738126754760742
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 6.834986686706543
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 6.338654518127441
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.0597147941589355
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.733717918395996
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.757967472076416
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.0692877769470215
    },
    {
      "epoch": 0.7267208672086721,
      "grad_norm": 21.294822692871094,
      "learning_rate": 1e-05,
      "loss": 6.9336,
      "step": 3352
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.444092273712158
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 6.828721523284912
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.01896858215332
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.132332801818848
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 4.723159313201904
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 6.899528980255127
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 6.673337459564209
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 6.420903205871582
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 7.416745185852051
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 6.257076263427734
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 6.709315776824951
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 9.79577350616455
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 6.053647518157959
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 8.67636775970459
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 5.282255172729492
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 7.566162109375
    },
    {
      "epoch": 0.7275880758807588,
      "grad_norm": 13.931527137756348,
      "learning_rate": 1e-05,
      "loss": 6.9311,
      "step": 3356
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 5.488318920135498
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 6.814448833465576
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 5.047756671905518
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 6.607519149780273
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 7.033056735992432
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 8.017457008361816
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 4.161733627319336
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 7.368022918701172
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 4.549083709716797
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 6.927207946777344
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 6.702557563781738
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 6.740141868591309
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 6.394684314727783
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 4.520086288452148
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 6.280473232269287
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 8.302966117858887
    },
    {
      "epoch": 0.7284552845528456,
      "grad_norm": 18.06097412109375,
      "learning_rate": 1e-05,
      "loss": 6.3097,
      "step": 3360
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 6.204003810882568
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 7.469984531402588
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 5.910090446472168
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 6.706489086151123
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 7.279725551605225
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 6.4793596267700195
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 7.306637287139893
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 5.500570297241211
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 6.440319061279297
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 7.269827365875244
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 7.061553955078125
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 7.88242244720459
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 7.110286712646484
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 7.806362628936768
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 7.486142635345459
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 6.677070617675781
    },
    {
      "epoch": 0.7293224932249323,
      "grad_norm": 14.949912071228027,
      "learning_rate": 1e-05,
      "loss": 6.9119,
      "step": 3364
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 6.677322864532471
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 6.036816120147705
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 4.440389633178711
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 5.449214935302734
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 3.933384895324707
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 7.119516849517822
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 6.745733261108398
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 7.828314304351807
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 7.085655689239502
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 6.996811866760254
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 6.8332743644714355
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 6.921618461608887
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 6.874229431152344
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 7.62389612197876
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 8.034796714782715
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 7.717001438140869
    },
    {
      "epoch": 0.7301897018970189,
      "grad_norm": 26.63125228881836,
      "learning_rate": 1e-05,
      "loss": 6.6449,
      "step": 3368
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 5.072106838226318
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 7.008825778961182
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 9.507524490356445
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 6.832985877990723
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 6.721214771270752
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 5.169590950012207
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 6.646645545959473
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 5.636688232421875
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 7.101596832275391
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 6.723128795623779
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 5.966545581817627
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 7.396427154541016
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 7.653599739074707
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 6.919116497039795
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 7.863467693328857
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 6.74593448638916
    },
    {
      "epoch": 0.7310569105691057,
      "grad_norm": 14.480169296264648,
      "learning_rate": 1e-05,
      "loss": 6.8103,
      "step": 3372
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 8.60947322845459
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 8.071161270141602
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 7.2001566886901855
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 8.093914031982422
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.653173923492432
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.377368927001953
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.857308864593506
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.50447940826416
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 7.567777633666992
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 6.0613112449646
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 6.830347061157227
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 6.411806583404541
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 6.6913161277771
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 6.52271032333374
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 6.337671756744385
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 7.503272533416748
    },
    {
      "epoch": 0.7319241192411924,
      "grad_norm": 11.608097076416016,
      "learning_rate": 1e-05,
      "loss": 7.0183,
      "step": 3376
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 7.198551654815674
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 5.272755146026611
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 8.764835357666016
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 6.745051383972168
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 6.38339376449585
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 7.224996566772461
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 6.967336654663086
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 7.029544830322266
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 8.674359321594238
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 6.661015033721924
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 7.129473686218262
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 7.349859714508057
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 6.156205177307129
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 6.511218547821045
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 7.777262210845947
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 8.57578182220459
    },
    {
      "epoch": 0.7327913279132792,
      "grad_norm": 15.714985847473145,
      "learning_rate": 1e-05,
      "loss": 7.1514,
      "step": 3380
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 7.280941009521484
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 7.239181995391846
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 6.1868205070495605
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 8.19788932800293
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 6.262342929840088
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 7.720124244689941
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 7.144593715667725
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 5.724450588226318
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 4.2768635749816895
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 4.14863395690918
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 6.036834239959717
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 6.788954257965088
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 7.351738452911377
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 5.685083866119385
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 6.380725860595703
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 7.21791410446167
    },
    {
      "epoch": 0.7336585365853658,
      "grad_norm": 15.62509536743164,
      "learning_rate": 1e-05,
      "loss": 6.4777,
      "step": 3384
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 6.840036392211914
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 5.361733436584473
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 5.534323215484619
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 6.341242790222168
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 8.035676956176758
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 6.834462642669678
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 7.80925178527832
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 7.11088228225708
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 6.7587409019470215
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 7.263095378875732
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 6.75362491607666
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 7.944533824920654
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 5.2087507247924805
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 6.859724521636963
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 6.454679489135742
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 7.102622032165527
    },
    {
      "epoch": 0.7345257452574526,
      "grad_norm": 14.221355438232422,
      "learning_rate": 1e-05,
      "loss": 6.7633,
      "step": 3388
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 6.1575398445129395
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 7.377570629119873
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 6.802182197570801
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 7.0963239669799805
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 6.761874675750732
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 6.537649154663086
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 7.023806095123291
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 5.6473517417907715
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 5.945401191711426
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 5.872385501861572
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 6.558958530426025
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 6.517328262329102
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 6.6768798828125
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 7.614936828613281
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 7.799166202545166
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 6.664029598236084
    },
    {
      "epoch": 0.7353929539295393,
      "grad_norm": 15.259177207946777,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 3392
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 7.164586067199707
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 7.34160852432251
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 7.004995822906494
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 6.570675373077393
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 6.190585613250732
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 5.9918646812438965
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 6.135822296142578
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 6.0048136711120605
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 5.145915985107422
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 7.557599067687988
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 7.379264831542969
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 6.813363075256348
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.757209777832031
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.5287251472473145
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.8968915939331055
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.230513572692871
    },
    {
      "epoch": 0.736260162601626,
      "grad_norm": 13.81402587890625,
      "learning_rate": 1e-05,
      "loss": 6.6072,
      "step": 3396
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 6.5161566734313965
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 8.511812210083008
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 7.188973903656006
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 7.015244007110596
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 5.8144073486328125
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 6.871386528015137
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 7.5026469230651855
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 6.890693187713623
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 7.037573337554932
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 7.81972074508667
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 7.203583240509033
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 7.176076889038086
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 7.321051120758057
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 7.4436187744140625
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 6.934223175048828
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 5.629475116729736
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 13.41969108581543,
      "learning_rate": 1e-05,
      "loss": 7.0548,
      "step": 3400
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 6.984321117401123
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 7.489194869995117
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 6.37110710144043
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 6.9622063636779785
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 6.965167999267578
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 8.09913444519043
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 6.604910850524902
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 5.471158027648926
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 6.637260437011719
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 6.8614068031311035
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 6.372348308563232
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 3.8202250003814697
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.6064910888671875
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.487419605255127
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.902987480163574
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 5.963483810424805
    },
    {
      "epoch": 0.7379945799457994,
      "grad_norm": 13.111842155456543,
      "learning_rate": 1e-05,
      "loss": 6.5374,
      "step": 3404
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 5.666539192199707
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 6.722813606262207
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 7.659843921661377
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 7.138172149658203
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 5.458805084228516
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 6.362800121307373
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 7.81609582901001
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 4.450840473175049
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.973498344421387
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.855433940887451
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.973735809326172
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 5.816617012023926
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 7.909529685974121
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 6.906761169433594
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 6.244825839996338
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 6.2328667640686035
    },
    {
      "epoch": 0.7388617886178862,
      "grad_norm": 17.229026794433594,
      "learning_rate": 1e-05,
      "loss": 6.5743,
      "step": 3408
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 5.783202171325684
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 6.903456687927246
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 6.6620941162109375
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 5.914115905761719
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 6.0559468269348145
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 7.9906511306762695
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 7.036901950836182
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 6.782407760620117
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 6.878818988800049
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 6.974152565002441
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 4.001099109649658
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 7.459590435028076
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 5.839109420776367
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 7.562864303588867
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 7.362908840179443
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 6.563493728637695
    },
    {
      "epoch": 0.7397289972899729,
      "grad_norm": 20.204233169555664,
      "learning_rate": 1e-05,
      "loss": 6.6107,
      "step": 3412
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 5.852397918701172
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 6.941450595855713
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 7.399586200714111
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 10.374917030334473
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 6.487245082855225
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 6.597123146057129
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 6.89003324508667
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 6.648542404174805
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 6.455105304718018
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 5.326300621032715
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 6.018863201141357
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 7.549676418304443
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 6.94221305847168
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 6.734075546264648
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 6.942083358764648
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 7.269373416900635
    },
    {
      "epoch": 0.7405962059620597,
      "grad_norm": 16.43470001220703,
      "learning_rate": 1e-05,
      "loss": 6.9018,
      "step": 3416
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 6.549410820007324
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 6.115475177764893
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 6.52970027923584
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 7.3771562576293945
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 7.693464756011963
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 6.855691909790039
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 7.610246658325195
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 8.412367820739746
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 7.3028435707092285
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 5.998562335968018
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 7.1189165115356445
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 5.833676815032959
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 7.981201171875
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 6.882123947143555
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 5.646326541900635
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 6.31149435043335
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 13.678523063659668,
      "learning_rate": 1e-05,
      "loss": 6.8887,
      "step": 3420
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.5337371826171875
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.00402307510376
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.308234691619873
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.169408321380615
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 6.717641830444336
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 6.4827961921691895
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 8.23169994354248
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 6.837252140045166
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 5.882997512817383
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 8.050235748291016
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 7.468235492706299
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 5.296477794647217
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.496669292449951
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 7.218963623046875
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 4.081733226776123
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.946385383605957
    },
    {
      "epoch": 0.7423306233062331,
      "grad_norm": 13.951834678649902,
      "learning_rate": 1e-05,
      "loss": 6.7954,
      "step": 3424
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 6.941123962402344
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 7.012242317199707
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 6.974539279937744
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 7.58628511428833
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 5.770778656005859
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 6.957424163818359
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 7.085047721862793
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 6.0125627517700195
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 6.11157751083374
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 6.970773220062256
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 7.2385640144348145
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 4.4155592918396
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 7.214015483856201
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 7.422475337982178
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 6.1391401290893555
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 6.929595470428467
    },
    {
      "epoch": 0.7431978319783198,
      "grad_norm": 12.722206115722656,
      "learning_rate": 1e-05,
      "loss": 6.6739,
      "step": 3428
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 6.35740852355957
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 6.194307804107666
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 7.762943744659424
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 5.9255523681640625
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 6.556663513183594
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 6.470245838165283
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 7.188589096069336
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 7.256097793579102
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 6.6418256759643555
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 6.674935817718506
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 7.697142124176025
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 7.243182182312012
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 7.136287689208984
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 6.050233364105225
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 6.929582118988037
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 6.068004608154297
    },
    {
      "epoch": 0.7440650406504065,
      "grad_norm": 19.482973098754883,
      "learning_rate": 1e-05,
      "loss": 6.7596,
      "step": 3432
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 7.439026355743408
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 8.493302345275879
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 6.220556735992432
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 6.91301965713501
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 7.075578212738037
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 7.209280967712402
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 6.9769744873046875
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 6.830937385559082
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 7.9459757804870605
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 7.239011287689209
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 6.123220920562744
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 7.453268051147461
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 8.080204010009766
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 6.888296127319336
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 7.120974063873291
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 9.224213600158691
    },
    {
      "epoch": 0.7449322493224932,
      "grad_norm": 18.907304763793945,
      "learning_rate": 1e-05,
      "loss": 7.3271,
      "step": 3436
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 5.805942535400391
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 9.411150932312012
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 5.979994297027588
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 5.694899082183838
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 6.879204273223877
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 6.324708461761475
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 6.637026786804199
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 5.345198154449463
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 7.482656002044678
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 6.776343822479248
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 6.618833065032959
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 6.880126476287842
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.444680690765381
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 5.686013698577881
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.745988368988037
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 8.128207206726074
    },
    {
      "epoch": 0.7457994579945799,
      "grad_norm": 11.981847763061523,
      "learning_rate": 1e-05,
      "loss": 6.6776,
      "step": 3440
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 6.880577564239502
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 6.64906644821167
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 7.631837844848633
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 7.440202713012695
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 5.36706018447876
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 6.9291276931762695
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 6.61174201965332
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 4.497148036956787
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 7.3219146728515625
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 6.72227144241333
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 7.0656633377075195
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 7.36624002456665
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 3.8702213764190674
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 7.196200370788574
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 6.701779842376709
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 6.659112453460693
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 13.782434463500977,
      "learning_rate": 1e-05,
      "loss": 6.5569,
      "step": 3444
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 5.867447376251221
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 6.425840854644775
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 6.936200141906738
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 8.054844856262207
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 4.498712062835693
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 7.47820520401001
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 5.436869144439697
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 6.4985737800598145
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 5.836691856384277
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 5.234186172485352
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 5.102415084838867
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 7.084846019744873
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 6.913100719451904
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 7.204523086547852
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 7.047781944274902
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 7.3730669021606445
    },
    {
      "epoch": 0.7475338753387534,
      "grad_norm": 12.01313304901123,
      "learning_rate": 1e-05,
      "loss": 6.4371,
      "step": 3448
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 8.035099983215332
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 6.064810276031494
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 6.364290237426758
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 6.58661413192749
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 6.605403900146484
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 7.060616493225098
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 8.402753829956055
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 5.447113037109375
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 8.230986595153809
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 6.696675777435303
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 5.314293384552002
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 6.165225028991699
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 6.583657741546631
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 6.229305744171143
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 5.838199615478516
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 6.834482669830322
    },
    {
      "epoch": 0.7484010840108402,
      "grad_norm": 13.923524856567383,
      "learning_rate": 1e-05,
      "loss": 6.6537,
      "step": 3452
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 6.936347961425781
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 6.598989486694336
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 5.395073890686035
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 6.9992804527282715
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 7.157163143157959
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 8.078304290771484
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 7.028230667114258
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 7.66440486907959
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 7.602042198181152
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 7.500716209411621
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 7.286951065063477
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 7.770499229431152
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 7.1967267990112305
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 6.325648784637451
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 6.222370624542236
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 6.270505428314209
    },
    {
      "epoch": 0.7492682926829268,
      "grad_norm": 13.649347305297852,
      "learning_rate": 1e-05,
      "loss": 7.0021,
      "step": 3456
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 7.801323413848877
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 7.773441314697266
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 7.117019176483154
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 6.746940612792969
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 7.271656513214111
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 5.525252342224121
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 7.726458549499512
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 6.070897102355957
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 7.032258987426758
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 6.965558052062988
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 6.3341383934021
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 6.176686763763428
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 7.71970796585083
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 7.331247806549072
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 7.03146505355835
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 7.674808502197266
    },
    {
      "epoch": 0.7501355013550135,
      "grad_norm": 12.651094436645508,
      "learning_rate": 1e-05,
      "loss": 7.0187,
      "step": 3460
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.183168411254883
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.040289402008057
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.779841899871826
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.115227699279785
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 6.502120494842529
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 6.365252494812012
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 6.315873622894287
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 5.084397315979004
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 6.247817516326904
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 7.184713363647461
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 6.374581336975098
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 6.905489921569824
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 5.035872936248779
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 5.769308090209961
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 6.794219017028809
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 6.277885437011719
    },
    {
      "epoch": 0.7510027100271003,
      "grad_norm": 16.034700393676758,
      "learning_rate": 1e-05,
      "loss": 6.2485,
      "step": 3464
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 7.128399848937988
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 8.258888244628906
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 6.3118181228637695
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 6.756279945373535
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 6.3017497062683105
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 5.921316623687744
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 6.168540000915527
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 7.934200286865234
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 7.3084259033203125
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 8.325141906738281
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 6.802872657775879
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 6.753945350646973
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 6.944657325744629
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 7.5446319580078125
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 7.1698713302612305
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 6.795557975769043
    },
    {
      "epoch": 0.751869918699187,
      "grad_norm": 15.607582092285156,
      "learning_rate": 1e-05,
      "loss": 7.0266,
      "step": 3468
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 7.515823841094971
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 6.968005180358887
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 5.010869026184082
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 6.011822700500488
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 5.153698444366455
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 8.055334091186523
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 6.816886901855469
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 7.044361591339111
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 3.356858730316162
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 5.645941257476807
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 7.134970664978027
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 7.234687328338623
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 7.114287853240967
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 6.27345609664917
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 7.308409690856934
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 7.33633279800415
    },
    {
      "epoch": 0.7527371273712737,
      "grad_norm": 11.676203727722168,
      "learning_rate": 1e-05,
      "loss": 6.4989,
      "step": 3472
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 6.646615028381348
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 7.241799354553223
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 5.293205738067627
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 7.874599933624268
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 5.856523036956787
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 5.914767265319824
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 6.291163444519043
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 7.279372215270996
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 7.3620500564575195
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 7.9398088455200195
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 6.371486186981201
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 7.0648088455200195
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 6.285707950592041
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 6.303462982177734
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 7.537223815917969
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 5.92971134185791
    },
    {
      "epoch": 0.7536043360433604,
      "grad_norm": 16.83917236328125,
      "learning_rate": 1e-05,
      "loss": 6.6995,
      "step": 3476
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 6.8764967918396
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 7.251805305480957
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 6.730321884155273
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 5.472437381744385
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 6.731786251068115
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 5.947966575622559
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 6.544642448425293
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 7.450783729553223
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 8.030323028564453
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 7.526453971862793
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 6.985428333282471
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 8.727869987487793
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 6.935243129730225
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 5.795629501342773
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 7.859811305999756
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 6.7248053550720215
    },
    {
      "epoch": 0.7544715447154472,
      "grad_norm": 13.320108413696289,
      "learning_rate": 1e-05,
      "loss": 6.9745,
      "step": 3480
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 6.715122222900391
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 6.53743314743042
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 8.251696586608887
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 6.76814603805542
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 7.599170684814453
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 6.413508892059326
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 8.303985595703125
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 6.969612121582031
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 8.401510238647461
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 7.338431358337402
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 6.438366889953613
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 6.760756969451904
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 7.6503424644470215
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 6.243679046630859
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 6.61000394821167
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 6.118446350097656
    },
    {
      "epoch": 0.7553387533875339,
      "grad_norm": 11.243555068969727,
      "learning_rate": 1e-05,
      "loss": 7.07,
      "step": 3484
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 7.017316818237305
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 8.331842422485352
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 7.241495132446289
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 7.829176902770996
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.904023170471191
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.49179744720459
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.289264678955078
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.78740930557251
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 7.121612071990967
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 5.754706382751465
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 7.156157970428467
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 6.9286017417907715
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 6.741555213928223
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 6.8228068351745605
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 7.275869846343994
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 7.09585428237915
    },
    {
      "epoch": 0.7562059620596207,
      "grad_norm": 11.894326210021973,
      "learning_rate": 1e-05,
      "loss": 6.9868,
      "step": 3488
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 7.192660331726074
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 7.593531608581543
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 6.667202949523926
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 5.885546684265137
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 7.129655838012695
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 7.01845121383667
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 6.903065204620361
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 9.7022123336792
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 5.6076130867004395
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 6.977901935577393
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 6.910799503326416
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 6.253021240234375
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 6.5322065353393555
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 7.082671165466309
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 8.001763343811035
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 6.9189910888671875
    },
    {
      "epoch": 0.7570731707317073,
      "grad_norm": 16.652849197387695,
      "learning_rate": 1e-05,
      "loss": 7.0236,
      "step": 3492
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 7.259779930114746
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 6.6657562255859375
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 6.034444332122803
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 7.05380916595459
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 4.675399303436279
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 6.618626117706299
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 8.634779930114746
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 8.018181800842285
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.193655014038086
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.707522869110107
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 5.952290058135986
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 5.558384418487549
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 6.887099742889404
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 4.852156162261963
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 5.901098251342773
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 7.23435115814209
    },
    {
      "epoch": 0.757940379403794,
      "grad_norm": 16.379528045654297,
      "learning_rate": 1e-05,
      "loss": 6.5155,
      "step": 3496
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 3.6448545455932617
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 5.683452606201172
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 6.580270290374756
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 6.6195969581604
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 6.963014602661133
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 6.891229152679443
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 7.238409042358398
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 6.636824131011963
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 7.314892292022705
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 6.11826229095459
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 7.086680889129639
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 7.28303861618042
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 7.150570392608643
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 7.113933086395264
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 7.203548908233643
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 6.515158176422119
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 16.0262451171875,
      "learning_rate": 1e-05,
      "loss": 6.6277,
      "step": 3500
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 6.517780303955078
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 6.59035062789917
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 7.331584930419922
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 6.863316059112549
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 4.552422046661377
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 6.198561191558838
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 6.759405136108398
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 6.984805583953857
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 7.41951322555542
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 6.331705570220947
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 8.861074447631836
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 7.004081726074219
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 6.580770492553711
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 10.339493751525879
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 7.0384931564331055
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 7.589338302612305
    },
    {
      "epoch": 0.7596747967479675,
      "grad_norm": 23.03581428527832,
      "learning_rate": 1e-05,
      "loss": 7.0602,
      "step": 3504
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 8.275270462036133
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 6.723049163818359
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 7.873100280761719
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 7.100693702697754
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 6.341540813446045
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 4.080327987670898
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 4.1773247718811035
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 6.8952507972717285
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 4.717340469360352
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 5.962658405303955
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 4.794768810272217
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 6.630721092224121
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 7.038177013397217
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 7.413748741149902
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 8.354008674621582
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 7.426471710205078
    },
    {
      "epoch": 0.7605420054200542,
      "grad_norm": 11.616175651550293,
      "learning_rate": 1e-05,
      "loss": 6.4878,
      "step": 3508
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 8.226415634155273
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 6.633303165435791
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 7.511622905731201
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 6.893072128295898
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 6.996702671051025
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 7.128098487854004
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 6.850664138793945
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 5.5399699211120605
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 7.725648403167725
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 7.967281818389893
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 7.117379665374756
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 6.537217617034912
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 5.7841997146606445
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 7.344322681427002
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 6.825120449066162
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 7.316498756408691
    },
    {
      "epoch": 0.7614092140921409,
      "grad_norm": 14.560534477233887,
      "learning_rate": 1e-05,
      "loss": 7.0248,
      "step": 3512
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 4.745868682861328
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 5.702034950256348
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 3.903911590576172
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 6.176066875457764
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 7.12335205078125
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 6.271625995635986
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 7.208487033843994
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 5.8019890785217285
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 6.911231517791748
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 7.2302021980285645
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 8.105854988098145
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 6.090840816497803
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 6.370438575744629
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 6.616978168487549
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 5.869658470153809
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 5.826816082000732
    },
    {
      "epoch": 0.7622764227642277,
      "grad_norm": 14.683398246765137,
      "learning_rate": 1e-05,
      "loss": 6.2472,
      "step": 3516
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 6.748804569244385
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 7.9349517822265625
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 6.452507019042969
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 6.954674243927002
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.7006072998046875
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.681740760803223
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.062130928039551
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.63726806640625
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 7.2935075759887695
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 7.045524597167969
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 6.724793434143066
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 7.656362533569336
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 6.82827615737915
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 7.599297523498535
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 7.238986968994141
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 7.507319450378418
    },
    {
      "epoch": 0.7631436314363144,
      "grad_norm": 13.33966064453125,
      "learning_rate": 1e-05,
      "loss": 7.2542,
      "step": 3520
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 5.647104263305664
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 5.269036293029785
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 6.200883388519287
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 6.959915637969971
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 7.821142196655273
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 6.604280948638916
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 4.604735374450684
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 7.3409647941589355
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.721080303192139
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.36650276184082
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.745190620422363
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.075213432312012
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 7.378561973571777
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 7.025827407836914
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 7.089361667633057
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 6.790378570556641
    },
    {
      "epoch": 0.764010840108401,
      "grad_norm": 13.24052906036377,
      "learning_rate": 1e-05,
      "loss": 6.54,
      "step": 3524
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 6.8436994552612305
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 5.567415714263916
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 5.753048896789551
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 6.561251640319824
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 7.068797588348389
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 7.998579502105713
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 5.613437652587891
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 6.645345211029053
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 6.300825595855713
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 6.135466575622559
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 7.23946475982666
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 7.7218499183654785
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 7.13036584854126
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 7.47707986831665
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 6.796119213104248
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 4.733854293823242
    },
    {
      "epoch": 0.7648780487804878,
      "grad_norm": 16.889009475708008,
      "learning_rate": 1e-05,
      "loss": 6.5992,
      "step": 3528
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 5.9270124435424805
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 5.129350185394287
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 5.544180393218994
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 6.4879350662231445
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 7.209420680999756
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 4.619874954223633
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 6.290703773498535
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 5.3089823722839355
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 7.211405277252197
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 8.053813934326172
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 7.291547775268555
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 6.151996612548828
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 6.919955253601074
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 6.192776203155518
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 7.007551193237305
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 6.41413688659668
    },
    {
      "epoch": 0.7657452574525745,
      "grad_norm": 13.679954528808594,
      "learning_rate": 1e-05,
      "loss": 6.36,
      "step": 3532
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 6.177062034606934
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 6.961019515991211
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 5.680210590362549
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 6.469544887542725
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 7.332186698913574
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 6.483574390411377
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 7.0290327072143555
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 7.470130443572998
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 7.391110897064209
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 7.688634872436523
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 7.416210651397705
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 8.40161418914795
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 6.844468593597412
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 7.013255596160889
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 4.226550579071045
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 4.484894752502441
    },
    {
      "epoch": 0.7666124661246613,
      "grad_norm": 17.622896194458008,
      "learning_rate": 1e-05,
      "loss": 6.6918,
      "step": 3536
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.250907897949219
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.888041019439697
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.263441562652588
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 7.096538066864014
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.568143844604492
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.7443528175354
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 7.3669114112854
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.496252059936523
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 7.377636909484863
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 7.377486705780029
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 6.757935047149658
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 5.3285813331604
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 7.847687244415283
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 7.168097972869873
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 7.6819024085998535
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 7.4614667892456055
    },
    {
      "epoch": 0.767479674796748,
      "grad_norm": 21.468040466308594,
      "learning_rate": 1e-05,
      "loss": 6.9172,
      "step": 3540
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 7.913746356964111
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 5.611222743988037
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 6.706326961517334
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 7.065804481506348
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 7.733999252319336
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 7.6383957862854
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 7.127615928649902
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 7.142899990081787
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 7.4692769050598145
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.914478302001953
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.968459606170654
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.7628045082092285
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 6.963676452636719
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 6.196025848388672
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 7.761689186096191
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 6.776766300201416
    },
    {
      "epoch": 0.7683468834688347,
      "grad_norm": 16.225297927856445,
      "learning_rate": 1e-05,
      "loss": 7.0471,
      "step": 3544
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 6.7666754722595215
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 5.450312614440918
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 8.464946746826172
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 6.313307762145996
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 5.824222564697266
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 5.364648342132568
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 11.180076599121094
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 6.742505073547363
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 6.625804901123047
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 6.887735843658447
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 7.0915117263793945
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 7.6088457107543945
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 6.688697814941406
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 6.086038112640381
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 6.018669128417969
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 7.809135913848877
    },
    {
      "epoch": 0.7692140921409214,
      "grad_norm": 20.37356185913086,
      "learning_rate": 1e-05,
      "loss": 6.9327,
      "step": 3548
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 5.779708385467529
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 6.436683654785156
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 8.253190040588379
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 7.659493923187256
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 6.070195198059082
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 7.655609607696533
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 6.902717113494873
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 7.779636383056641
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.353557586669922
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.633029937744141
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.248600006103516
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.776341438293457
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 5.798938751220703
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 6.046056270599365
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 5.730094909667969
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 6.540633201599121
    },
    {
      "epoch": 0.7700813008130082,
      "grad_norm": 13.214437484741211,
      "learning_rate": 1e-05,
      "loss": 6.6665,
      "step": 3552
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 6.151689529418945
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 6.999851703643799
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 7.494880676269531
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 7.281131267547607
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 6.630733966827393
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 6.281681537628174
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 7.014419078826904
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 7.32828950881958
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 6.882608890533447
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 6.720500469207764
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 6.811438083648682
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 7.507652759552002
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 6.154704570770264
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 7.18438196182251
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 7.973763465881348
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 7.328845977783203
    },
    {
      "epoch": 0.7709485094850949,
      "grad_norm": 12.818550109863281,
      "learning_rate": 1e-05,
      "loss": 6.9842,
      "step": 3556
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 6.3635125160217285
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 7.299934387207031
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 8.433496475219727
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 5.022976398468018
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 7.196192264556885
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 7.18966817855835
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 6.701577186584473
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 5.4592671394348145
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 6.47788143157959
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 7.693458080291748
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 4.066567420959473
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 6.872927188873291
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 6.297100067138672
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 6.903647422790527
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 7.318498611450195
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 7.018534183502197
    },
    {
      "epoch": 0.7718157181571815,
      "grad_norm": 20.037500381469727,
      "learning_rate": 1e-05,
      "loss": 6.6447,
      "step": 3560
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 8.31662654876709
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 7.797830104827881
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 6.265031814575195
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 6.921003818511963
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 6.62382698059082
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 7.033785343170166
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 5.998532295227051
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 7.5321574211120605
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 8.783781051635742
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.8037872314453125
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.794406890869141
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.957828998565674
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 6.039639949798584
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 7.026317119598389
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 6.717916011810303
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 5.185666561126709
    },
    {
      "epoch": 0.7726829268292683,
      "grad_norm": 15.88558292388916,
      "learning_rate": 1e-05,
      "loss": 6.9249,
      "step": 3564
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 6.3080549240112305
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 7.192523002624512
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 7.283313274383545
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 7.250460147857666
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 8.044183731079102
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 6.1661376953125
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 6.920857906341553
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 6.423593997955322
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 6.948400020599365
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 6.292802333831787
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 7.049204349517822
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 7.189591884613037
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 7.384268760681152
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 6.871411323547363
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 5.81137752532959
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 7.002353191375732
    },
    {
      "epoch": 0.773550135501355,
      "grad_norm": 12.747314453125,
      "learning_rate": 1e-05,
      "loss": 6.8837,
      "step": 3568
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 7.1329522132873535
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 5.140217304229736
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 6.794442176818848
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 7.470464706420898
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 6.668825149536133
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 5.999253273010254
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 6.089648246765137
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 5.389104843139648
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 6.312067031860352
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 7.126444339752197
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 6.852630138397217
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 4.2331061363220215
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 4.112801551818848
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 7.988800525665283
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 4.785020351409912
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 6.294710636138916
    },
    {
      "epoch": 0.7744173441734418,
      "grad_norm": 14.800786972045898,
      "learning_rate": 1e-05,
      "loss": 6.1494,
      "step": 3572
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 4.578628063201904
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 6.624556064605713
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 6.258243083953857
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 6.839642524719238
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 4.542715549468994
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 4.838900089263916
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 7.283225059509277
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 6.742404460906982
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 6.9755859375
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 7.689767837524414
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 6.308018684387207
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 6.593755722045898
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 6.092499732971191
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 7.338521957397461
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 7.057218074798584
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 5.44298791885376
    },
    {
      "epoch": 0.7752845528455284,
      "grad_norm": 19.796009063720703,
      "learning_rate": 1e-05,
      "loss": 6.3254,
      "step": 3576
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 7.183072090148926
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 6.993072509765625
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 6.496968746185303
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 8.127849578857422
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 7.003917694091797
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 6.1318817138671875
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 6.195878028869629
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 9.324820518493652
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 6.574684143066406
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 3.4889025688171387
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 7.012863636016846
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 6.953671932220459
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 5.482812881469727
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 6.758411884307861
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 6.204026222229004
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 7.50576114654541
    },
    {
      "epoch": 0.7761517615176152,
      "grad_norm": 12.60088062286377,
      "learning_rate": 1e-05,
      "loss": 6.7149,
      "step": 3580
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 6.422284126281738
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 7.951011657714844
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 5.059590816497803
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 7.52030086517334
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 7.097043037414551
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 5.8784003257751465
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 6.335052490234375
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 6.733588695526123
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 7.877321243286133
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 8.142041206359863
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 7.238826751708984
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 6.673286437988281
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 9.16854476928711
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 5.626178741455078
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 3.265500783920288
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 7.336848735809326
    },
    {
      "epoch": 0.7770189701897019,
      "grad_norm": 17.879098892211914,
      "learning_rate": 1e-05,
      "loss": 6.7704,
      "step": 3584
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 6.038568019866943
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 7.984438419342041
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 5.641942977905273
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 5.049629211425781
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 6.275177001953125
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 7.5275044441223145
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 7.10493278503418
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 6.50478458404541
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 6.843478202819824
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 5.7281599044799805
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 4.74664831161499
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 7.0590925216674805
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 5.605204105377197
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 7.717984676361084
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 7.976346015930176
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 8.193405151367188
    },
    {
      "epoch": 0.7778861788617886,
      "grad_norm": 18.39545440673828,
      "learning_rate": 1e-05,
      "loss": 6.6248,
      "step": 3588
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 7.142768859863281
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 7.14962911605835
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 6.275252819061279
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 6.559977054595947
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 5.608835697174072
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 7.040919780731201
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 6.7077412605285645
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 6.267157554626465
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 6.1418304443359375
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 6.002151012420654
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 7.801671981811523
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 7.9748921394348145
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 7.619633674621582
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 5.138945579528809
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 7.506616115570068
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 6.550957202911377
    },
    {
      "epoch": 0.7787533875338754,
      "grad_norm": 15.70956802368164,
      "learning_rate": 1e-05,
      "loss": 6.7181,
      "step": 3592
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 3.573176622390747
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 6.877673149108887
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 4.083765506744385
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 6.440670967102051
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 7.066086769104004
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 6.055073261260986
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 4.918593406677246
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 7.013021469116211
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 7.463070392608643
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 8.302691459655762
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 8.016915321350098
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 7.575095176696777
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 6.4199676513671875
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 6.82914924621582
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 8.317641258239746
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 7.354950428009033
    },
    {
      "epoch": 0.779620596205962,
      "grad_norm": 14.762733459472656,
      "learning_rate": 1e-05,
      "loss": 6.6442,
      "step": 3596
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 7.898672580718994
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 6.27141809463501
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 5.837114334106445
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 6.45585823059082
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 5.881669044494629
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 7.0920562744140625
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 7.900805950164795
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 6.215806007385254
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.303557395935059
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.313052654266357
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 6.900197982788086
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.305929183959961
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 7.083059310913086
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 6.990489482879639
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 4.64491081237793
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 7.838226795196533
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 15.464839935302734,
      "learning_rate": 1e-05,
      "loss": 6.8083,
      "step": 3600
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 6.970743656158447
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 5.673229694366455
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 7.183288097381592
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 7.319763660430908
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 6.991379261016846
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 6.315899848937988
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 6.779386520385742
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 7.294766426086426
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 7.3731465339660645
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 8.23198127746582
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 4.901171684265137
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 6.478769302368164
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 6.974985122680664
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 6.709353923797607
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 6.482083320617676
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 6.769828796386719
    },
    {
      "epoch": 0.7813550135501355,
      "grad_norm": 12.396868705749512,
      "learning_rate": 1e-05,
      "loss": 6.7781,
      "step": 3604
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 8.029749870300293
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 3.910796880722046
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 5.915617942810059
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 6.917918682098389
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 6.39708948135376
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 4.916219711303711
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 6.299567699432373
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 6.357255935668945
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 6.029327869415283
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 6.874779224395752
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 6.161753177642822
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 5.151246547698975
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 5.147794246673584
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 6.77334451675415
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 7.182019233703613
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 6.959548473358154
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 14.569211959838867,
      "learning_rate": 1e-05,
      "loss": 6.189,
      "step": 3608
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 5.593123912811279
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 6.213888168334961
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 4.370433330535889
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 6.995401859283447
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 8.010201454162598
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 5.609889030456543
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 8.768196105957031
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 6.3014235496521
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 7.033061504364014
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 5.285562992095947
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 6.89307165145874
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 6.611021518707275
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 5.406676769256592
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 6.192479133605957
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 8.002013206481934
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 7.247503757476807
    },
    {
      "epoch": 0.7830894308943089,
      "grad_norm": 14.755996704101562,
      "learning_rate": 1e-05,
      "loss": 6.5334,
      "step": 3612
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 7.300957679748535
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 8.129838943481445
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 6.6462860107421875
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 6.581971168518066
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 6.007893085479736
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 7.121072769165039
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 7.499358654022217
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 7.623749732971191
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 8.091700553894043
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 5.617407321929932
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 4.324397087097168
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 6.218490123748779
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 6.741406440734863
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 7.130615234375
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 7.4692792892456055
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 6.980903148651123
    },
    {
      "epoch": 0.7839566395663957,
      "grad_norm": 10.945306777954102,
      "learning_rate": 1e-05,
      "loss": 6.8428,
      "step": 3616
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 7.475438594818115
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 6.901336193084717
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 6.1119256019592285
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 4.54946756362915
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 5.606199741363525
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 6.490927696228027
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 7.5300445556640625
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 6.730996131896973
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.600831508636475
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.666292190551758
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.5681986808776855
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.369879245758057
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 5.948186874389648
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 7.133898735046387
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 7.430214881896973
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 7.419851303100586
    },
    {
      "epoch": 0.7848238482384824,
      "grad_norm": 16.58163833618164,
      "learning_rate": 1e-05,
      "loss": 6.5959,
      "step": 3620
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 6.346874713897705
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 6.436647415161133
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 7.18479585647583
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 7.918543815612793
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 6.62250280380249
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 7.312014102935791
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 7.859042644500732
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 6.975703716278076
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 4.763447284698486
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 5.9157843589782715
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 7.460872650146484
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 5.8511576652526855
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 6.977260112762451
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 5.406353950500488
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 6.693587303161621
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 5.050319194793701
    },
    {
      "epoch": 0.7856910569105691,
      "grad_norm": 12.625853538513184,
      "learning_rate": 1e-05,
      "loss": 6.5484,
      "step": 3624
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 5.2274322509765625
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 7.184502601623535
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 3.4709534645080566
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 6.6393609046936035
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.993441104888916
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 8.078045845031738
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.67867374420166
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 8.475488662719727
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 7.902617931365967
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 7.030766010284424
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 5.943370819091797
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 5.304101467132568
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 6.38103723526001
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 6.529709815979004
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 6.178974151611328
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 5.788256645202637
    },
    {
      "epoch": 0.7865582655826558,
      "grad_norm": 17.296537399291992,
      "learning_rate": 1e-05,
      "loss": 6.4879,
      "step": 3628
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 7.007932662963867
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 6.86963415145874
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 7.299191951751709
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 4.685156345367432
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 3.8628132343292236
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 5.701751708984375
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 7.913345813751221
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 6.2156081199646
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 7.143858432769775
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 5.882172107696533
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 6.370633125305176
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 6.859952449798584
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 7.097771167755127
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 6.016974449157715
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 7.138031005859375
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 5.580217361450195
    },
    {
      "epoch": 0.7874254742547425,
      "grad_norm": 16.440767288208008,
      "learning_rate": 1e-05,
      "loss": 6.3528,
      "step": 3632
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.81749153137207
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.604897499084473
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 5.829192638397217
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.270193099975586
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 6.011441707611084
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 7.930764675140381
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 6.276917934417725
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 6.064677715301514
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 6.972795009613037
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 4.020627975463867
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 6.55676794052124
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 8.494139671325684
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.669271945953369
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.15187931060791
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 4.420614719390869
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.941563129425049
    },
    {
      "epoch": 0.7882926829268293,
      "grad_norm": 13.0546236038208,
      "learning_rate": 1e-05,
      "loss": 6.3771,
      "step": 3636
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 6.330934047698975
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 5.250024795532227
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 3.409573793411255
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 8.417323112487793
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 4.604842662811279
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.2118754386901855
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.670340538024902
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.54549503326416
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 5.454188346862793
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 4.820237159729004
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 7.606829643249512
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 6.480247497558594
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 6.705336093902588
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 6.933591842651367
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 7.83279275894165
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 7.296586990356445
    },
    {
      "epoch": 0.789159891598916,
      "grad_norm": 14.222684860229492,
      "learning_rate": 1e-05,
      "loss": 6.2856,
      "step": 3640
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 7.1214752197265625
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 7.07874059677124
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 7.350781440734863
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 6.928879261016846
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 5.769372463226318
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 7.0362548828125
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 5.263605117797852
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 3.796457529067993
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 5.973058700561523
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 5.986724853515625
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 7.029490947723389
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 4.6138691902160645
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 7.118331432342529
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 5.87172794342041
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 7.823642253875732
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 7.558880805969238
    },
    {
      "epoch": 0.7900271002710028,
      "grad_norm": 16.594202041625977,
      "learning_rate": 1e-05,
      "loss": 6.3951,
      "step": 3644
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 5.480320930480957
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 7.052136421203613
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 6.089034557342529
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 6.914863586425781
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 7.255929470062256
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 6.483794689178467
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 6.414391040802002
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 6.803028106689453
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 7.020901679992676
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 6.255168914794922
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 5.633852005004883
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 6.481295585632324
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 6.397029876708984
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 7.2008280754089355
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 7.0922532081604
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 7.0233283042907715
    },
    {
      "epoch": 0.7908943089430894,
      "grad_norm": 12.194537162780762,
      "learning_rate": 1e-05,
      "loss": 6.5999,
      "step": 3648
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 6.955442428588867
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 6.856322765350342
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 8.507711410522461
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 5.809343338012695
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 5.37354040145874
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 6.990806579589844
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 6.996834754943848
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 6.332849502563477
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 11.993621826171875
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 6.954551696777344
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 6.362704753875732
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 7.20838737487793
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 8.067302703857422
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 5.61896276473999
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 8.563484191894531
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 6.30835485458374
    },
    {
      "epoch": 0.7917615176151761,
      "grad_norm": 16.499177932739258,
      "learning_rate": 1e-05,
      "loss": 7.1813,
      "step": 3652
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 3.9085116386413574
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 6.50376033782959
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 7.325084209442139
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 8.186532020568848
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 7.153994560241699
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 6.152493000030518
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 7.436547756195068
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 6.362758636474609
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 7.459369659423828
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 5.98834753036499
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 6.416043281555176
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 7.827784061431885
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 7.287596225738525
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 7.487613677978516
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 7.026135444641113
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 7.203584671020508
    },
    {
      "epoch": 0.7926287262872629,
      "grad_norm": 16.16663360595703,
      "learning_rate": 1e-05,
      "loss": 6.8579,
      "step": 3656
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 7.068490982055664
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 7.442595481872559
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 6.531569480895996
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 7.482291221618652
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 6.611059665679932
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 6.842409133911133
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 6.008978843688965
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 6.905176639556885
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 5.974086761474609
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 7.072012424468994
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 6.9405975341796875
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 6.55839729309082
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 6.015535831451416
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 7.375522136688232
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 7.855347156524658
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 6.810822486877441
    },
    {
      "epoch": 0.7934959349593496,
      "grad_norm": 20.46733283996582,
      "learning_rate": 1e-05,
      "loss": 6.8434,
      "step": 3660
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 6.616729736328125
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 6.098380088806152
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 7.995730876922607
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 6.89872407913208
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.607105255126953
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.553343296051025
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.768754959106445
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.136202335357666
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 7.881157398223877
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 5.676861763000488
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 4.881654739379883
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 7.380890846252441
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 6.80646276473999
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 5.5240397453308105
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 6.736739158630371
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 8.214597702026367
    },
    {
      "epoch": 0.7943631436314363,
      "grad_norm": 16.60972785949707,
      "learning_rate": 1e-05,
      "loss": 6.6736,
      "step": 3664
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 6.696902275085449
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 6.3586602210998535
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 8.003323554992676
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 6.926000118255615
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 7.868894100189209
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 6.028919696807861
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 4.179794788360596
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 6.16197395324707
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 7.301156997680664
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 7.307615280151367
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 6.834264278411865
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 8.02770709991455
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 6.99143123626709
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 5.7606072425842285
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 4.409071922302246
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 7.496224403381348
    },
    {
      "epoch": 0.795230352303523,
      "grad_norm": 14.345611572265625,
      "learning_rate": 1e-05,
      "loss": 6.647,
      "step": 3668
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 6.952149868011475
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 6.760958671569824
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 6.2143449783325195
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 7.350420951843262
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 6.483701705932617
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 7.11899995803833
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 8.222954750061035
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 7.40501594543457
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 7.922351360321045
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 5.73894739151001
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 6.220889091491699
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 7.110348224639893
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 6.845238208770752
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 5.751880168914795
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 5.881984233856201
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 4.48739767074585
    },
    {
      "epoch": 0.7960975609756098,
      "grad_norm": 14.434660911560059,
      "learning_rate": 1e-05,
      "loss": 6.6542,
      "step": 3672
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 7.043031215667725
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.953954696655273
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 7.027605056762695
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.973852634429932
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 7.269421577453613
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 6.673717021942139
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 6.23935604095459
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 6.703485012054443
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 7.11294412612915
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 7.053531646728516
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 6.8920063972473145
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 7.562860488891602
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 6.577491760253906
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 6.574222087860107
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 3.840440273284912
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 6.9623122215271
    },
    {
      "epoch": 0.7969647696476965,
      "grad_norm": 17.8739013671875,
      "learning_rate": 1e-05,
      "loss": 6.7163,
      "step": 3676
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 7.356434345245361
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 6.718420028686523
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 6.851227283477783
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 7.020012378692627
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 7.032863616943359
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 6.346086502075195
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 7.337136268615723
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 7.195230960845947
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 7.135528087615967
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 4.7190775871276855
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 5.782758712768555
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 7.01276159286499
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 6.354964733123779
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 7.979950904846191
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 7.054026126861572
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 4.919867992401123
    },
    {
      "epoch": 0.7978319783197833,
      "grad_norm": 15.129444122314453,
      "learning_rate": 1e-05,
      "loss": 6.676,
      "step": 3680
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 7.782817840576172
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 7.814123630523682
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 6.6317853927612305
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 7.016176223754883
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 5.589316368103027
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 7.951066970825195
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 7.330316543579102
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 11.067526817321777
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 6.23952054977417
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 5.991946220397949
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 6.956498146057129
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 5.038914203643799
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 8.573702812194824
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 5.539838790893555
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 6.886303901672363
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 5.093918800354004
    },
    {
      "epoch": 0.7986991869918699,
      "grad_norm": 18.82767105102539,
      "learning_rate": 1e-05,
      "loss": 6.969,
      "step": 3684
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 5.554949760437012
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 6.671183109283447
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 7.3180060386657715
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 7.048262596130371
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 6.8578267097473145
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 6.969882488250732
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 5.339090824127197
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 8.112895011901855
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 7.744119167327881
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 7.688114643096924
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 8.452773094177246
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 7.205939292907715
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 5.892094612121582
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 6.677700519561768
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 5.816189765930176
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 4.682083606719971
    },
    {
      "epoch": 0.7995663956639566,
      "grad_norm": 17.221939086914062,
      "learning_rate": 1e-05,
      "loss": 6.7519,
      "step": 3688
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 7.800174713134766
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 5.749761581420898
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 6.04700231552124
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 5.34304141998291
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 6.558116912841797
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 3.477644443511963
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 6.04393196105957
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 7.725766658782959
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 8.000933647155762
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 6.0671563148498535
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 6.749191761016846
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 7.048308372497559
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 6.664924621582031
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 9.054230690002441
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 7.198811054229736
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 7.210139751434326
    },
    {
      "epoch": 0.8004336043360434,
      "grad_norm": 16.004575729370117,
      "learning_rate": 1e-05,
      "loss": 6.6712,
      "step": 3692
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 7.181339740753174
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 7.509708881378174
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 7.019882678985596
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 5.770796775817871
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 7.0654778480529785
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 6.1342997550964355
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 5.471686840057373
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 7.170782566070557
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 7.46721887588501
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 7.285132884979248
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 6.855512619018555
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 7.528236389160156
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 6.155566692352295
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 7.080057144165039
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 7.318236827850342
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 7.0994038581848145
    },
    {
      "epoch": 0.80130081300813,
      "grad_norm": 15.231241226196289,
      "learning_rate": 1e-05,
      "loss": 6.8821,
      "step": 3696
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 6.190291404724121
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 3.7253787517547607
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 7.002737522125244
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 7.812043190002441
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 6.747037410736084
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 6.432081699371338
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 6.940554141998291
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 5.562460899353027
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 8.021027565002441
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 8.28995418548584
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 7.755572319030762
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 4.569762706756592
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 8.571037292480469
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 6.820679187774658
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 6.732874393463135
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 7.318397045135498
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 14.43976879119873,
      "learning_rate": 1e-05,
      "loss": 6.7807,
      "step": 3700
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 6.323032379150391
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 5.387605667114258
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 6.775747299194336
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 7.121707439422607
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 7.120253086090088
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 5.429031848907471
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 6.310999393463135
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 7.635948181152344
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 6.462661266326904
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 6.296597003936768
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 7.852869033813477
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 7.37548303604126
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 7.548112869262695
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 6.139506816864014
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 6.1180877685546875
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 4.61031436920166
    },
    {
      "epoch": 0.8030352303523035,
      "grad_norm": 17.6416015625,
      "learning_rate": 1e-05,
      "loss": 6.5317,
      "step": 3704
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 7.699329853057861
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 7.89748477935791
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 7.443710803985596
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 5.505795955657959
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 7.13011360168457
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 6.845722198486328
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 7.021052837371826
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 7.348811626434326
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 5.085941791534424
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 5.689216136932373
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 6.897410869598389
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 5.3919758796691895
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 5.121706008911133
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 6.744478702545166
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 6.5534257888793945
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 6.198530197143555
    },
    {
      "epoch": 0.8039024390243903,
      "grad_norm": 22.50472640991211,
      "learning_rate": 1e-05,
      "loss": 6.5359,
      "step": 3708
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 7.398184776306152
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 5.349944591522217
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 6.39071798324585
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 6.596714496612549
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 6.207527160644531
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 6.471654415130615
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 5.507906913757324
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 6.2854437828063965
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 7.242685317993164
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 6.530498027801514
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 6.753413677215576
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 6.491065979003906
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 7.036457061767578
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 5.859739303588867
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 6.806361675262451
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 7.619997978210449
    },
    {
      "epoch": 0.804769647696477,
      "grad_norm": 19.165292739868164,
      "learning_rate": 1e-05,
      "loss": 6.5343,
      "step": 3712
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 6.113523006439209
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 7.122904300689697
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 5.302755832672119
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 6.014402389526367
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 7.762395858764648
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 6.44364070892334
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 7.6363396644592285
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 6.074376106262207
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 6.478114604949951
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 6.32652473449707
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 7.441309928894043
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 4.740721225738525
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 7.058160781860352
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 6.973685264587402
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 7.5900397300720215
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 6.396053791046143
    },
    {
      "epoch": 0.8056368563685636,
      "grad_norm": 11.490242004394531,
      "learning_rate": 1e-05,
      "loss": 6.5922,
      "step": 3716
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 3.87383770942688
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 3.678253412246704
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 6.108640193939209
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 6.41398811340332
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 5.590907573699951
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 6.469937324523926
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 7.921142101287842
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 6.44965124130249
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 5.332952976226807
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 6.938912868499756
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 6.801615238189697
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 8.992544174194336
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 6.160640239715576
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 8.084139823913574
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 6.230442523956299
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 3.81360125541687
    },
    {
      "epoch": 0.8065040650406504,
      "grad_norm": 17.604673385620117,
      "learning_rate": 1e-05,
      "loss": 6.1788,
      "step": 3720
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 7.0910539627075195
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 6.846372604370117
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 6.235993385314941
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 7.580031394958496
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 6.21074104309082
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 6.056973457336426
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 8.807693481445312
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 7.677806377410889
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 6.936645984649658
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 6.62955904006958
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 6.309110641479492
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 6.02452278137207
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 6.882250785827637
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 4.956846237182617
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 4.196446418762207
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 7.120657920837402
    },
    {
      "epoch": 0.8073712737127371,
      "grad_norm": 13.806764602661133,
      "learning_rate": 1e-05,
      "loss": 6.5977,
      "step": 3724
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 6.924370288848877
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 7.661717414855957
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 7.258934020996094
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 7.592982769012451
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 6.7069091796875
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 8.14016056060791
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 4.946260929107666
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 7.466798782348633
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 5.396596908569336
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 5.860714912414551
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 7.4290242195129395
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 6.768008708953857
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 7.116723537445068
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 6.103334426879883
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 7.83983850479126
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 7.246111869812012
    },
    {
      "epoch": 0.8082384823848239,
      "grad_norm": 16.340103149414062,
      "learning_rate": 1e-05,
      "loss": 6.9037,
      "step": 3728
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 7.5985612869262695
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 5.460052490234375
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 7.818872928619385
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 7.5058369636535645
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 6.945862293243408
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 7.698120594024658
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 4.8291730880737305
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 6.9123759269714355
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 6.853550910949707
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 6.93682336807251
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 6.198329448699951
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 7.038816928863525
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 5.4482221603393555
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 8.108683586120605
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 6.732404708862305
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 6.335695266723633
    },
    {
      "epoch": 0.8091056910569105,
      "grad_norm": 19.089366912841797,
      "learning_rate": 1e-05,
      "loss": 6.7763,
      "step": 3732
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 7.944916725158691
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 8.797212600708008
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 7.782839775085449
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 6.2516961097717285
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 7.163643836975098
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 7.223745346069336
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 6.968489170074463
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 6.0748677253723145
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 6.987066745758057
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 6.944591999053955
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 8.526402473449707
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 6.815047264099121
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 7.309187412261963
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 6.27509880065918
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 6.804038047790527
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 4.572853088378906
    },
    {
      "epoch": 0.8099728997289973,
      "grad_norm": 15.807036399841309,
      "learning_rate": 1e-05,
      "loss": 7.0276,
      "step": 3736
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 7.235403060913086
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 6.4888410568237305
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 5.318455696105957
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 7.157045364379883
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 6.009499549865723
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 5.218231678009033
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 6.9446916580200195
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 6.476055145263672
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 6.659790515899658
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 6.556092262268066
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 7.571267127990723
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 7.60468864440918
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 5.507294178009033
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 6.5824456214904785
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 6.415796756744385
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 6.989295482635498
    },
    {
      "epoch": 0.810840108401084,
      "grad_norm": 17.31580352783203,
      "learning_rate": 1e-05,
      "loss": 6.5459,
      "step": 3740
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 6.619375228881836
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 6.747331142425537
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 7.381922245025635
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 9.135108947753906
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 7.421525955200195
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 7.359626770019531
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 6.091061115264893
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 8.245538711547852
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 6.69853401184082
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 6.460906505584717
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 7.170617580413818
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 6.052396774291992
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 5.974188327789307
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 6.194503307342529
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 3.7546398639678955
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 5.717858791351318
    },
    {
      "epoch": 0.8117073170731708,
      "grad_norm": 14.279324531555176,
      "learning_rate": 1e-05,
      "loss": 6.6891,
      "step": 3744
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 5.334619998931885
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 6.589829921722412
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 8.018655776977539
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 6.275008678436279
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 6.27695894241333
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 8.076444625854492
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 6.6866230964660645
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 7.234100818634033
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 3.8841190338134766
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 7.597132682800293
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 6.289047718048096
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 6.40485143661499
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 8.259557723999023
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 7.352513790130615
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 7.746532917022705
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 7.681116104125977
    },
    {
      "epoch": 0.8125745257452575,
      "grad_norm": 15.346720695495605,
      "learning_rate": 1e-05,
      "loss": 6.8567,
      "step": 3748
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 6.88539981842041
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 7.662898063659668
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 6.959648609161377
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 6.434896945953369
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 7.41446590423584
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 6.037517070770264
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 5.50232458114624
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 6.702821254730225
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 7.128112316131592
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 7.609838962554932
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 6.621288299560547
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 7.037539482116699
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 7.36722993850708
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 6.793091297149658
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 6.124168872833252
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 6.852647304534912
    },
    {
      "epoch": 0.8134417344173441,
      "grad_norm": 12.90480899810791,
      "learning_rate": 1e-05,
      "loss": 6.8209,
      "step": 3752
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 7.847611427307129
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 8.208883285522461
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 7.89808988571167
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 4.513584613800049
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 7.491985321044922
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 7.865971565246582
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 6.978739261627197
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 6.7564873695373535
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 7.455310344696045
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 6.952282905578613
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 7.007268905639648
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 7.169731140136719
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 5.44074010848999
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 6.845138072967529
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 8.049092292785645
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 7.1035637855529785
    },
    {
      "epoch": 0.8143089430894309,
      "grad_norm": 13.039566040039062,
      "learning_rate": 1e-05,
      "loss": 7.099,
      "step": 3756
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 7.153603553771973
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 7.491196632385254
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 6.442001819610596
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 7.389439105987549
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 7.736853122711182
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 6.186159133911133
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 6.497617721557617
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 5.647065162658691
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 7.667762279510498
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 7.038631439208984
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 6.720261096954346
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 4.025716781616211
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 6.139263153076172
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 7.47670316696167
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 9.583100318908691
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 7.963384628295898
    },
    {
      "epoch": 0.8151761517615176,
      "grad_norm": 16.846866607666016,
      "learning_rate": 1e-05,
      "loss": 6.9474,
      "step": 3760
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 5.923904895782471
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 8.101654052734375
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 7.026862621307373
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 7.746918678283691
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 7.758529186248779
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 8.481399536132812
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 6.357640743255615
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 5.6681227684021
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 7.7024641036987305
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 6.723954677581787
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 6.836224555969238
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 5.722382068634033
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 5.393756866455078
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 7.01542329788208
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 5.188498497009277
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 5.810667514801025
    },
    {
      "epoch": 0.8160433604336044,
      "grad_norm": 18.882553100585938,
      "learning_rate": 1e-05,
      "loss": 6.7162,
      "step": 3764
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 5.8399977684021
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 6.764688491821289
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 6.8198676109313965
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 7.4271111488342285
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 7.146924018859863
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 7.756193161010742
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 7.15977668762207
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 5.014039039611816
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 8.060063362121582
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 6.508636951446533
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 6.482905864715576
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 6.315044403076172
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 7.303702354431152
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.65247917175293
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.493901252746582
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 7.7279253005981445
    },
    {
      "epoch": 0.816910569105691,
      "grad_norm": 13.761014938354492,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 3768
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.514878749847412
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.886328220367432
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.792048454284668
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.528380870819092
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 5.789552688598633
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 7.141644477844238
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 7.253775119781494
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 5.637519359588623
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 6.434135437011719
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 7.876784324645996
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 7.351062774658203
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 4.3940815925598145
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 6.94006872177124
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 7.321459770202637
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 6.172088146209717
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 6.84336519241333
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 13.888805389404297,
      "learning_rate": 1e-05,
      "loss": 6.6173,
      "step": 3772
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 7.210259914398193
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 6.6827497482299805
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 4.950843811035156
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 6.159305095672607
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 7.780290603637695
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 5.1914167404174805
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 6.788193225860596
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 4.742027282714844
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 6.509607791900635
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 7.834074974060059
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 6.035720348358154
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 6.392119884490967
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 6.83131742477417
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 7.992671966552734
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 6.590178489685059
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 5.528038501739502
    },
    {
      "epoch": 0.8186449864498645,
      "grad_norm": 13.594735145568848,
      "learning_rate": 1e-05,
      "loss": 6.4512,
      "step": 3776
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 6.637717247009277
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 6.998053073883057
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 6.302465438842773
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 7.256378173828125
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 7.799470901489258
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 7.27506685256958
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 7.062987804412842
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 5.891319274902344
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 6.619875907897949
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 5.352121353149414
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 6.460512638092041
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 5.686215877532959
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 7.522937297821045
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 4.965939044952393
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 6.1049346923828125
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 6.355971336364746
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 14.685460090637207,
      "learning_rate": 1e-05,
      "loss": 6.5182,
      "step": 3780
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 7.853171348571777
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 6.8811259269714355
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 7.748448371887207
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 6.086607456207275
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 6.551215648651123
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 7.449558258056641
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 7.130702018737793
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 6.699009895324707
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 8.003691673278809
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 6.719414234161377
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 7.398751258850098
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 7.3938446044921875
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 5.724828720092773
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 5.752732276916504
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 6.74717903137207
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 7.226962566375732
    },
    {
      "epoch": 0.820379403794038,
      "grad_norm": 15.21741008758545,
      "learning_rate": 1e-05,
      "loss": 6.9605,
      "step": 3784
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 7.1407294273376465
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 6.4730095863342285
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 5.953371047973633
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 4.12317419052124
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 6.5263671875
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 5.544349193572998
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 6.169813632965088
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 5.289087772369385
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 8.860084533691406
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 6.380432605743408
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 7.341116905212402
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 7.124709129333496
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 7.423746109008789
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 5.961688041687012
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 8.540623664855957
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 6.746509075164795
    },
    {
      "epoch": 0.8212466124661246,
      "grad_norm": 15.549065589904785,
      "learning_rate": 1e-05,
      "loss": 6.5999,
      "step": 3788
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 8.258922576904297
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 7.452349662780762
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 6.561308860778809
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 4.48783016204834
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 6.326295375823975
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 7.343738079071045
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 6.105341911315918
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 5.454342365264893
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 7.037726402282715
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 6.824007511138916
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 7.609238624572754
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 7.3137006759643555
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 7.859728813171387
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 3.7022674083709717
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 6.200881004333496
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 6.493093967437744
    },
    {
      "epoch": 0.8221138211382114,
      "grad_norm": 14.185118675231934,
      "learning_rate": 1e-05,
      "loss": 6.5644,
      "step": 3792
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 7.119093894958496
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 3.4679665565490723
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 5.942534446716309
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 5.60927677154541
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 4.7875847816467285
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 7.049192905426025
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 7.371920108795166
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 6.325350761413574
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 6.6993408203125
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 7.173487663269043
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 5.727361679077148
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 6.290579795837402
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 7.096437931060791
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 8.09000301361084
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 5.941425800323486
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 7.027253150939941
    },
    {
      "epoch": 0.8229810298102981,
      "grad_norm": 14.583175659179688,
      "learning_rate": 1e-05,
      "loss": 6.3574,
      "step": 3796
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 8.879616737365723
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 8.156806945800781
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 5.058197021484375
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 4.370684623718262
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 6.745346546173096
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 5.811398506164551
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 7.621641635894775
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 7.118402481079102
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 7.293818473815918
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 5.160435199737549
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 6.891854286193848
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 6.212397575378418
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 7.240635395050049
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 5.507091522216797
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 7.193221092224121
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 8.06558895111084
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 21.265993118286133,
      "learning_rate": 1e-05,
      "loss": 6.7079,
      "step": 3800
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 6.463860511779785
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 7.400820255279541
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 6.984976291656494
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 6.348628520965576
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 6.229372024536133
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 7.635159015655518
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 8.017720222473145
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 7.008718967437744
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 7.5202531814575195
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 7.457956314086914
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 7.177402019500732
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 5.817544460296631
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 6.9165825843811035
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 6.084760665893555
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 7.176361560821533
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 6.128375053405762
    },
    {
      "epoch": 0.8247154471544715,
      "grad_norm": 12.255219459533691,
      "learning_rate": 1e-05,
      "loss": 6.898,
      "step": 3804
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 7.667819976806641
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 6.929074287414551
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 6.937117099761963
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 6.622730731964111
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 7.483913421630859
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 7.949345111846924
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 7.060406684875488
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 7.613263130187988
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.814634799957275
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.873613357543945
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.75576639175415
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.819404125213623
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 6.635611057281494
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 4.58261251449585
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 8.440201759338379
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 7.406522750854492
    },
    {
      "epoch": 0.8255826558265583,
      "grad_norm": 17.5023136138916,
      "learning_rate": 1e-05,
      "loss": 7.037,
      "step": 3808
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 6.304322719573975
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 3.853886604309082
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 7.004610538482666
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 3.5382392406463623
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 7.248308181762695
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 5.890176773071289
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 6.254034519195557
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 6.350072860717773
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 7.3240461349487305
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 6.277472019195557
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 6.356631278991699
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 7.504573822021484
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 7.645402908325195
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 5.792773246765137
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 6.802501678466797
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 7.515289306640625
    },
    {
      "epoch": 0.826449864498645,
      "grad_norm": 11.676941871643066,
      "learning_rate": 1e-05,
      "loss": 6.3539,
      "step": 3812
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 6.751302719116211
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 7.8175368309021
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 7.499120235443115
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 7.26882266998291
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 6.495354175567627
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 5.940853595733643
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 4.447089672088623
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 3.971463680267334
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 6.561552047729492
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 7.395626068115234
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 6.655822277069092
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 7.298951148986816
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 6.299719333648682
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 5.578520774841309
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 7.437089920043945
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 6.917527198791504
    },
    {
      "epoch": 0.8273170731707317,
      "grad_norm": 19.163894653320312,
      "learning_rate": 1e-05,
      "loss": 6.521,
      "step": 3816
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 6.059154033660889
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 6.792959213256836
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 7.0562872886657715
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 7.136905193328857
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 7.23501443862915
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 8.207401275634766
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 7.239351749420166
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 6.655571460723877
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 5.196777820587158
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 6.554616451263428
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 6.74769926071167
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 6.350857257843018
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 6.996234893798828
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 6.963924407958984
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 7.146350860595703
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 6.998922824859619
    },
    {
      "epoch": 0.8281842818428184,
      "grad_norm": 14.613749504089355,
      "learning_rate": 1e-05,
      "loss": 6.8336,
      "step": 3820
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 5.821125030517578
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 7.566864490509033
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 7.455035209655762
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 5.197683811187744
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 5.560856819152832
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 7.412756443023682
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 6.0494279861450195
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 7.251405715942383
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 6.566617488861084
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 7.664554119110107
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 6.411994934082031
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 6.144308090209961
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 6.803718090057373
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 7.1022491455078125
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 4.776677131652832
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 7.079460144042969
    },
    {
      "epoch": 0.8290514905149051,
      "grad_norm": 19.177967071533203,
      "learning_rate": 1e-05,
      "loss": 6.554,
      "step": 3824
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 7.1894989013671875
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 7.025681018829346
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 7.194257736206055
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 5.547571182250977
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 5.99577522277832
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 5.93953275680542
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 7.423672676086426
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 8.225509643554688
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 6.876791954040527
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 5.227893829345703
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 5.682882308959961
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 6.488105773925781
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 7.299672603607178
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 8.846102714538574
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 7.289919376373291
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 6.860815048217773
    },
    {
      "epoch": 0.8299186991869919,
      "grad_norm": 16.510099411010742,
      "learning_rate": 1e-05,
      "loss": 6.8196,
      "step": 3828
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 7.627890586853027
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 5.9529242515563965
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 3.912755012512207
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 7.101988315582275
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 6.817619800567627
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 7.10031795501709
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 6.546054840087891
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 6.280760765075684
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 6.402444362640381
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 8.23479175567627
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 5.876780986785889
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 6.008767604827881
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 6.470843315124512
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 5.904598236083984
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 8.813533782958984
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 6.92913293838501
    },
    {
      "epoch": 0.8307859078590786,
      "grad_norm": 18.05006980895996,
      "learning_rate": 1e-05,
      "loss": 6.6238,
      "step": 3832
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 6.631364345550537
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 5.41774320602417
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 5.35866117477417
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 6.723364353179932
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 10.632874488830566
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 5.680885314941406
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 5.826000690460205
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 8.245344161987305
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 6.9567365646362305
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 7.188982963562012
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 7.430187225341797
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 7.4882354736328125
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.1723456382751465
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.653690814971924
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.802728176116943
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.528810501098633
    },
    {
      "epoch": 0.8316531165311654,
      "grad_norm": 16.743450164794922,
      "learning_rate": 1e-05,
      "loss": 6.8586,
      "step": 3836
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 6.836535453796387
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 7.772523403167725
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 6.793612957000732
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 5.948173999786377
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 5.661203861236572
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 7.097510814666748
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 5.440858840942383
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 7.656114101409912
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 6.804778099060059
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 7.556570529937744
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 3.572127342224121
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 5.468878269195557
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 6.331562519073486
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 7.031075954437256
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 6.573608875274658
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 8.666669845581055
    },
    {
      "epoch": 0.832520325203252,
      "grad_norm": 15.116025924682617,
      "learning_rate": 1e-05,
      "loss": 6.5757,
      "step": 3840
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 6.699482440948486
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 7.0428972244262695
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 7.240004062652588
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 6.39348030090332
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 5.2257795333862305
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 7.008921146392822
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 5.191691875457764
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 6.259011745452881
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 8.342329978942871
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 6.923979759216309
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 8.022473335266113
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 6.867080211639404
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 6.34243106842041
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 5.5741400718688965
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 6.1738762855529785
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 7.468094825744629
    },
    {
      "epoch": 0.8333875338753387,
      "grad_norm": 18.56125831604004,
      "learning_rate": 1e-05,
      "loss": 6.6735,
      "step": 3844
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 6.67806339263916
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 6.047698020935059
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 5.6979899406433105
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 7.140422344207764
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 7.021993637084961
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 6.13662576675415
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 3.8933298587799072
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 4.912874698638916
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 6.549578666687012
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 7.247262001037598
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 5.717391490936279
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 7.0132341384887695
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 6.827352046966553
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 7.534709930419922
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 6.912734508514404
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 5.9033203125
    },
    {
      "epoch": 0.8342547425474255,
      "grad_norm": 16.34008026123047,
      "learning_rate": 1e-05,
      "loss": 6.3272,
      "step": 3848
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 3.832822799682617
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 6.116336822509766
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 3.536127805709839
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 6.03834342956543
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 5.213611125946045
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 7.782221794128418
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 6.419025897979736
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 7.898274898529053
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 5.3458428382873535
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 5.9152021408081055
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 6.83720064163208
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 6.997623920440674
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 7.667402267456055
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 5.206607818603516
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 6.860005855560303
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 4.59357213973999
    },
    {
      "epoch": 0.8351219512195122,
      "grad_norm": 12.566777229309082,
      "learning_rate": 1e-05,
      "loss": 6.0163,
      "step": 3852
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 3.824883460998535
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 6.80322265625
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 5.561996936798096
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 6.745598316192627
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 6.025972366333008
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 7.861886501312256
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 7.074139595031738
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 6.300565719604492
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 7.003954887390137
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 5.288205623626709
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 7.006931781768799
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 5.75208044052124
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 6.6557464599609375
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 7.043333530426025
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 5.969181060791016
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 6.614997386932373
    },
    {
      "epoch": 0.8359891598915989,
      "grad_norm": 19.211132049560547,
      "learning_rate": 1e-05,
      "loss": 6.3458,
      "step": 3856
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 6.800467491149902
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 6.670133590698242
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 6.578460693359375
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 8.653468132019043
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 5.936169147491455
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 5.98797082901001
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 7.191008567810059
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 5.022199630737305
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 6.738029956817627
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 7.582482814788818
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 8.112479209899902
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 6.466128826141357
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 6.914120197296143
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 6.725029945373535
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 6.671573162078857
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 6.842633247375488
    },
    {
      "epoch": 0.8368563685636856,
      "grad_norm": 16.73307991027832,
      "learning_rate": 1e-05,
      "loss": 6.8058,
      "step": 3860
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 7.856907844543457
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 6.035400867462158
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 5.078101634979248
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 7.140047550201416
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 6.803536891937256
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 8.408547401428223
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 5.573288440704346
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 6.271778583526611
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 7.026991844177246
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 6.880963325500488
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 4.408910274505615
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 4.432944297790527
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 6.297593116760254
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 6.744277000427246
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 6.8916335105896
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 7.034131050109863
    },
    {
      "epoch": 0.8377235772357724,
      "grad_norm": 17.114883422851562,
      "learning_rate": 1e-05,
      "loss": 6.4303,
      "step": 3864
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 7.853994369506836
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 7.109827518463135
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 5.226511001586914
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 4.904212474822998
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 6.4507832527160645
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 6.483070373535156
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 4.831504821777344
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 5.559045791625977
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 7.504055976867676
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 7.123035907745361
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 7.332084655761719
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 6.9132866859436035
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 7.3218255043029785
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 7.109918594360352
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 5.8343610763549805
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 6.131751537322998
    },
    {
      "epoch": 0.8385907859078591,
      "grad_norm": 14.630270004272461,
      "learning_rate": 1e-05,
      "loss": 6.4806,
      "step": 3868
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.707357883453369
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.561493873596191
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.70853853225708
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.802481174468994
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 7.869490146636963
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 7.2660017013549805
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 6.918724536895752
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 7.053618907928467
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 6.675186634063721
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 5.538788318634033
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 7.41380500793457
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 6.810330390930176
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 6.851363182067871
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 7.26479959487915
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 4.353672504425049
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 7.1942009925842285
    },
    {
      "epoch": 0.8394579945799459,
      "grad_norm": 16.169782638549805,
      "learning_rate": 1e-05,
      "loss": 6.7494,
      "step": 3872
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 8.168123245239258
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 6.620953559875488
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 7.438698768615723
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 7.515560150146484
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 6.6932244300842285
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 6.96863317489624
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 7.611411094665527
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 6.4354095458984375
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 6.140284061431885
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 6.858749866485596
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 8.132441520690918
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 7.562844276428223
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 4.766510963439941
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 3.635051727294922
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 7.293797969818115
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 6.609991073608398
    },
    {
      "epoch": 0.8403252032520325,
      "grad_norm": 17.554807662963867,
      "learning_rate": 1e-05,
      "loss": 6.7782,
      "step": 3876
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 6.181870460510254
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 5.081933498382568
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 5.155611038208008
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 7.086566925048828
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 7.601115703582764
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 6.251305103302002
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 8.834978103637695
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 7.542801856994629
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 6.451673984527588
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 7.069619178771973
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 7.087271213531494
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 5.422782897949219
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 6.251830101013184
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 6.12329626083374
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 6.721299648284912
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 7.694148540496826
    },
    {
      "epoch": 0.8411924119241192,
      "grad_norm": 12.720345497131348,
      "learning_rate": 1e-05,
      "loss": 6.6599,
      "step": 3880
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 6.914871692657471
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 4.129939079284668
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 7.1824259757995605
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 7.042663097381592
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 7.332406997680664
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 7.229395389556885
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 5.62214469909668
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 4.988040447235107
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 5.131688594818115
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 6.9652509689331055
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 5.139530658721924
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 6.095413684844971
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 3.9468178749084473
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 4.24533224105835
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 7.138631343841553
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 6.206371307373047
    },
    {
      "epoch": 0.842059620596206,
      "grad_norm": 18.2919864654541,
      "learning_rate": 1e-05,
      "loss": 5.9569,
      "step": 3884
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 4.085792064666748
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 7.808477878570557
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 7.06245231628418
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 7.239735126495361
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 6.497430324554443
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 5.520773410797119
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 6.90577507019043
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 6.091341972351074
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 7.237241268157959
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.413149833679199
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.404193878173828
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.426577091217041
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 6.944092273712158
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 6.987534046173096
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 7.4251885414123535
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 6.909087181091309
    },
    {
      "epoch": 0.8429268292682927,
      "grad_norm": 19.7762508392334,
      "learning_rate": 1e-05,
      "loss": 6.6224,
      "step": 3888
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 3.862502336502075
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 6.288655757904053
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 8.250600814819336
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 7.5006232261657715
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 6.694211006164551
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 8.709908485412598
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 5.97640323638916
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 6.423153877258301
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 7.34472131729126
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 5.264847755432129
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 7.307657718658447
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 6.828161716461182
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 7.002608776092529
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 7.018885135650635
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 7.606717109680176
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 7.035024642944336
    },
    {
      "epoch": 0.8437940379403794,
      "grad_norm": 19.67967987060547,
      "learning_rate": 1e-05,
      "loss": 6.8197,
      "step": 3892
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 7.171986103057861
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 7.366341590881348
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 7.305785655975342
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 7.41831111907959
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 5.776379585266113
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 7.525139331817627
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 6.185719013214111
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 6.6043195724487305
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 6.562626361846924
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 7.828397274017334
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 8.892436981201172
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 6.536136150360107
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 7.836907863616943
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 7.238539695739746
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 6.450555801391602
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 7.01673698425293
    },
    {
      "epoch": 0.8446612466124661,
      "grad_norm": 15.369317054748535,
      "learning_rate": 1e-05,
      "loss": 7.1073,
      "step": 3896
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 7.077971935272217
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 4.577506065368652
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 7.122830867767334
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 7.146759033203125
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 6.430657863616943
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 6.870913505554199
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 4.7669291496276855
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 5.791162490844727
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 6.636760711669922
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 6.423328876495361
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 5.787743091583252
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 5.074941635131836
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 6.766121864318848
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 6.080466270446777
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 7.1858296394348145
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 7.069193363189697
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 13.012810707092285,
      "learning_rate": 1e-05,
      "loss": 6.3006,
      "step": 3900
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 7.220019340515137
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 6.397586822509766
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 7.711588382720947
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 7.5368499755859375
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 6.778480529785156
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 6.225411415100098
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 7.442949295043945
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 7.015225410461426
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 7.077475547790527
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 7.542478084564209
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 6.602530002593994
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 4.306198596954346
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 8.041412353515625
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 7.700749397277832
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 5.001430034637451
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 6.610301971435547
    },
    {
      "epoch": 0.8463956639566396,
      "grad_norm": 17.158824920654297,
      "learning_rate": 1e-05,
      "loss": 6.8257,
      "step": 3904
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 6.3897199630737305
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 6.045322418212891
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 6.755069255828857
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 5.784992694854736
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 5.345001697540283
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 5.8319573402404785
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 7.053210258483887
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 8.837736129760742
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 6.517571926116943
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 7.302897930145264
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 5.852343559265137
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 6.683840274810791
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 6.159488201141357
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 7.4325270652771
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 7.21386194229126
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 4.67197322845459
    },
    {
      "epoch": 0.8472628726287262,
      "grad_norm": 20.727794647216797,
      "learning_rate": 1e-05,
      "loss": 6.4923,
      "step": 3908
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.450495719909668
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.697780609130859
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.3383283615112305
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.374095916748047
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 7.718414783477783
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 6.484164237976074
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 4.899024486541748
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 6.7079362869262695
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 7.618337631225586
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 7.58712100982666
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 6.29911994934082
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 6.261428356170654
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 7.0350341796875
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 5.499242305755615
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 5.320292949676514
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 5.523620128631592
    },
    {
      "epoch": 0.848130081300813,
      "grad_norm": 16.33281135559082,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 3912
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 7.218216896057129
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 4.9171833992004395
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 7.018824100494385
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 4.9917192459106445
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 7.5561113357543945
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 6.737947940826416
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 5.010372161865234
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 7.350625038146973
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 8.220766067504883
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 7.9013447761535645
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 7.1805739402771
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 7.736093044281006
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 6.640148162841797
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 5.314715385437012
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 5.450746536254883
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 3.7604422569274902
    },
    {
      "epoch": 0.8489972899728997,
      "grad_norm": 14.638679504394531,
      "learning_rate": 1e-05,
      "loss": 6.4379,
      "step": 3916
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 6.093080043792725
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 5.094106674194336
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 4.792888641357422
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 6.076627731323242
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 7.135514736175537
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 6.402363300323486
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 7.489738941192627
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 7.17110538482666
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 7.278849124908447
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 5.661900043487549
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 7.02262544631958
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 6.784487247467041
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 6.775032997131348
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 4.9813079833984375
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 8.275053024291992
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 7.094629764556885
    },
    {
      "epoch": 0.8498644986449865,
      "grad_norm": 26.881084442138672,
      "learning_rate": 1e-05,
      "loss": 6.5081,
      "step": 3920
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 6.915966033935547
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 7.984334945678711
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 7.983837604522705
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 6.7414069175720215
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 4.893569469451904
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 6.88425874710083
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 7.527535438537598
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 7.007075786590576
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 6.051663398742676
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 7.308980941772461
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 7.94271183013916
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 6.879310131072998
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 6.358694076538086
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 4.53992223739624
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 8.21183967590332
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 6.856847763061523
    },
    {
      "epoch": 0.8507317073170731,
      "grad_norm": 14.058666229248047,
      "learning_rate": 1e-05,
      "loss": 6.8805,
      "step": 3924
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 7.5688157081604
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 7.515164852142334
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 7.10615348815918
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 7.472980499267578
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 7.230251312255859
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 4.1935811042785645
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 6.165955543518066
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 7.0005998611450195
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 6.887783050537109
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 7.339094638824463
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 6.728943824768066
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 7.207752227783203
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 7.92879581451416
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 6.208078384399414
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 7.078421592712402
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 7.236940860748291
    },
    {
      "epoch": 0.8515989159891599,
      "grad_norm": 15.276259422302246,
      "learning_rate": 1e-05,
      "loss": 6.9293,
      "step": 3928
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 6.654925346374512
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 7.03813362121582
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 6.477838039398193
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 7.182857513427734
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 7.829541206359863
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 6.695071697235107
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 6.477536201477051
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 7.744576454162598
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 7.218969821929932
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 6.1801629066467285
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 6.636757850646973
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 7.80668306350708
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 7.348127841949463
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 7.634247303009033
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 7.409185409545898
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 7.091274738311768
    },
    {
      "epoch": 0.8524661246612466,
      "grad_norm": 11.124883651733398,
      "learning_rate": 1e-05,
      "loss": 7.0891,
      "step": 3932
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 6.98539924621582
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 6.375324249267578
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 5.185163974761963
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 6.851570129394531
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 6.680850982666016
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 6.757199764251709
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 6.5192670822143555
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 6.26029109954834
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 7.834493160247803
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 6.93263053894043
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 6.53687047958374
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 4.893036842346191
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 6.171374797821045
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 6.326467990875244
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 7.333685874938965
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 5.7233195304870605
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 22.440866470336914,
      "learning_rate": 1e-05,
      "loss": 6.4604,
      "step": 3936
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 6.820164680480957
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 6.6203389167785645
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 7.9161529541015625
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 7.222611904144287
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 5.081671714782715
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 5.919948577880859
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 5.412871360778809
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 4.207154750823975
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 6.702280521392822
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 6.0976338386535645
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 7.627815246582031
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 5.097445011138916
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 6.7226972579956055
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 6.571677207946777
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 7.27531623840332
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 6.896815299987793
    },
    {
      "epoch": 0.8542005420054201,
      "grad_norm": 14.832704544067383,
      "learning_rate": 1e-05,
      "loss": 6.387,
      "step": 3940
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 7.362079620361328
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 7.860658168792725
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 8.295372009277344
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 6.73381233215332
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 7.4830403327941895
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 6.804121494293213
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 5.947322368621826
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 5.807426929473877
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 5.702014923095703
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 5.5768141746521
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 6.254998207092285
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 5.817955493927002
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 7.172117233276367
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 7.437819480895996
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 6.550134658813477
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 6.806239604949951
    },
    {
      "epoch": 0.8550677506775067,
      "grad_norm": 15.57042121887207,
      "learning_rate": 1e-05,
      "loss": 6.7257,
      "step": 3944
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 7.582018852233887
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 6.74995231628418
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 5.051684856414795
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 4.813162326812744
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 7.013327121734619
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 6.07212495803833
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 6.342064380645752
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 5.490571022033691
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 7.266986846923828
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 7.785189628601074
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 7.806234359741211
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 7.171455383300781
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 4.158121585845947
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 6.44834566116333
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 5.558139801025391
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 7.711381435394287
    },
    {
      "epoch": 0.8559349593495935,
      "grad_norm": 12.828030586242676,
      "learning_rate": 1e-05,
      "loss": 6.4388,
      "step": 3948
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 6.5232133865356445
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 6.7989678382873535
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 7.032195568084717
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 7.433534145355225
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.0447306632995605
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.434812068939209
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.278404712677002
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.989066123962402
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 7.2662506103515625
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 6.583251476287842
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 6.972836017608643
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 5.607797622680664
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.320216655731201
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.286975860595703
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 5.993540287017822
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.944048881530762
    },
    {
      "epoch": 0.8568021680216802,
      "grad_norm": 14.554107666015625,
      "learning_rate": 1e-05,
      "loss": 6.5944,
      "step": 3952
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 6.320957660675049
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 3.4195644855499268
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 5.159271717071533
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 5.350557327270508
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 8.143250465393066
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 5.827877521514893
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 5.790924549102783
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 7.665431499481201
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 6.510140895843506
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 6.905539512634277
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 7.268579006195068
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 5.4884796142578125
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 7.885168075561523
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 7.117920398712158
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 7.395164966583252
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 6.998421669006348
    },
    {
      "epoch": 0.857669376693767,
      "grad_norm": 12.838676452636719,
      "learning_rate": 1e-05,
      "loss": 6.453,
      "step": 3956
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 7.567470073699951
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 7.35791015625
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 6.549563884735107
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 7.150010585784912
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.13886833190918
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.064216613769531
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.700563430786133
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 6.8014068603515625
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 7.116313934326172
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 5.358708381652832
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 5.376163959503174
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 7.3017988204956055
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 6.696701526641846
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 5.341546535491943
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 7.64594841003418
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 5.265409469604492
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 15.667760848999023,
      "learning_rate": 1e-05,
      "loss": 6.7145,
      "step": 3960
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 6.3883795738220215
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 6.210061550140381
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 5.654895305633545
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 4.753324508666992
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 7.805515289306641
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 6.9119415283203125
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 5.918534278869629
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 6.038741588592529
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 7.017168045043945
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 6.896833419799805
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 7.127053260803223
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 5.7319207191467285
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 6.834888935089111
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 7.095011234283447
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 7.1042070388793945
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 7.640890121459961
    },
    {
      "epoch": 0.8594037940379404,
      "grad_norm": 12.810233116149902,
      "learning_rate": 1e-05,
      "loss": 6.5706,
      "step": 3964
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 5.825666427612305
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 7.1132402420043945
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 6.685787677764893
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 5.968231201171875
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 8.063385963439941
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 6.382086277008057
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 6.437231540679932
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 6.1379780769348145
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 7.578709602355957
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 7.376401424407959
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 6.265513896942139
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 6.945366382598877
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 6.397703647613525
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 8.532804489135742
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 5.128964900970459
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 6.856507778167725
    },
    {
      "epoch": 0.8602710027100271,
      "grad_norm": 14.472023010253906,
      "learning_rate": 1e-05,
      "loss": 6.731,
      "step": 3968
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 6.019774913787842
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 7.402833938598633
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 7.729003429412842
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 6.87913703918457
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 6.617918491363525
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 5.046036720275879
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 7.551590919494629
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 6.507708549499512
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 6.791133403778076
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 6.66942024230957
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 6.360174179077148
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 5.843759536743164
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 6.7336320877075195
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 5.698828220367432
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 6.289028167724609
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 7.083045482635498
    },
    {
      "epoch": 0.8611382113821138,
      "grad_norm": 15.698107719421387,
      "learning_rate": 1e-05,
      "loss": 6.5764,
      "step": 3972
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 6.7563066482543945
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 7.117514610290527
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 6.944364070892334
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 7.8584370613098145
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 7.003349304199219
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 7.76878023147583
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 6.24328088760376
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 6.450489044189453
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.490074634552002
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.827210903167725
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.769278526306152
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 7.308055877685547
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 7.292926788330078
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 3.2750959396362305
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 6.089228630065918
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 6.93459415435791
    },
    {
      "epoch": 0.8620054200542006,
      "grad_norm": 15.847566604614258,
      "learning_rate": 1e-05,
      "loss": 6.6956,
      "step": 3976
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 6.211947441101074
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 5.788972854614258
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 7.272461891174316
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 7.665835857391357
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 6.631475925445557
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 6.480956554412842
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 6.640324115753174
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 5.82747220993042
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 7.012790203094482
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 5.9720139503479
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 7.387238502502441
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 6.409384250640869
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 8.831724166870117
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 6.3022847175598145
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 7.571967601776123
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 6.439213275909424
    },
    {
      "epoch": 0.8628726287262872,
      "grad_norm": 16.157848358154297,
      "learning_rate": 1e-05,
      "loss": 6.7779,
      "step": 3980
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 6.678579807281494
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 7.228268146514893
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 6.929973602294922
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 7.192352771759033
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 7.018308639526367
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 7.0427069664001465
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 7.861610412597656
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 7.095727443695068
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.829268455505371
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.452876567840576
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 8.2503023147583
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.693973541259766
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 6.596642017364502
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 7.226239204406738
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 6.904682636260986
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 5.760814666748047
    },
    {
      "epoch": 0.863739837398374,
      "grad_norm": 13.996621131896973,
      "learning_rate": 1e-05,
      "loss": 6.9851,
      "step": 3984
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 6.622155666351318
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 7.728626251220703
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 5.741454601287842
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 7.788511753082275
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 8.310077667236328
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 8.243989944458008
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 9.434341430664062
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 7.2890625
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 6.87217903137207
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 7.728607177734375
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 6.916199207305908
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 4.023895263671875
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 7.300844669342041
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 5.967746257781982
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 6.336906909942627
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 6.876228332519531
    },
    {
      "epoch": 0.8646070460704607,
      "grad_norm": 13.142544746398926,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 3988
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.941969394683838
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.738375663757324
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.746884822845459
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 7.218328475952148
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 7.648667335510254
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 7.010362148284912
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 7.994920253753662
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 7.124230861663818
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 6.835980415344238
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 7.786077976226807
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 7.64595365524292
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 6.140051364898682
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 6.4233293533325195
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 5.286513805389404
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 6.058281898498535
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 7.828361511230469
    },
    {
      "epoch": 0.8654742547425475,
      "grad_norm": 15.090248107910156,
      "learning_rate": 1e-05,
      "loss": 6.9643,
      "step": 3992
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 6.481398582458496
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 6.958860397338867
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 6.671426773071289
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 7.277188777923584
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 6.633552551269531
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 7.160679817199707
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 8.612520217895508
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 6.096423149108887
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 7.166905879974365
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 3.5478267669677734
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 7.1195173263549805
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 5.879117012023926
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 5.75156831741333
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 7.178102970123291
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 8.299001693725586
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 7.126216411590576
    },
    {
      "epoch": 0.8663414634146341,
      "grad_norm": 18.438127517700195,
      "learning_rate": 1e-05,
      "loss": 6.7475,
      "step": 3996
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 6.352917671203613
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 5.921064376831055
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 5.436773777008057
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 7.188216686248779
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 6.933093547821045
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 7.942883491516113
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 7.056480884552002
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 4.203474044799805
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 4.518377304077148
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 6.643729209899902
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 6.77630090713501
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 5.694079399108887
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 4.686221599578857
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 3.7486066818237305
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 7.222292900085449
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 6.606314182281494
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 14.31647777557373,
      "learning_rate": 1e-05,
      "loss": 6.0582,
      "step": 4000
    },
    {
      "epoch": 0.8672086720867209,
      "eval_runtime": 481.2763,
      "eval_samples_per_second": 4.26,
      "eval_steps_per_second": 4.26,
      "step": 4000
    }
  ],
  "logging_steps": 4,
  "max_steps": 4612,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.1091655614464e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
